{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sección De Hiperparámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hiperparámetros para la carga y preprocesamiento de datos\n",
    "CLASS_FILTER_AMOUNT = 50           # Número de clases a clasificar\n",
    "IMAGE_SIZE = 224                   # Tamaño de la imagen de entrada (224x224)\n",
    "BATCH_SIZE = 64                    # Tamaño del batch para entrenamiento\n",
    "\n",
    "# Hiperparámetros de entrenamiento\n",
    "NON_DOWNSAMPLING_KERNEL_SIZE = 3   # Tamaño del kernel para convoluciones sin downsampling\n",
    "DOWNSAMPLING_KERNEL_SIZE = 1       # Tamaño del kernel para convoluciones con downsampling\n",
    "NON_DOWNSAMPLING_STRIDE = 1        # Stride para convoluciones sin downsampling\n",
    "DOWNSAMPLING_STRIDE = 2            # Stride para convoluciones con downsampling\n",
    "PADDING_SIZE = 1\n",
    "RESNET20_LAYERS = 3                # Padding para las convoluciones\n",
    "\n",
    "# Hiperparámetros de regularización y optimización\n",
    "EARLY_PATIENCE = 15                # Paciencia para early stopping\n",
    "DROPOUT_RATE_CONV = 0.25           # Tasa de dropout para capas convolucionales (si se utiliza)\n",
    "DROPOUT_RATE_FC1 = 0.4             # Tasa de dropout para la capa totalmente conectada (ajustada para evitar pérdida excesiva de información)\n",
    "LEARNING_RATE = 0.001              # Tasa de aprendizaje\n",
    "NORM_LAYER_INIT_WEIGHT = 1         # Valor de inicialización para pesos de BatchNorm\n",
    "NORM_LAYER_INIT_BIAS = 0           # Valor de inicialización para bias de BatchNorm\n",
    "MOMENTUM = 0.9                     # Momentum para el optimizador\n",
    "EPOCHS = 100                        # Número de épocas de entrenamiento\n",
    "SCHEDULER_PATIENCE = 7             # Paciencia para el scheduler de tasa de aprendizaje\n",
    "SCHEDULER_FACTOR = 0.2             # Factor de reducción para el scheduler\n",
    "WEIGHT_DECAY = 0.0001              # Peso de regularización L2\n",
    "\n",
    "# Hiperparámetros de seguridad\n",
    "CHECKPOINT_PATH = './saved_models/plantnet_300k_resnet20_checkpoint.pth' # Ruta para guardar el checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library import and dataset download.\n",
    "\n",
    "In the next block, the import of the necessary libraries for the operation of the convolutional neural network takes place, together with secondary tools for auxiliary tasks such as graphing, etc. In addition, the set of images to be classified, belonging to a dataset hosted in the Kaggle platform, is downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adrian/Dev/Plant-Buddies/backend/model/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "import torch\n",
    "import os\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# We begin by downloading the required dataset from the kaggle platform.\n",
    "# The selected dataset is PlantNet-300k by Noah Badoa. It contains approximately 300k images from several plant species.\n",
    "\n",
    "def download_dataset(dataset_name, version = None, path = None, force_download = False):\n",
    "    source = f\"{dataset_name}/versions/{version}\" if version else dataset_name\n",
    "    return kagglehub.dataset_download(\n",
    "        source,\n",
    "        path = path,\n",
    "        force_download = force_download\n",
    "    )\n",
    "\n",
    "dataset_path = download_dataset(dataset_name = \"noahbadoa/plantnet-300k-images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting the set of classes with the highest number of images.\n",
    "\n",
    "Then, according to the number of images in the dataset for each of the classes, we will filter out those with the fifty largest numbers. We aim to facilitate and adapt the model to an adequate classification capacity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fifty most populated classes are:\n",
      "---------------------------------\n",
      "\n",
      "['1363227', '1392475', '1356022', '1364099', '1355937', '1359517', '1357330', '1358752', '1359620', '1363128', '1363991', '1355936', '1394460', '1363740', '1394994', '1364173', '1359616', '1364164', '1361824', '1361823', '1397364', '1358095', '1363130', '1389510', '1374048', '1367432', '1409238', '1397268', '1393614', '1356781', '1369887', '1393241', '1394420', '1398178', '1408774', '1435714', '1394591', '1385937', '1355932', '1358094', '1393425', '1393423', '1398592', '1408961', '1358133', '1358766', '1361656', '1384485', '1356257', '1358689']\n"
     ]
    }
   ],
   "source": [
    "# Specifying the paths to the training, evaluation and test directories.\n",
    "subpath = 'plantnet_300K'\n",
    "train_path = os.path.join(dataset_path, f\"{subpath}/images_train\")\n",
    "val_path = os.path.join(dataset_path, f\"{subpath}/images_val\")\n",
    "test_path = os.path.join(dataset_path, f\"{subpath}/images_test\")\n",
    "\n",
    "# We iterate trough the training directory to get the total number of images of each class\n",
    "class_count = []\n",
    "for class_id in os.listdir(train_path):\n",
    "    class_dir = os.path.join(train_path, class_id)\n",
    "    num_images = len(os.listdir(class_dir))\n",
    "    class_count.append((class_id, num_images))\n",
    "\n",
    "# Once we have the number of images of each class, we sort the array and filter the desired ones\n",
    "class_filter_amount = 50\n",
    "class_count.sort(key = lambda x: x[1], reverse = True)\n",
    "top_classes = [cls[0] for cls in class_count[:CLASS_FILTER_AMOUNT]]\n",
    "\n",
    "# Printing out the ids of the 50 most populated classes.\n",
    "print(f\"\\nFifty most populated classes are:\\n---------------------------------\\n\\n{top_classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation of datasets filtered by the obtained classes.\n",
    "\n",
    "From the test, training and validation directories, several datasets are created, filtering out those images not corresponding to the allowed classes. A python class is implemented by extending ‘ImageFolder’, manipulating its properties as appropriate to obtain the expected results. In addition, the labels associated with each image in a dataset are reassigned so that they belong to a range between zero and the total number of classes minus one. Finally, a primitive transformation is applied to each of the images in the data sets, manipulating the size and shape of the images (tensor transformation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlanetNet300K_Filtered_Dataset(datasets.ImageFolder):\n",
    "    \n",
    "    \"\"\"\n",
    "    Data classes extending ‘ImageFolder’. Responsible for creating exclusionary datasets, reasigning labels and applying\n",
    "    unnormalized transformations.\n",
    "\n",
    "    Attributes\n",
    "    -----------------------------------\n",
    "    path (str): path to the folder where the set of images to be filtered is located.\n",
    "    allowed_classes (list): list with the set of class names to keep. \n",
    "    tranform (callable): primitive set of transformations to apply to each filtered image.\n",
    "    loader (callable): function in charge of loading the set of images from the specified path.\n",
    "    is_valid_file (callable): function that defines if a found file is valid or not at the time of loading.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, path, allowed_classes, transform=None, loader=datasets.folder.default_loader, is_valid_file=None):\n",
    "        # The ‘Image Folder’ object is initialized with the received attributes.\n",
    "        super().__init__(path, transform=transform, loader=loader, is_valid_file=is_valid_file)\n",
    "        \n",
    "        # The new mapping is created between the automatically originated tags for each image and the class number to which they belong.\n",
    "        self.allowed_classes = allowed_classes\n",
    "        self.new_class_to_idx = {class_name: idx for idx, class_name in enumerate(allowed_classes)}\n",
    "        \n",
    "        # The process of filtering and reassigning labels is performed.\n",
    "        filtered_samples = []\n",
    "        for path, orig_label in self.samples:\n",
    "            class_name = self.classes[orig_label]\n",
    "            if class_name in self.allowed_classes:\n",
    "                new_label = self.new_class_to_idx[class_name]\n",
    "                filtered_samples.append((path, new_label))\n",
    "\n",
    "        # Properties 'samples', 'classes' and 'class_to_idx' are updated.\n",
    "        self.samples = filtered_samples\n",
    "        self.classes = allowed_classes\n",
    "        self.class_to_idx = self.new_class_to_idx\n",
    "\n",
    "unnormalized_transformation = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Defining training, test and validation datasets\n",
    "train_dataset = PlanetNet300K_Filtered_Dataset(train_path, top_classes, unnormalized_transformation)\n",
    "val_dataset = PlanetNet300K_Filtered_Dataset(val_path, top_classes, unnormalized_transformation)\n",
    "test_dataset = PlanetNet300K_Filtered_Dataset(test_path, top_classes, unnormalized_transformation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the global mean and standart desviation for each dataset\n",
    "\n",
    "The global mean and standard deviation of each of the datasets is calculated, in order to perform a normalization of the pixel values of the images. For this purpose, we use a method that accumulates the sum and sum of squares of all pixels in the dataset.\n",
    "\n",
    "The overall mean of the data set is calculated according to the following formula:\n",
    "\n",
    "$mean = \\frac{total\\,sum\\,of\\,pixels}{total\\,number\\,of\\,pixels}$\n",
    "\n",
    "The standard deviation follows the following formula:\n",
    "\n",
    "$std = \\sqrt{\\frac{total\\,sum\\,of\\,squares}{total\\,number\\,of\\,pixels} - (mean)^{2}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset statistics\n",
      "---------------------------------------------------------------------------\n",
      "Train dataset mean: tensor([0.4399, 0.4692, 0.3228], device='cuda:0')\n",
      "Train dataset std: tensor([0.2337, 0.2185, 0.2297], device='cuda:0')\n",
      "\n",
      "Val dataset statistics\n",
      "---------------------------------------------------------------------------\n",
      "Val dataset mean: tensor([0.4403, 0.4687, 0.3238], device='cuda:0')\n",
      "Val dataset std: tensor([0.2345, 0.2186, 0.2302], device='cuda:0')\n",
      "\n",
      "Test dataset statistics\n",
      "---------------------------------------------------------------------------\n",
      "Test dataset mean: tensor([0.4407, 0.4702, 0.3237], device='cuda:0')\n",
      "Test dataset std: tensor([0.2340, 0.2182, 0.2297], device='cuda:0')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def compute_mean_std(dataset, batch_size = BATCH_SIZE, num_workers = 0, identifier = None):\n",
    "    \"\"\"\n",
    "    Computes the maan and standard desviation of a given dataset.\n",
    "\n",
    "    Parameters\n",
    "    -------------------------------------------------------------\n",
    "    dataset (ImageFolder): the dataset on which the operations are going to be based\n",
    "    batch_size (int): number of images from the dataset to be processed in parallel in a single segment.\n",
    "    num_workers (int): indicates the number of threads (parallel processes) that will be used to load the data.\n",
    "    \"\"\"\n",
    "\n",
    "    loader = DataLoader(dataset,batch_size = batch_size,num_workers = num_workers,pin_memory = True,shuffle = False)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Defining variables to keep track of total sum of pixels, total sum of squares and total number of pixels\n",
    "    dataset_sum_of_pixels = torch.zeros(3, device = device)\n",
    "    dataset_sum_of_squares = torch.zeros(3, device = device)\n",
    "    dataset_number_of_pixels = 0\n",
    "\n",
    "    for batch, _ in loader:\n",
    "        \"\"\"\n",
    "        Each batch (set of images) is represented in the format [b, c, h, w] beign:\n",
    "        b: number of images\n",
    "        c: number of channels per image\n",
    "        h: height of each individual image\n",
    "        w: width of each individual image\n",
    "        \"\"\"\n",
    "        batch = batch.to(device)\n",
    "        samples = batch.size(0)\n",
    "        batch_number_of_pixels = samples * batch.size(2) * batch.size(3)\n",
    "        dataset_number_of_pixels += batch_number_of_pixels\n",
    "\n",
    "        # The dimensions of the images are flattened in the form {b, c, h * w}.\n",
    "        batch = batch.view(samples, batch.size(1), -1)\n",
    "\n",
    "        # Both sum and sum of squares of all images in the batch are obtained.\n",
    "        dataset_sum_of_pixels += batch.sum(dim = (0, 2))\n",
    "        dataset_sum_of_squares += (batch ** 2).sum(dim = (0, 2))\n",
    "\n",
    "    # Mean and standard deviation of the dataset are calculated and printed out.\n",
    "    mean = dataset_sum_of_pixels / dataset_number_of_pixels\n",
    "    std = torch.sqrt((dataset_sum_of_squares / dataset_number_of_pixels) - (mean ** 2))\n",
    "\n",
    "    print(f\"{identifier.capitalize()} dataset statistics\\n{'-' * 75}\")\n",
    "    print(f\"{identifier.capitalize()} dataset mean: {mean}\\n{identifier.capitalize()} dataset std: {std}\\n\")\n",
    "\n",
    "    return mean, std\n",
    "\n",
    "# Definition of mean and std variables for all testing, validation and training datasets\n",
    "train_mean, train_std = compute_mean_std(train_dataset, BATCH_SIZE, identifier = 'train')\n",
    "val_mean, val_std = compute_mean_std(val_dataset, BATCH_SIZE, identifier = 'val')\n",
    "test_mean, test_std = compute_mean_std(test_dataset, BATCH_SIZE, identifier = 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation of data loaders. Data Augmentation.\n",
    "\n",
    "In the next section, new transformations are defined allowing data augmentation techniques to be performed on the different sets of images. More specifically, on the training dataset, operations\n",
    "such as angular rotations, horizontal and vertical flips, modifications in contrast, brightness, saturation or other image parameters will be performed. The validation and testing datasets will\n",
    "be more lightly modified, as they will be used for tasks that do not require these techniques. The previously calculated mean and standard deviation values will be used to normalize the pixels of\n",
    "all images used in the model. After applying these transformations, data loaders are generated, whose main task is to programmatically introduce the images into the neural network model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining final transformation for train dataset.\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE), antialias = True),\n",
    "    transforms.RandomRotation(180),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation = 0.05, hue = 0.05),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(train_mean, train_std)\n",
    "])\n",
    "\n",
    "# Defining final transformation for validation dataset.\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE), antialias = True),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(val_mean, val_std)\n",
    "])\n",
    "\n",
    "# Defining final tranformation for testing dataset.\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE), antialias = True),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(test_mean, test_std)\n",
    "])\n",
    "\n",
    "# Transformations are applied by generating new datasets that manifest these characteristics.\n",
    "train_dataset = PlanetNet300K_Filtered_Dataset(train_path, top_classes, train_transforms)\n",
    "test_dataset = PlanetNet300K_Filtered_Dataset(test_path, top_classes, test_transforms)\n",
    "val_dataset = PlanetNet300K_Filtered_Dataset(val_path, top_classes, val_transforms)\n",
    "\n",
    "# Defining data loaders for training, testing and validation\n",
    "train_loader = DataLoader(train_dataset, batch_size = BATCH_SIZE, shuffle = True, num_workers = 0, pin_memory = True)\n",
    "test_loader = DataLoader(test_dataset, batch_size = BATCH_SIZE, shuffle = False, num_workers = 0, pin_memory = True)\n",
    "val_loader = DataLoader(val_dataset, batch_size = BATCH_SIZE, shuffle = False, num_workers = 0, pin_memory = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Stopping\n",
    "\n",
    "The early stopping mechanism for model training is established. This technique helps to avoid overfitting and save training time. To detect whether the model should be stopped, the loss produced in the\n",
    "validation stage is used, which if it does not decrease in a certain number of stages produces the stopping mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Early_Stopper:\n",
    "    \"\"\"\n",
    "    Class in charge of stopping the training process if the validation loss does not improve after a certain number of epochs.\n",
    "\n",
    "    Attributes\n",
    "    -----------------------------------\n",
    "    patience (int): number of epochs to wait before stopping the training process.\n",
    "    counter (int): number of epochs without improvement.\n",
    "    best_loss (float): best loss obtained during the training process.\n",
    "    stop (bool): flag that indicates if the training process should be stopped.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, patience = EARLY_PATIENCE, callback = lambda **_: None):\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.best_loss = float('inf')\n",
    "        self.stop = False\n",
    "        self.callback = callback\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        \"\"\"\n",
    "        The method is called to update the counter and the best loss obtained during the training process.\n",
    "\n",
    "        Parameters\n",
    "        -----------------------------------\n",
    "        val_loss (float): loss obtained during the validation process.\n",
    "        \"\"\"\n",
    "        \n",
    "        if val_loss < self.best_loss:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.callback()\n",
    "                self.stop = True\n",
    "\n",
    "        return self.stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model architecture. Layers and distribution.\n",
    "\n",
    "Convolutional network model characterized by having and using two types of structurally and functionally differentiated layers. First, we find the convolution layers, so called because of the\n",
    "mathematical operation they perform on the image pixels. They are mainly in charge of extracting the relevant features or patterns in the images in order to classify them later on. Secondly,\n",
    "we find the classification layers, whose function is to classify the features resulting from applying the different convolution layers. They are responsible for defining to which class each of\n",
    "the images that pass through the model belongs. Finally, a series of secondary but very important layers are used, including pooling layers, normalization layers and dropout layers, each with a\n",
    "role specified in the architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of a resnet20 block\n",
    "class ResNet20_Block(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride = 1, change_size = True):\n",
    "        super().__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        Architecture of a ResNet20 block as follows:\n",
    "\n",
    "        1. Convolutional layer with kernel size 3x3 (non dowsampling) and dependent on dowsampling stride size.\n",
    "        2. Batch normalization layer.\n",
    "        3. Convolutional layer with kernel size 3x3 (non dowsampling) and stride 1 (non dowsampling).\n",
    "        4. Batch normalization layer.\n",
    "        5. If change size is true, convolutional layer with kernel size 1x1 (dowsampling) and stride 2 (dowsampling).\n",
    "        \"\"\"\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size = NON_DOWNSAMPLING_KERNEL_SIZE, stride = stride, padding = PADDING_SIZE, bias = False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size = NON_DOWNSAMPLING_KERNEL_SIZE, stride = NON_DOWNSAMPLING_STRIDE, padding = PADDING_SIZE, bias = False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.change_size = change_size\n",
    "        if change_size:\n",
    "            self.residual_conv = nn.Sequential(nn.Conv2d(in_channels, out_channels, kernel_size = DOWNSAMPLING_KERNEL_SIZE, stride = DOWNSAMPLING_STRIDE, bias = False), nn.BatchNorm2d(out_channels))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        identity = x if not self.change_size else self.residual_conv(x)\n",
    "        y = F.relu(self.bn1(self.conv1(x)))\n",
    "        y = self.bn2(self.conv2(y))\n",
    "        y += identity\n",
    "        return F.relu(y)\n",
    "    \n",
    "class ResNet20(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        Architecture of a ResNet20 model as follows:\n",
    "\n",
    "        1. Convolutional layer with kernel size 3x3 (non dowsampling) and stride 1 (non dowsampling).\n",
    "        2. Batch normalization layer.\n",
    "        3. ResNet20 block consisting on 9 layers of 32 in channels, 32 out channels of size 224x224.\n",
    "        4. ResNet20 block consisting on 9 layers of 64 in channels, 64 out channels of size 112x112.\n",
    "        5. ResNet20 block consisting on 9 layers of 128 in channels, 128 out channels of size 56x56.\n",
    "        6. Fully connected layer with 128 input features and 50 output features.\n",
    "        \"\"\"\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size = NON_DOWNSAMPLING_KERNEL_SIZE, stride = NON_DOWNSAMPLING_STRIDE, padding = PADDING_SIZE, bias = False)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.block1 = self._create_block(in_channels = 32, out_channels = 32, stride = NON_DOWNSAMPLING_STRIDE, change_size = False)\n",
    "        self.block2 = self._create_block(in_channels = 32, out_channels = 64, stride = DOWNSAMPLING_STRIDE, change_size = True)\n",
    "        self.block3 = self._create_block(in_channels = 64, out_channels = 128, stride = DOWNSAMPLING_STRIDE, change_size = True)\n",
    "        self.fc = nn.Linear(128, CLASS_FILTER_AMOUNT)\n",
    "\n",
    "    def _create_block(self, in_channels, out_channels, stride, change_size = True):\n",
    "        block = [ResNet20_Block(in_channels, out_channels, stride, change_size = change_size)]\n",
    "        for _ in range(RESNET20_LAYERS - 1):\n",
    "            block.append(ResNet20_Block(out_channels, out_channels, stride = NON_DOWNSAMPLING_STRIDE, change_size = False))\n",
    "        return nn.Sequential(*block)\n",
    "    \n",
    "    # Fully connected and convolutional layers are initializated following the Kaiming method.\n",
    "    # Batch normalization layers are initialized following the J. He method.\n",
    "    def _initialize_weights(self):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Conv2d):\n",
    "                init.kaiming_normal_(module.weight, mode = 'fan_out', nonlinearity = 'relu')\n",
    "            elif isinstance(module, nn.BatchNorm2d):\n",
    "                init.constant_(module.weight, NORM_LAYER_INIT_WEIGHT)\n",
    "                init.constant_(module.bias, NORM_LAYER_INIT_BIAS)\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.block3(self.block2(self.block1(x)))\n",
    "        x = F.adaptive_avg_pool2d(x, 1)\n",
    "        x = self.fc(x.view(x.size(0), -1))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Section\n",
    "\n",
    "Function where the evaluation process of the parametric settings of the convolutional neural network will be carried out during the training procedure. The values collected in each evaluation process correspond to the comparison between the preditions and the real values, the reliability of the model and the accuracy of the predictions on failures and hits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model, loader, criterion, device):\n",
    "    \"\"\"\n",
    "    Evaluates the performance of the model on a given dataset.\n",
    "\n",
    "    Parameters\n",
    "    -------------------------------------------------------------\n",
    "    model (nn.Module): model to be evaluated.\n",
    "    loader (DataLoader): data loader containing the dataset to be evaluated.\n",
    "    criterion (callable): loss function to be used.\n",
    "    device (torch.device): device on which the operations are going to be performed.\n",
    "    \"\"\" \n",
    "    \n",
    "    if not (device and criterion): raise Exception(\"Device and criterion must be specified on eval function.\")\n",
    "    \n",
    "    model.eval()\n",
    "    accurate_predictions = 0\n",
    "    total_loss = 0\n",
    "    total_samples = 0\n",
    "    predictions = []\n",
    "    targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch, target in loader:\n",
    "            batch, target = batch.to(device), target.to(device)\n",
    "            output = model(batch)\n",
    "            total_loss += criterion(output, target).item()\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            accurate_predictions += (predicted == target).sum().item()\n",
    "            total_samples += target.size(0)\n",
    "            predictions.extend(predicted.tolist())\n",
    "            targets.extend(target.tolist())\n",
    "\n",
    "    return predictions, targets, 100 * accurate_predictions / total_samples, total_loss / len(loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save and load checkpoint functions.\n",
    "\n",
    "We define two functions, allowing us to store the weights and statistics provided by the neural network during the training process, so that, in case of an accident, we can restore the process from the last stored record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of chechpoint save function\n",
    "def save_checkpoint(model, scheduler, optimizer, epoch, tracking_lists, path = CHECKPOINT_PATH):\n",
    "    \"\"\"\n",
    "    Saves the current state of the model, optimizer and scheduler to a file.\n",
    "\n",
    "    Parameters\n",
    "    -------------------------------------------------------------\n",
    "    model (nn.Module): model to be saved.\n",
    "    scheduler (callable): scheduler to be saved.\n",
    "    optimizer (callable): optimizer to be saved.\n",
    "    epoch (int): current epoch.\n",
    "    tracking_lists (list): list of tracking values to be saved.\n",
    "    path (str): path to the file where the model is going to be saved.\n",
    "    \"\"\"\n",
    "    \n",
    "    torch.save({\n",
    "        'model': model.state_dict(),\n",
    "        'scheduler': scheduler.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'epoch': epoch,\n",
    "        'tracking': tracking_lists\n",
    "    }, path)\n",
    "\n",
    "# Definition of checkpoint load function\n",
    "def load_checkpoint(model, optimizer, scheduler, path):\n",
    "    \"\"\"\n",
    "    Loads the state of the model, optimizer and scheduler from a file.\n",
    "\n",
    "    Parameters\n",
    "    -------------------------------------------------------------\n",
    "    model (nn.Module): model to be loaded.\n",
    "    optimizer (callable): optimizer to be loaded.\n",
    "    scheduler (callable): scheduler to be loaded.\n",
    "    path (str): path to the file where the model is going to be loaded.\n",
    "    \"\"\"\n",
    "    \n",
    "    checkpoint = torch.load(path)\n",
    "    return checkpoint\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "\n",
    "Function that executes the training loop upon the neural network. The model is successively trained through a series of stages or epochs. In each of them, the parameters governing the performance of the network are reassigned according to the loss detected during the process. In addition, an evaluation of each of the iterative adjustments made to the parameters guiding the network is performed. Finally, data concerning the efficiency of the model after the training process are collected, including the hit and miss accuracies in each of the stages performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loaders, optimizer, criterion, scheduler, tracking_lists, callback, early_stopper, start_epoch, epochs = 50, device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')):\n",
    "    \"\"\"\n",
    "    Trains the model on a given dataset.\n",
    "\n",
    "    Parameters\n",
    "    -------------------------------------------------------------\n",
    "    model (nn.Module): model to be trained.\n",
    "    loaders (dict): dictionary containing the training and validation data loaders.\n",
    "    optimizer (callable): optimization algorithm to be used.\n",
    "    criterion (callable): loss function to be used.\n",
    "    epochs (int): number of epochs to train the model.\n",
    "    scheduler (callable): learning rate scheduler.\n",
    "    device (torch.device): device on which the operations are going to be performed.\n",
    "    early_stopper (callable): object in charge of stopping the training process if the validation loss does not improve.\n",
    "    callback (callable): function to be called during training loop to print statistics on display.\n",
    "    \"\"\"\n",
    "\n",
    "    if not (optimizer and criterion and early_stopper and scheduler and callback): raise Exception(\"Optimizer, criterion, early_stopper, scheduler and callback must be specified on train function.\")\n",
    "    model.to(device)\n",
    "\n",
    "    # Evaluating the model on the validation dataset before starting the training process\n",
    "    if start_epoch == 0:\n",
    "        val_predictions, val_targets, val_accuracy, val_loss = eval(model, loaders['val'], criterion, device)\n",
    "        callback(epoch = 0, val_accuracy = val_accuracy, val_loss = val_loss, train_accuracy = 0, train_loss = float('inf'))\n",
    "    \n",
    "    # Starting the training process\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        total_loss = 0\n",
    "        accurate_predictions = 0\n",
    "        total_samples = 0\n",
    "        model.train()\n",
    "\n",
    "        # Iterating through the training dataset\n",
    "        for i, (batch, target) in enumerate(loaders['train'], 1):\n",
    "            batch, target = batch.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(batch)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            accurate_predictions += (predicted == target).sum().item()\n",
    "            total_samples += target.size(0)\n",
    "            if not i % 100:\n",
    "                print(f\"\\tEpoch {epoch + 1}, Batch {i}, Loss: {loss.item()}\")\n",
    "\n",
    "        # Evaluating the model for the current epoch \n",
    "        val_predictions, val_targets, val_accuracy, val_loss = eval(model, loaders['val'], criterion, device)\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        # Analyzing the performance of the model on the current epoch\n",
    "        train_accuracy = 100 * accurate_predictions / total_samples\n",
    "        train_loss = total_loss / len(loaders['train'])\n",
    "        tracking_lists['train_accuracies'].append(train_accuracy)\n",
    "        tracking_lists['train_losses'].append(train_loss)\n",
    "        tracking_lists['eval_accuracies'].append(val_accuracy)\n",
    "        tracking_lists['eval_losses'].append(val_loss)\n",
    "\n",
    "        # Printing out the statistics of the model on the current epoch\n",
    "        callback(epoch = epoch + 1, val_accuracy = val_accuracy, val_loss = val_loss, train_accuracy = train_accuracy, train_loss = train_loss)\n",
    "\n",
    "        # Saving the model periodically\n",
    "        if not (epoch + 1) % 5:\n",
    "            save_checkpoint(model, scheduler, optimizer, epoch + 1, tracking_lists, CHECKPOINT_PATH)\n",
    "\n",
    "        # Checking if the training process should be stopped\n",
    "        if early_stopper(val_loss): break\n",
    "\n",
    "    return tracking_lists, val_predictions, val_targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Configuration And Training Launch\n",
    "\n",
    "The final step is to configure the various parameters to be used during training, in the form of optimizations, selected loss function, scheduler, learning rate, etc. The callback functions that will be used to print the model analytics during learning are defined. The model is executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adrian/Dev/PDIGS/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Train Accuracy: 0.00%, Train Loss: inf, Val Accuracy: 0.86%, Val Loss: 3.9175\n",
      "\tEpoch 1, Batch 100, Loss: 3.5017521381378174\n",
      "\tEpoch 1, Batch 200, Loss: 3.2437257766723633\n",
      "\tEpoch 1, Batch 300, Loss: 3.1331992149353027\n",
      "\tEpoch 1, Batch 400, Loss: 3.2277631759643555\n",
      "\tEpoch 1, Batch 500, Loss: 3.317502021789551\n",
      "\tEpoch 1, Batch 600, Loss: 2.9863219261169434\n",
      "\tEpoch 1, Batch 700, Loss: 2.9984569549560547\n",
      "\tEpoch 1, Batch 800, Loss: 2.8361964225769043\n",
      "\tEpoch 1, Batch 900, Loss: 2.9615871906280518\n",
      "\tEpoch 1, Batch 1000, Loss: 2.7336435317993164\n",
      "\tEpoch 1, Batch 1100, Loss: 2.6556286811828613\n",
      "\tEpoch 1, Batch 1200, Loss: 2.772244930267334\n",
      "\tEpoch 1, Batch 1300, Loss: 2.76749849319458\n",
      "\tEpoch 1, Batch 1400, Loss: 2.6954855918884277\n",
      "\tEpoch 1, Batch 1500, Loss: 2.6448585987091064\n",
      "\tEpoch 1, Batch 1600, Loss: 2.8177878856658936\n",
      "\tEpoch 1, Batch 1700, Loss: 2.9305520057678223\n",
      "\tEpoch 1, Batch 1800, Loss: 2.78593111038208\n",
      "\tEpoch 1, Batch 1900, Loss: 2.453545570373535\n",
      "\tEpoch 1, Batch 2000, Loss: 2.541017532348633\n",
      "\tEpoch 1, Batch 2100, Loss: 2.1434178352355957\n",
      "Epoch: 1, Train Accuracy: 26.45%, Train Loss: 2.8690, Val Accuracy: 32.88%, Val Loss: 2.5188\n",
      "\tEpoch 2, Batch 100, Loss: 2.437174081802368\n",
      "\tEpoch 2, Batch 200, Loss: 2.2792370319366455\n",
      "\tEpoch 2, Batch 300, Loss: 2.3504300117492676\n",
      "\tEpoch 2, Batch 400, Loss: 2.626126766204834\n",
      "\tEpoch 2, Batch 500, Loss: 2.4135706424713135\n",
      "\tEpoch 2, Batch 600, Loss: 2.0101613998413086\n",
      "\tEpoch 2, Batch 700, Loss: 2.0753164291381836\n",
      "\tEpoch 2, Batch 800, Loss: 2.102844476699829\n",
      "\tEpoch 2, Batch 900, Loss: 2.4050538539886475\n",
      "\tEpoch 2, Batch 1000, Loss: 2.2607553005218506\n",
      "\tEpoch 2, Batch 1100, Loss: 2.1348955631256104\n",
      "\tEpoch 2, Batch 1200, Loss: 2.249276876449585\n",
      "\tEpoch 2, Batch 1300, Loss: 2.4521069526672363\n",
      "\tEpoch 2, Batch 1400, Loss: 2.011019229888916\n",
      "\tEpoch 2, Batch 1500, Loss: 2.1336758136749268\n",
      "\tEpoch 2, Batch 1600, Loss: 2.0124521255493164\n",
      "\tEpoch 2, Batch 1700, Loss: 2.4817659854888916\n",
      "\tEpoch 2, Batch 1800, Loss: 2.259044647216797\n",
      "\tEpoch 2, Batch 1900, Loss: 2.377403497695923\n",
      "\tEpoch 2, Batch 2000, Loss: 2.167205810546875\n",
      "\tEpoch 2, Batch 2100, Loss: 2.423123598098755\n",
      "Epoch: 2, Train Accuracy: 39.08%, Train Loss: 2.2885, Val Accuracy: 40.69%, Val Loss: 2.1645\n",
      "\tEpoch 3, Batch 100, Loss: 2.468015193939209\n",
      "\tEpoch 3, Batch 200, Loss: 2.3299319744110107\n",
      "\tEpoch 3, Batch 300, Loss: 2.07770037651062\n",
      "\tEpoch 3, Batch 400, Loss: 1.7841018438339233\n",
      "\tEpoch 3, Batch 500, Loss: 2.539512872695923\n",
      "\tEpoch 3, Batch 600, Loss: 2.1738126277923584\n",
      "\tEpoch 3, Batch 700, Loss: 1.9598158597946167\n",
      "\tEpoch 3, Batch 800, Loss: 2.0994672775268555\n",
      "\tEpoch 3, Batch 900, Loss: 2.0472350120544434\n",
      "\tEpoch 3, Batch 1000, Loss: 1.8899863958358765\n",
      "\tEpoch 3, Batch 1100, Loss: 2.0136659145355225\n",
      "\tEpoch 3, Batch 1200, Loss: 1.603248953819275\n",
      "\tEpoch 3, Batch 1300, Loss: 1.8454095125198364\n",
      "\tEpoch 3, Batch 1400, Loss: 1.8640105724334717\n",
      "\tEpoch 3, Batch 1500, Loss: 2.071443796157837\n",
      "\tEpoch 3, Batch 1600, Loss: 2.250683307647705\n",
      "\tEpoch 3, Batch 1700, Loss: 2.102726936340332\n",
      "\tEpoch 3, Batch 1800, Loss: 1.4967997074127197\n",
      "\tEpoch 3, Batch 1900, Loss: 1.7644622325897217\n",
      "\tEpoch 3, Batch 2000, Loss: 1.8310273885726929\n",
      "\tEpoch 3, Batch 2100, Loss: 1.8835656642913818\n",
      "Epoch: 3, Train Accuracy: 47.78%, Train Loss: 1.9370, Val Accuracy: 50.63%, Val Loss: 1.7985\n",
      "\tEpoch 4, Batch 100, Loss: 1.6535961627960205\n",
      "\tEpoch 4, Batch 200, Loss: 1.7154028415679932\n",
      "\tEpoch 4, Batch 300, Loss: 2.018301010131836\n",
      "\tEpoch 4, Batch 400, Loss: 1.71306312084198\n",
      "\tEpoch 4, Batch 500, Loss: 1.8815206289291382\n",
      "\tEpoch 4, Batch 600, Loss: 1.3555876016616821\n",
      "\tEpoch 4, Batch 700, Loss: 1.8402950763702393\n",
      "\tEpoch 4, Batch 800, Loss: 2.0469982624053955\n",
      "\tEpoch 4, Batch 900, Loss: 1.3534624576568604\n",
      "\tEpoch 4, Batch 1000, Loss: 1.6147054433822632\n",
      "\tEpoch 4, Batch 1100, Loss: 1.7372043132781982\n",
      "\tEpoch 4, Batch 1200, Loss: 1.7957167625427246\n",
      "\tEpoch 4, Batch 1300, Loss: 2.041290521621704\n",
      "\tEpoch 4, Batch 1400, Loss: 1.612620234489441\n",
      "\tEpoch 4, Batch 1500, Loss: 1.4304955005645752\n",
      "\tEpoch 4, Batch 1600, Loss: 1.7626900672912598\n",
      "\tEpoch 4, Batch 1700, Loss: 1.8734122514724731\n",
      "\tEpoch 4, Batch 1800, Loss: 1.784287452697754\n",
      "\tEpoch 4, Batch 1900, Loss: 1.573820948600769\n",
      "\tEpoch 4, Batch 2000, Loss: 1.8342770338058472\n",
      "\tEpoch 4, Batch 2100, Loss: 1.33955717086792\n",
      "Epoch: 4, Train Accuracy: 54.52%, Train Loss: 1.6649, Val Accuracy: 49.20%, Val Loss: 1.8438\n",
      "\tEpoch 5, Batch 100, Loss: 1.3665552139282227\n",
      "\tEpoch 5, Batch 200, Loss: 1.6912657022476196\n",
      "\tEpoch 5, Batch 300, Loss: 1.7117536067962646\n",
      "\tEpoch 5, Batch 400, Loss: 1.566206693649292\n",
      "\tEpoch 5, Batch 500, Loss: 1.3087694644927979\n",
      "\tEpoch 5, Batch 600, Loss: 1.4025557041168213\n",
      "\tEpoch 5, Batch 700, Loss: 1.50782310962677\n",
      "\tEpoch 5, Batch 800, Loss: 1.5339007377624512\n",
      "\tEpoch 5, Batch 900, Loss: 1.5211368799209595\n",
      "\tEpoch 5, Batch 1000, Loss: 1.2165628671646118\n",
      "\tEpoch 5, Batch 1100, Loss: 1.5086334943771362\n",
      "\tEpoch 5, Batch 1200, Loss: 1.4464491605758667\n",
      "\tEpoch 5, Batch 1300, Loss: 1.364677906036377\n",
      "\tEpoch 5, Batch 1400, Loss: 1.2704583406448364\n",
      "\tEpoch 5, Batch 1500, Loss: 1.365085482597351\n",
      "\tEpoch 5, Batch 1600, Loss: 1.76324462890625\n",
      "\tEpoch 5, Batch 1700, Loss: 1.5263550281524658\n",
      "\tEpoch 5, Batch 1800, Loss: 1.4546077251434326\n",
      "\tEpoch 5, Batch 1900, Loss: 1.4669075012207031\n",
      "\tEpoch 5, Batch 2000, Loss: 1.2905863523483276\n",
      "\tEpoch 5, Batch 2100, Loss: 1.5079855918884277\n",
      "Epoch: 5, Train Accuracy: 59.56%, Train Loss: 1.4716, Val Accuracy: 58.90%, Val Loss: 1.4953\n",
      "\tEpoch 6, Batch 100, Loss: 1.260063648223877\n",
      "\tEpoch 6, Batch 200, Loss: 1.3165278434753418\n",
      "\tEpoch 6, Batch 300, Loss: 1.2544057369232178\n",
      "\tEpoch 6, Batch 400, Loss: 1.7429771423339844\n",
      "\tEpoch 6, Batch 500, Loss: 1.3105705976486206\n",
      "\tEpoch 6, Batch 600, Loss: 1.4962265491485596\n",
      "\tEpoch 6, Batch 700, Loss: 1.4328553676605225\n",
      "\tEpoch 6, Batch 800, Loss: 1.1876212358474731\n",
      "\tEpoch 6, Batch 900, Loss: 1.3314186334609985\n",
      "\tEpoch 6, Batch 1000, Loss: 1.492357611656189\n",
      "\tEpoch 6, Batch 1100, Loss: 1.2876527309417725\n",
      "\tEpoch 6, Batch 1200, Loss: 1.3864367008209229\n",
      "\tEpoch 6, Batch 1300, Loss: 1.4775668382644653\n",
      "\tEpoch 6, Batch 1400, Loss: 1.4384546279907227\n",
      "\tEpoch 6, Batch 1500, Loss: 1.6292145252227783\n",
      "\tEpoch 6, Batch 1600, Loss: 1.5146139860153198\n",
      "\tEpoch 6, Batch 1700, Loss: 1.3196897506713867\n",
      "\tEpoch 6, Batch 1800, Loss: 1.0989327430725098\n",
      "\tEpoch 6, Batch 1900, Loss: 1.1326583623886108\n",
      "\tEpoch 6, Batch 2000, Loss: 1.3540713787078857\n",
      "\tEpoch 6, Batch 2100, Loss: 1.1322396993637085\n",
      "Epoch: 6, Train Accuracy: 63.40%, Train Loss: 1.3269, Val Accuracy: 62.09%, Val Loss: 1.3723\n",
      "\tEpoch 7, Batch 100, Loss: 1.461580753326416\n",
      "\tEpoch 7, Batch 200, Loss: 0.960368812084198\n",
      "\tEpoch 7, Batch 300, Loss: 1.0825153589248657\n",
      "\tEpoch 7, Batch 400, Loss: 1.3159610033035278\n",
      "\tEpoch 7, Batch 500, Loss: 1.4119689464569092\n",
      "\tEpoch 7, Batch 600, Loss: 1.2776800394058228\n",
      "\tEpoch 7, Batch 700, Loss: 1.3104065656661987\n",
      "\tEpoch 7, Batch 800, Loss: 1.2071032524108887\n",
      "\tEpoch 7, Batch 900, Loss: 0.879337728023529\n",
      "\tEpoch 7, Batch 1000, Loss: 1.225597620010376\n",
      "\tEpoch 7, Batch 1100, Loss: 1.2416300773620605\n",
      "\tEpoch 7, Batch 1200, Loss: 1.1267808675765991\n",
      "\tEpoch 7, Batch 1300, Loss: 1.1708853244781494\n",
      "\tEpoch 7, Batch 1400, Loss: 1.325716495513916\n",
      "\tEpoch 7, Batch 1500, Loss: 1.1663936376571655\n",
      "\tEpoch 7, Batch 1600, Loss: 1.2080719470977783\n",
      "\tEpoch 7, Batch 1700, Loss: 0.982275128364563\n",
      "\tEpoch 7, Batch 1800, Loss: 1.0663361549377441\n",
      "\tEpoch 7, Batch 1900, Loss: 1.3773980140686035\n",
      "\tEpoch 7, Batch 2000, Loss: 1.1973294019699097\n",
      "\tEpoch 7, Batch 2100, Loss: 0.9413703680038452\n",
      "Epoch: 7, Train Accuracy: 66.59%, Train Loss: 1.2092, Val Accuracy: 66.11%, Val Loss: 1.1955\n",
      "\tEpoch 8, Batch 100, Loss: 0.8947188258171082\n",
      "\tEpoch 8, Batch 200, Loss: 1.1869449615478516\n",
      "\tEpoch 8, Batch 300, Loss: 1.004878044128418\n",
      "\tEpoch 8, Batch 400, Loss: 1.3421523571014404\n",
      "\tEpoch 8, Batch 500, Loss: 1.1058937311172485\n",
      "\tEpoch 8, Batch 600, Loss: 1.0916193723678589\n",
      "\tEpoch 8, Batch 700, Loss: 1.0303503274917603\n",
      "\tEpoch 8, Batch 800, Loss: 1.4073079824447632\n",
      "\tEpoch 8, Batch 900, Loss: 1.1756861209869385\n",
      "\tEpoch 8, Batch 1000, Loss: 1.0145502090454102\n",
      "\tEpoch 8, Batch 1100, Loss: 1.2501283884048462\n",
      "\tEpoch 8, Batch 1200, Loss: 1.0562702417373657\n",
      "\tEpoch 8, Batch 1300, Loss: 1.132973551750183\n",
      "\tEpoch 8, Batch 1400, Loss: 1.2026005983352661\n",
      "\tEpoch 8, Batch 1500, Loss: 1.1972311735153198\n",
      "\tEpoch 8, Batch 1600, Loss: 1.2326582670211792\n",
      "\tEpoch 8, Batch 1700, Loss: 1.1429569721221924\n",
      "\tEpoch 8, Batch 1800, Loss: 1.0568584203720093\n",
      "\tEpoch 8, Batch 1900, Loss: 1.0242674350738525\n",
      "\tEpoch 8, Batch 2000, Loss: 1.0757901668548584\n",
      "\tEpoch 8, Batch 2100, Loss: 0.8996171951293945\n",
      "Epoch: 8, Train Accuracy: 68.94%, Train Loss: 1.1170, Val Accuracy: 63.28%, Val Loss: 1.3194\n",
      "\tEpoch 9, Batch 100, Loss: 1.3080644607543945\n",
      "\tEpoch 9, Batch 200, Loss: 0.780552864074707\n",
      "\tEpoch 9, Batch 300, Loss: 1.1345369815826416\n",
      "\tEpoch 9, Batch 400, Loss: 1.0476393699645996\n",
      "\tEpoch 9, Batch 500, Loss: 1.0570228099822998\n",
      "\tEpoch 9, Batch 600, Loss: 0.9177260398864746\n",
      "\tEpoch 9, Batch 700, Loss: 0.9781449437141418\n",
      "\tEpoch 9, Batch 800, Loss: 1.1611158847808838\n",
      "\tEpoch 9, Batch 900, Loss: 0.9063704609870911\n",
      "\tEpoch 9, Batch 1000, Loss: 0.9142090678215027\n",
      "\tEpoch 9, Batch 1100, Loss: 1.1334137916564941\n",
      "\tEpoch 9, Batch 1200, Loss: 0.7792490124702454\n",
      "\tEpoch 9, Batch 1300, Loss: 0.973309338092804\n",
      "\tEpoch 9, Batch 1400, Loss: 1.0555108785629272\n",
      "\tEpoch 9, Batch 1500, Loss: 1.0054947137832642\n",
      "\tEpoch 9, Batch 1600, Loss: 1.2108371257781982\n",
      "\tEpoch 9, Batch 1700, Loss: 0.8928462266921997\n",
      "\tEpoch 9, Batch 1800, Loss: 0.9970011115074158\n",
      "\tEpoch 9, Batch 1900, Loss: 1.0640435218811035\n",
      "\tEpoch 9, Batch 2000, Loss: 0.943291187286377\n",
      "\tEpoch 9, Batch 2100, Loss: 1.071832299232483\n",
      "Epoch: 9, Train Accuracy: 71.35%, Train Loss: 1.0311, Val Accuracy: 69.66%, Val Loss: 1.0666\n",
      "\tEpoch 10, Batch 100, Loss: 0.9380913972854614\n",
      "\tEpoch 10, Batch 200, Loss: 0.9889286160469055\n",
      "\tEpoch 10, Batch 300, Loss: 1.0815143585205078\n",
      "\tEpoch 10, Batch 400, Loss: 1.00625741481781\n",
      "\tEpoch 10, Batch 500, Loss: 1.1492747068405151\n",
      "\tEpoch 10, Batch 600, Loss: 0.9125524163246155\n",
      "\tEpoch 10, Batch 700, Loss: 1.0023763179779053\n",
      "\tEpoch 10, Batch 800, Loss: 1.0409603118896484\n",
      "\tEpoch 10, Batch 900, Loss: 1.0559918880462646\n",
      "\tEpoch 10, Batch 1000, Loss: 1.0478686094284058\n",
      "\tEpoch 10, Batch 1100, Loss: 0.9814060926437378\n",
      "\tEpoch 10, Batch 1200, Loss: 1.1513044834136963\n",
      "\tEpoch 10, Batch 1300, Loss: 0.7262089252471924\n",
      "\tEpoch 10, Batch 1400, Loss: 0.7784544825553894\n",
      "\tEpoch 10, Batch 1500, Loss: 0.7563758492469788\n",
      "\tEpoch 10, Batch 1600, Loss: 1.1109405755996704\n",
      "\tEpoch 10, Batch 1700, Loss: 1.0729539394378662\n",
      "\tEpoch 10, Batch 1800, Loss: 1.0112398862838745\n",
      "\tEpoch 10, Batch 1900, Loss: 0.790128767490387\n",
      "\tEpoch 10, Batch 2000, Loss: 1.1857424974441528\n",
      "\tEpoch 10, Batch 2100, Loss: 0.6284875273704529\n",
      "Epoch: 10, Train Accuracy: 73.14%, Train Loss: 0.9638, Val Accuracy: 68.85%, Val Loss: 1.0974\n",
      "\tEpoch 11, Batch 100, Loss: 1.1614850759506226\n",
      "\tEpoch 11, Batch 200, Loss: 0.9941264390945435\n",
      "\tEpoch 11, Batch 300, Loss: 1.0666066408157349\n",
      "\tEpoch 11, Batch 400, Loss: 0.978895902633667\n",
      "\tEpoch 11, Batch 500, Loss: 0.5350630879402161\n",
      "\tEpoch 11, Batch 600, Loss: 1.005784511566162\n",
      "\tEpoch 11, Batch 700, Loss: 0.857826292514801\n",
      "\tEpoch 11, Batch 800, Loss: 0.9750840663909912\n",
      "\tEpoch 11, Batch 900, Loss: 0.8402945399284363\n",
      "\tEpoch 11, Batch 1000, Loss: 0.710610568523407\n",
      "\tEpoch 11, Batch 1100, Loss: 1.0179898738861084\n",
      "\tEpoch 11, Batch 1200, Loss: 0.8438971638679504\n",
      "\tEpoch 11, Batch 1300, Loss: 0.8669586181640625\n",
      "\tEpoch 11, Batch 1400, Loss: 0.8357119560241699\n",
      "\tEpoch 11, Batch 1500, Loss: 0.8555685877799988\n",
      "\tEpoch 11, Batch 1600, Loss: 0.9019148349761963\n",
      "\tEpoch 11, Batch 1700, Loss: 0.8245816230773926\n",
      "\tEpoch 11, Batch 1800, Loss: 0.892449140548706\n",
      "\tEpoch 11, Batch 1900, Loss: 0.5992617011070251\n",
      "\tEpoch 11, Batch 2000, Loss: 0.8818524479866028\n",
      "\tEpoch 11, Batch 2100, Loss: 0.8529853224754333\n",
      "Epoch: 11, Train Accuracy: 74.67%, Train Loss: 0.9095, Val Accuracy: 66.32%, Val Loss: 1.2560\n",
      "\tEpoch 12, Batch 100, Loss: 0.6891524195671082\n",
      "\tEpoch 12, Batch 200, Loss: 0.8460963368415833\n",
      "\tEpoch 12, Batch 300, Loss: 1.13554847240448\n",
      "\tEpoch 12, Batch 400, Loss: 1.0350911617279053\n",
      "\tEpoch 12, Batch 500, Loss: 1.0857445001602173\n",
      "\tEpoch 12, Batch 600, Loss: 0.7184174656867981\n",
      "\tEpoch 12, Batch 700, Loss: 1.0727190971374512\n",
      "\tEpoch 12, Batch 800, Loss: 0.9171220064163208\n",
      "\tEpoch 12, Batch 900, Loss: 0.870091438293457\n",
      "\tEpoch 12, Batch 1000, Loss: 0.9672216773033142\n",
      "\tEpoch 12, Batch 1100, Loss: 0.9629920125007629\n",
      "\tEpoch 12, Batch 1200, Loss: 0.8196123242378235\n",
      "\tEpoch 12, Batch 1300, Loss: 0.7634139060974121\n",
      "\tEpoch 12, Batch 1400, Loss: 1.0503339767456055\n",
      "\tEpoch 12, Batch 1500, Loss: 1.0376641750335693\n",
      "\tEpoch 12, Batch 1600, Loss: 0.5580157041549683\n",
      "\tEpoch 12, Batch 1700, Loss: 0.8299477696418762\n",
      "\tEpoch 12, Batch 1800, Loss: 1.2607346773147583\n",
      "\tEpoch 12, Batch 1900, Loss: 0.952923059463501\n",
      "\tEpoch 12, Batch 2000, Loss: 0.7044050097465515\n",
      "\tEpoch 12, Batch 2100, Loss: 0.9549033045768738\n",
      "Epoch: 12, Train Accuracy: 75.80%, Train Loss: 0.8637, Val Accuracy: 70.29%, Val Loss: 1.0605\n",
      "\tEpoch 13, Batch 100, Loss: 0.7214136123657227\n",
      "\tEpoch 13, Batch 200, Loss: 0.8249030113220215\n",
      "\tEpoch 13, Batch 300, Loss: 0.5788865089416504\n",
      "\tEpoch 13, Batch 400, Loss: 0.6728906631469727\n",
      "\tEpoch 13, Batch 500, Loss: 0.663742184638977\n",
      "\tEpoch 13, Batch 600, Loss: 0.7488436102867126\n",
      "\tEpoch 13, Batch 700, Loss: 0.6969836354255676\n",
      "\tEpoch 13, Batch 800, Loss: 0.6802235841751099\n",
      "\tEpoch 13, Batch 900, Loss: 1.0127030611038208\n",
      "\tEpoch 13, Batch 1000, Loss: 0.8166031837463379\n",
      "\tEpoch 13, Batch 1100, Loss: 0.9603915810585022\n",
      "\tEpoch 13, Batch 1200, Loss: 1.0545119047164917\n",
      "\tEpoch 13, Batch 1300, Loss: 0.6312625408172607\n",
      "\tEpoch 13, Batch 1400, Loss: 0.837065577507019\n",
      "\tEpoch 13, Batch 1500, Loss: 0.9351431131362915\n",
      "\tEpoch 13, Batch 1600, Loss: 0.6773036122322083\n",
      "\tEpoch 13, Batch 1700, Loss: 0.7314708232879639\n",
      "\tEpoch 13, Batch 1800, Loss: 0.6795065402984619\n",
      "\tEpoch 13, Batch 1900, Loss: 0.8318194150924683\n",
      "\tEpoch 13, Batch 2000, Loss: 0.46963608264923096\n",
      "\tEpoch 13, Batch 2100, Loss: 1.0376662015914917\n",
      "Epoch: 13, Train Accuracy: 76.99%, Train Loss: 0.8191, Val Accuracy: 74.89%, Val Loss: 0.9211\n",
      "\tEpoch 14, Batch 100, Loss: 0.767836332321167\n",
      "\tEpoch 14, Batch 200, Loss: 0.6455757021903992\n",
      "\tEpoch 14, Batch 300, Loss: 0.7052626013755798\n",
      "\tEpoch 14, Batch 400, Loss: 0.8636014461517334\n",
      "\tEpoch 14, Batch 500, Loss: 0.67215496301651\n",
      "\tEpoch 14, Batch 600, Loss: 0.9821958541870117\n",
      "\tEpoch 14, Batch 700, Loss: 0.7515178322792053\n",
      "\tEpoch 14, Batch 800, Loss: 0.7815313935279846\n",
      "\tEpoch 14, Batch 900, Loss: 0.8653663992881775\n",
      "\tEpoch 14, Batch 1000, Loss: 0.916581392288208\n",
      "\tEpoch 14, Batch 1100, Loss: 0.5666560530662537\n",
      "\tEpoch 14, Batch 1200, Loss: 0.718921422958374\n",
      "\tEpoch 14, Batch 1300, Loss: 0.6997421383857727\n",
      "\tEpoch 14, Batch 1400, Loss: 0.9938361644744873\n",
      "\tEpoch 14, Batch 1500, Loss: 0.6214636564254761\n",
      "\tEpoch 14, Batch 1600, Loss: 0.7333775758743286\n",
      "\tEpoch 14, Batch 1700, Loss: 0.6697138547897339\n",
      "\tEpoch 14, Batch 1800, Loss: 0.9283125996589661\n",
      "\tEpoch 14, Batch 1900, Loss: 0.6991607546806335\n",
      "\tEpoch 14, Batch 2000, Loss: 0.7479585409164429\n",
      "\tEpoch 14, Batch 2100, Loss: 0.9969860911369324\n",
      "Epoch: 14, Train Accuracy: 78.10%, Train Loss: 0.7839, Val Accuracy: 78.10%, Val Loss: 0.7758\n",
      "\tEpoch 15, Batch 100, Loss: 0.8267250657081604\n",
      "\tEpoch 15, Batch 200, Loss: 0.5960288643836975\n",
      "\tEpoch 15, Batch 300, Loss: 0.7311933636665344\n",
      "\tEpoch 15, Batch 400, Loss: 0.6395771503448486\n",
      "\tEpoch 15, Batch 500, Loss: 0.6707392930984497\n",
      "\tEpoch 15, Batch 600, Loss: 0.49785181879997253\n",
      "\tEpoch 15, Batch 700, Loss: 0.631981611251831\n",
      "\tEpoch 15, Batch 800, Loss: 0.442849725484848\n",
      "\tEpoch 15, Batch 900, Loss: 0.5727511644363403\n",
      "\tEpoch 15, Batch 1000, Loss: 0.7894957065582275\n",
      "\tEpoch 15, Batch 1100, Loss: 0.6624002456665039\n",
      "\tEpoch 15, Batch 1200, Loss: 0.4012047052383423\n",
      "\tEpoch 15, Batch 1300, Loss: 0.6635911464691162\n",
      "\tEpoch 15, Batch 1400, Loss: 0.9578787684440613\n",
      "\tEpoch 15, Batch 1500, Loss: 0.788003146648407\n",
      "\tEpoch 15, Batch 1600, Loss: 0.6901302337646484\n",
      "\tEpoch 15, Batch 1700, Loss: 0.6666147708892822\n",
      "\tEpoch 15, Batch 1800, Loss: 0.8811147809028625\n",
      "\tEpoch 15, Batch 1900, Loss: 0.563646137714386\n",
      "\tEpoch 15, Batch 2000, Loss: 0.7284025549888611\n",
      "\tEpoch 15, Batch 2100, Loss: 0.9215826988220215\n",
      "Epoch: 15, Train Accuracy: 78.92%, Train Loss: 0.7540, Val Accuracy: 72.74%, Val Loss: 0.9952\n",
      "\tEpoch 16, Batch 100, Loss: 0.5338363647460938\n",
      "\tEpoch 16, Batch 200, Loss: 0.5655770301818848\n",
      "\tEpoch 16, Batch 300, Loss: 0.8085424900054932\n",
      "\tEpoch 16, Batch 400, Loss: 0.7648131251335144\n",
      "\tEpoch 16, Batch 500, Loss: 0.9516692161560059\n",
      "\tEpoch 16, Batch 600, Loss: 0.9133784174919128\n",
      "\tEpoch 16, Batch 700, Loss: 0.6164568066596985\n",
      "\tEpoch 16, Batch 800, Loss: 0.954984188079834\n",
      "\tEpoch 16, Batch 900, Loss: 0.6139343976974487\n",
      "\tEpoch 16, Batch 1000, Loss: 0.6697102785110474\n",
      "\tEpoch 16, Batch 1100, Loss: 0.6251458525657654\n",
      "\tEpoch 16, Batch 1200, Loss: 0.410189151763916\n",
      "\tEpoch 16, Batch 1300, Loss: 0.6549534201622009\n",
      "\tEpoch 16, Batch 1400, Loss: 0.5908046960830688\n",
      "\tEpoch 16, Batch 1500, Loss: 0.588630735874176\n",
      "\tEpoch 16, Batch 1600, Loss: 0.757024884223938\n",
      "\tEpoch 16, Batch 1700, Loss: 0.5885796546936035\n",
      "\tEpoch 16, Batch 1800, Loss: 0.5369123220443726\n",
      "\tEpoch 16, Batch 1900, Loss: 0.9446092844009399\n",
      "\tEpoch 16, Batch 2000, Loss: 1.0328601598739624\n",
      "\tEpoch 16, Batch 2100, Loss: 0.7001560926437378\n",
      "Epoch: 16, Train Accuracy: 79.72%, Train Loss: 0.7244, Val Accuracy: 70.67%, Val Loss: 1.1158\n",
      "\tEpoch 17, Batch 100, Loss: 0.6515330672264099\n",
      "\tEpoch 17, Batch 200, Loss: 0.7952901124954224\n",
      "\tEpoch 17, Batch 300, Loss: 0.7068023085594177\n",
      "\tEpoch 17, Batch 400, Loss: 0.6361150145530701\n",
      "\tEpoch 17, Batch 500, Loss: 0.5818411707878113\n",
      "\tEpoch 17, Batch 600, Loss: 0.39908677339553833\n",
      "\tEpoch 17, Batch 700, Loss: 0.7165536880493164\n",
      "\tEpoch 17, Batch 800, Loss: 0.7200554013252258\n",
      "\tEpoch 17, Batch 900, Loss: 0.756229043006897\n",
      "\tEpoch 17, Batch 1000, Loss: 0.8182629942893982\n",
      "\tEpoch 17, Batch 1100, Loss: 0.4111696779727936\n",
      "\tEpoch 17, Batch 1200, Loss: 0.6226649284362793\n",
      "\tEpoch 17, Batch 1300, Loss: 0.9151020050048828\n",
      "\tEpoch 17, Batch 1400, Loss: 0.4965360760688782\n",
      "\tEpoch 17, Batch 1500, Loss: 0.6343066692352295\n",
      "\tEpoch 17, Batch 1600, Loss: 0.6504131555557251\n",
      "\tEpoch 17, Batch 1700, Loss: 0.8252981901168823\n",
      "\tEpoch 17, Batch 1800, Loss: 0.626845121383667\n",
      "\tEpoch 17, Batch 1900, Loss: 0.6501659750938416\n",
      "\tEpoch 17, Batch 2000, Loss: 0.6020824909210205\n",
      "\tEpoch 17, Batch 2100, Loss: 0.711082398891449\n",
      "Epoch: 17, Train Accuracy: 80.35%, Train Loss: 0.6997, Val Accuracy: 78.15%, Val Loss: 0.7818\n",
      "\tEpoch 18, Batch 100, Loss: 0.746079683303833\n",
      "\tEpoch 18, Batch 200, Loss: 0.6353898644447327\n",
      "\tEpoch 18, Batch 300, Loss: 0.6811491250991821\n",
      "\tEpoch 18, Batch 400, Loss: 0.5672463178634644\n",
      "\tEpoch 18, Batch 500, Loss: 0.4979913830757141\n",
      "\tEpoch 18, Batch 600, Loss: 0.8965396285057068\n",
      "\tEpoch 18, Batch 700, Loss: 0.5568640232086182\n",
      "\tEpoch 18, Batch 800, Loss: 0.6796056032180786\n",
      "\tEpoch 18, Batch 900, Loss: 0.6201489567756653\n",
      "\tEpoch 18, Batch 1000, Loss: 0.5674440860748291\n",
      "\tEpoch 18, Batch 1100, Loss: 0.8138018846511841\n",
      "\tEpoch 18, Batch 1200, Loss: 0.644149661064148\n",
      "\tEpoch 18, Batch 1300, Loss: 0.6807454228401184\n",
      "\tEpoch 18, Batch 1400, Loss: 0.49170753359794617\n",
      "\tEpoch 18, Batch 1500, Loss: 0.7929757833480835\n",
      "\tEpoch 18, Batch 1600, Loss: 0.6583786606788635\n",
      "\tEpoch 18, Batch 1700, Loss: 0.6022331118583679\n",
      "\tEpoch 18, Batch 1800, Loss: 0.8695715069770813\n",
      "\tEpoch 18, Batch 1900, Loss: 0.5225131511688232\n",
      "\tEpoch 18, Batch 2000, Loss: 0.6588934063911438\n",
      "\tEpoch 18, Batch 2100, Loss: 0.6709455251693726\n",
      "Epoch: 18, Train Accuracy: 81.03%, Train Loss: 0.6729, Val Accuracy: 79.86%, Val Loss: 0.7305\n",
      "\tEpoch 19, Batch 100, Loss: 0.6992170214653015\n",
      "\tEpoch 19, Batch 200, Loss: 0.552520751953125\n",
      "\tEpoch 19, Batch 300, Loss: 0.5252355337142944\n",
      "\tEpoch 19, Batch 400, Loss: 0.7688268423080444\n",
      "\tEpoch 19, Batch 500, Loss: 0.5281595587730408\n",
      "\tEpoch 19, Batch 600, Loss: 0.5394705533981323\n",
      "\tEpoch 19, Batch 700, Loss: 0.6026960611343384\n",
      "\tEpoch 19, Batch 800, Loss: 0.5109701752662659\n",
      "\tEpoch 19, Batch 900, Loss: 0.8133438229560852\n",
      "\tEpoch 19, Batch 1000, Loss: 0.6480280756950378\n",
      "\tEpoch 19, Batch 1100, Loss: 0.6378751993179321\n",
      "\tEpoch 19, Batch 1200, Loss: 0.61624675989151\n",
      "\tEpoch 19, Batch 1300, Loss: 0.6177831888198853\n",
      "\tEpoch 19, Batch 1400, Loss: 0.662956714630127\n",
      "\tEpoch 19, Batch 1500, Loss: 0.5045112371444702\n",
      "\tEpoch 19, Batch 1600, Loss: 0.7644826769828796\n",
      "\tEpoch 19, Batch 1700, Loss: 0.7484530210494995\n",
      "\tEpoch 19, Batch 1800, Loss: 0.9265465140342712\n",
      "\tEpoch 19, Batch 1900, Loss: 0.7643147110939026\n",
      "\tEpoch 19, Batch 2000, Loss: 0.730728805065155\n",
      "\tEpoch 19, Batch 2100, Loss: 0.5701923370361328\n",
      "Epoch: 19, Train Accuracy: 81.61%, Train Loss: 0.6534, Val Accuracy: 80.47%, Val Loss: 0.6976\n",
      "\tEpoch 20, Batch 100, Loss: 0.6980540752410889\n",
      "\tEpoch 20, Batch 200, Loss: 0.8904876708984375\n",
      "\tEpoch 20, Batch 300, Loss: 0.8119724988937378\n",
      "\tEpoch 20, Batch 400, Loss: 0.44415283203125\n",
      "\tEpoch 20, Batch 500, Loss: 0.5511060953140259\n",
      "\tEpoch 20, Batch 600, Loss: 0.34996840357780457\n",
      "\tEpoch 20, Batch 700, Loss: 0.5499646067619324\n",
      "\tEpoch 20, Batch 800, Loss: 0.824190080165863\n",
      "\tEpoch 20, Batch 900, Loss: 0.4183235466480255\n",
      "\tEpoch 20, Batch 1000, Loss: 0.46398022770881653\n",
      "\tEpoch 20, Batch 1100, Loss: 0.48822686076164246\n",
      "\tEpoch 20, Batch 1200, Loss: 0.7239896655082703\n",
      "\tEpoch 20, Batch 1300, Loss: 0.6997790336608887\n",
      "\tEpoch 20, Batch 1400, Loss: 0.7546124458312988\n",
      "\tEpoch 20, Batch 1500, Loss: 0.41815611720085144\n",
      "\tEpoch 20, Batch 1600, Loss: 0.6089575886726379\n",
      "\tEpoch 20, Batch 1700, Loss: 0.6940575242042542\n",
      "\tEpoch 20, Batch 1800, Loss: 0.7282426357269287\n",
      "\tEpoch 20, Batch 1900, Loss: 0.5005013942718506\n",
      "\tEpoch 20, Batch 2000, Loss: 0.9074926376342773\n",
      "\tEpoch 20, Batch 2100, Loss: 0.9095860123634338\n",
      "Epoch: 20, Train Accuracy: 82.08%, Train Loss: 0.6354, Val Accuracy: 76.72%, Val Loss: 0.8434\n",
      "\tEpoch 21, Batch 100, Loss: 0.6788730621337891\n",
      "\tEpoch 21, Batch 200, Loss: 0.4908865690231323\n",
      "\tEpoch 21, Batch 300, Loss: 0.5676341652870178\n",
      "\tEpoch 21, Batch 400, Loss: 0.8084530234336853\n",
      "\tEpoch 21, Batch 500, Loss: 0.6101821660995483\n",
      "\tEpoch 21, Batch 600, Loss: 0.8024613261222839\n",
      "\tEpoch 21, Batch 700, Loss: 0.5926862359046936\n",
      "\tEpoch 21, Batch 800, Loss: 0.6141064167022705\n",
      "\tEpoch 21, Batch 900, Loss: 0.4829216003417969\n",
      "\tEpoch 21, Batch 1000, Loss: 0.6271222233772278\n",
      "\tEpoch 21, Batch 1100, Loss: 0.8959644436836243\n",
      "\tEpoch 21, Batch 1200, Loss: 0.46255192160606384\n",
      "\tEpoch 21, Batch 1300, Loss: 0.5285013318061829\n",
      "\tEpoch 21, Batch 1400, Loss: 0.964547872543335\n",
      "\tEpoch 21, Batch 1500, Loss: 0.5646243691444397\n",
      "\tEpoch 21, Batch 1600, Loss: 0.5604794025421143\n",
      "\tEpoch 21, Batch 1700, Loss: 0.6561113595962524\n",
      "\tEpoch 21, Batch 1800, Loss: 0.585911214351654\n",
      "\tEpoch 21, Batch 1900, Loss: 0.6318202614784241\n",
      "\tEpoch 21, Batch 2000, Loss: 0.5655935406684875\n",
      "\tEpoch 21, Batch 2100, Loss: 0.5764778256416321\n",
      "Epoch: 21, Train Accuracy: 82.62%, Train Loss: 0.6158, Val Accuracy: 81.24%, Val Loss: 0.6712\n",
      "\tEpoch 22, Batch 100, Loss: 0.5845022201538086\n",
      "\tEpoch 22, Batch 200, Loss: 0.5923181176185608\n",
      "\tEpoch 22, Batch 300, Loss: 0.557321310043335\n",
      "\tEpoch 22, Batch 400, Loss: 0.5539371967315674\n",
      "\tEpoch 22, Batch 500, Loss: 0.7094084620475769\n",
      "\tEpoch 22, Batch 600, Loss: 0.6883371472358704\n",
      "\tEpoch 22, Batch 700, Loss: 0.5995063781738281\n",
      "\tEpoch 22, Batch 800, Loss: 0.6164352297782898\n",
      "\tEpoch 22, Batch 900, Loss: 0.4820234775543213\n",
      "\tEpoch 22, Batch 1000, Loss: 0.5899597406387329\n",
      "\tEpoch 22, Batch 1100, Loss: 0.4829716980457306\n",
      "\tEpoch 22, Batch 1200, Loss: 0.5042453408241272\n",
      "\tEpoch 22, Batch 1300, Loss: 0.39998331665992737\n",
      "\tEpoch 22, Batch 1400, Loss: 0.691567599773407\n",
      "\tEpoch 22, Batch 1500, Loss: 0.3820280432701111\n",
      "\tEpoch 22, Batch 1600, Loss: 0.43333539366722107\n",
      "\tEpoch 22, Batch 1700, Loss: 0.3857043981552124\n",
      "\tEpoch 22, Batch 1800, Loss: 0.5009672045707703\n",
      "\tEpoch 22, Batch 1900, Loss: 0.45926687121391296\n",
      "\tEpoch 22, Batch 2000, Loss: 0.49279624223709106\n",
      "\tEpoch 22, Batch 2100, Loss: 0.7557384371757507\n",
      "Epoch: 22, Train Accuracy: 82.98%, Train Loss: 0.6024, Val Accuracy: 79.20%, Val Loss: 0.7503\n",
      "\tEpoch 23, Batch 100, Loss: 0.7091304063796997\n",
      "\tEpoch 23, Batch 200, Loss: 0.5604217052459717\n",
      "\tEpoch 23, Batch 300, Loss: 0.7187060117721558\n",
      "\tEpoch 23, Batch 400, Loss: 0.655193030834198\n",
      "\tEpoch 23, Batch 500, Loss: 0.5321642160415649\n",
      "\tEpoch 23, Batch 600, Loss: 0.5300516486167908\n",
      "\tEpoch 23, Batch 700, Loss: 0.5155303478240967\n",
      "\tEpoch 23, Batch 800, Loss: 0.4309404194355011\n",
      "\tEpoch 23, Batch 900, Loss: 0.6662260293960571\n",
      "\tEpoch 23, Batch 1000, Loss: 0.5140689611434937\n",
      "\tEpoch 23, Batch 1100, Loss: 0.44985973834991455\n",
      "\tEpoch 23, Batch 1200, Loss: 0.5233405232429504\n",
      "\tEpoch 23, Batch 1300, Loss: 0.5942608714103699\n",
      "\tEpoch 23, Batch 1400, Loss: 0.5735840201377869\n",
      "\tEpoch 23, Batch 1500, Loss: 0.4137674570083618\n",
      "\tEpoch 23, Batch 1600, Loss: 0.5714067816734314\n",
      "\tEpoch 23, Batch 1700, Loss: 0.4235067069530487\n",
      "\tEpoch 23, Batch 1800, Loss: 0.5005690455436707\n",
      "\tEpoch 23, Batch 1900, Loss: 0.6558452844619751\n",
      "\tEpoch 23, Batch 2000, Loss: 0.4540306031703949\n",
      "\tEpoch 23, Batch 2100, Loss: 0.7365567684173584\n",
      "Epoch: 23, Train Accuracy: 83.53%, Train Loss: 0.5829, Val Accuracy: 82.77%, Val Loss: 0.6167\n",
      "\tEpoch 24, Batch 100, Loss: 0.6968317627906799\n",
      "\tEpoch 24, Batch 200, Loss: 0.43797245621681213\n",
      "\tEpoch 24, Batch 300, Loss: 0.4923897981643677\n",
      "\tEpoch 24, Batch 400, Loss: 0.717376708984375\n",
      "\tEpoch 24, Batch 500, Loss: 0.3911810517311096\n",
      "\tEpoch 24, Batch 600, Loss: 0.8397387266159058\n",
      "\tEpoch 24, Batch 700, Loss: 0.6771090626716614\n",
      "\tEpoch 24, Batch 800, Loss: 0.650440514087677\n",
      "\tEpoch 24, Batch 900, Loss: 0.46362730860710144\n",
      "\tEpoch 24, Batch 1000, Loss: 0.6070632338523865\n",
      "\tEpoch 24, Batch 1100, Loss: 0.6425775289535522\n",
      "\tEpoch 24, Batch 1200, Loss: 0.6570184826850891\n",
      "\tEpoch 24, Batch 1300, Loss: 0.6633667349815369\n",
      "\tEpoch 24, Batch 1400, Loss: 0.2630727291107178\n",
      "\tEpoch 24, Batch 1500, Loss: 0.4112340211868286\n",
      "\tEpoch 24, Batch 1600, Loss: 0.51736980676651\n",
      "\tEpoch 24, Batch 1700, Loss: 0.6143014430999756\n",
      "\tEpoch 24, Batch 1800, Loss: 0.39488279819488525\n",
      "\tEpoch 24, Batch 1900, Loss: 0.7114877104759216\n",
      "\tEpoch 24, Batch 2000, Loss: 0.5109047889709473\n",
      "\tEpoch 24, Batch 2100, Loss: 0.5628332495689392\n",
      "Epoch: 24, Train Accuracy: 83.93%, Train Loss: 0.5707, Val Accuracy: 81.81%, Val Loss: 0.6482\n",
      "\tEpoch 25, Batch 100, Loss: 0.5058872103691101\n",
      "\tEpoch 25, Batch 200, Loss: 0.8020794987678528\n",
      "\tEpoch 25, Batch 300, Loss: 0.5060116648674011\n",
      "\tEpoch 25, Batch 400, Loss: 0.6605384945869446\n",
      "\tEpoch 25, Batch 500, Loss: 0.5607207417488098\n",
      "\tEpoch 25, Batch 600, Loss: 0.6080619096755981\n",
      "\tEpoch 25, Batch 700, Loss: 0.45272794365882874\n",
      "\tEpoch 25, Batch 800, Loss: 0.6768488883972168\n",
      "\tEpoch 25, Batch 900, Loss: 0.6890032291412354\n",
      "\tEpoch 25, Batch 1000, Loss: 0.6672000288963318\n",
      "\tEpoch 25, Batch 1100, Loss: 0.4673107862472534\n",
      "\tEpoch 25, Batch 1200, Loss: 0.5362276434898376\n",
      "\tEpoch 25, Batch 1300, Loss: 0.5190000534057617\n",
      "\tEpoch 25, Batch 1400, Loss: 0.5931874513626099\n",
      "\tEpoch 25, Batch 1500, Loss: 0.46963170170783997\n",
      "\tEpoch 25, Batch 1600, Loss: 0.8691561222076416\n",
      "\tEpoch 25, Batch 1700, Loss: 0.279921293258667\n",
      "\tEpoch 25, Batch 1800, Loss: 0.5767655968666077\n",
      "\tEpoch 25, Batch 1900, Loss: 0.7973474264144897\n",
      "\tEpoch 25, Batch 2000, Loss: 0.681706428527832\n",
      "\tEpoch 25, Batch 2100, Loss: 0.6639171838760376\n",
      "Epoch: 25, Train Accuracy: 84.13%, Train Loss: 0.5593, Val Accuracy: 82.33%, Val Loss: 0.6431\n",
      "\tEpoch 26, Batch 100, Loss: 0.563228964805603\n",
      "\tEpoch 26, Batch 200, Loss: 0.3199121654033661\n",
      "\tEpoch 26, Batch 300, Loss: 0.5999448299407959\n",
      "\tEpoch 26, Batch 400, Loss: 0.5699866414070129\n",
      "\tEpoch 26, Batch 500, Loss: 0.39285534620285034\n",
      "\tEpoch 26, Batch 600, Loss: 0.5121616125106812\n",
      "\tEpoch 26, Batch 700, Loss: 0.4328221082687378\n",
      "\tEpoch 26, Batch 800, Loss: 0.5048205852508545\n",
      "\tEpoch 26, Batch 900, Loss: 0.5239272713661194\n",
      "\tEpoch 26, Batch 1000, Loss: 0.6028435230255127\n",
      "\tEpoch 26, Batch 1100, Loss: 0.5850599408149719\n",
      "\tEpoch 26, Batch 1200, Loss: 0.5848839282989502\n",
      "\tEpoch 26, Batch 1300, Loss: 0.606830358505249\n",
      "\tEpoch 26, Batch 1400, Loss: 0.784358024597168\n",
      "\tEpoch 26, Batch 1500, Loss: 0.48576435446739197\n",
      "\tEpoch 26, Batch 1600, Loss: 0.5688799023628235\n",
      "\tEpoch 26, Batch 1700, Loss: 0.6356087327003479\n",
      "\tEpoch 26, Batch 1800, Loss: 0.5083117485046387\n",
      "\tEpoch 26, Batch 1900, Loss: 0.6037186980247498\n",
      "\tEpoch 26, Batch 2000, Loss: 0.6788654327392578\n",
      "\tEpoch 26, Batch 2100, Loss: 1.0004318952560425\n",
      "Epoch: 26, Train Accuracy: 84.60%, Train Loss: 0.5466, Val Accuracy: 82.69%, Val Loss: 0.6173\n",
      "\tEpoch 27, Batch 100, Loss: 0.6418895125389099\n",
      "\tEpoch 27, Batch 200, Loss: 0.5056771039962769\n",
      "\tEpoch 27, Batch 300, Loss: 0.24503087997436523\n",
      "\tEpoch 27, Batch 400, Loss: 0.45978906750679016\n",
      "\tEpoch 27, Batch 500, Loss: 0.5474300980567932\n",
      "\tEpoch 27, Batch 600, Loss: 0.2945839464664459\n",
      "\tEpoch 27, Batch 700, Loss: 0.7439137101173401\n",
      "\tEpoch 27, Batch 800, Loss: 0.3066222369670868\n",
      "\tEpoch 27, Batch 900, Loss: 0.4363670349121094\n",
      "\tEpoch 27, Batch 1000, Loss: 0.5556126832962036\n",
      "\tEpoch 27, Batch 1100, Loss: 0.49833354353904724\n",
      "\tEpoch 27, Batch 1200, Loss: 0.6987255811691284\n",
      "\tEpoch 27, Batch 1300, Loss: 0.6208785772323608\n",
      "\tEpoch 27, Batch 1400, Loss: 0.544131875038147\n",
      "\tEpoch 27, Batch 1500, Loss: 0.5635258555412292\n",
      "\tEpoch 27, Batch 1600, Loss: 0.5966617465019226\n",
      "\tEpoch 27, Batch 1700, Loss: 0.4472302794456482\n",
      "\tEpoch 27, Batch 1800, Loss: 0.5699736475944519\n",
      "\tEpoch 27, Batch 1900, Loss: 0.47827669978141785\n",
      "\tEpoch 27, Batch 2000, Loss: 0.5541338920593262\n",
      "\tEpoch 27, Batch 2100, Loss: 0.5002477765083313\n",
      "Epoch: 27, Train Accuracy: 84.87%, Train Loss: 0.5356, Val Accuracy: 82.23%, Val Loss: 0.6196\n",
      "\tEpoch 28, Batch 100, Loss: 0.3569917678833008\n",
      "\tEpoch 28, Batch 200, Loss: 0.4323657751083374\n",
      "\tEpoch 28, Batch 300, Loss: 0.6485489010810852\n",
      "\tEpoch 28, Batch 400, Loss: 0.413894921541214\n",
      "\tEpoch 28, Batch 500, Loss: 0.4763069450855255\n",
      "\tEpoch 28, Batch 600, Loss: 0.6856743097305298\n",
      "\tEpoch 28, Batch 700, Loss: 0.6730969548225403\n",
      "\tEpoch 28, Batch 800, Loss: 0.45408767461776733\n",
      "\tEpoch 28, Batch 900, Loss: 0.6742222309112549\n",
      "\tEpoch 28, Batch 1000, Loss: 0.45354560017585754\n",
      "\tEpoch 28, Batch 1100, Loss: 0.6684990525245667\n",
      "\tEpoch 28, Batch 1200, Loss: 0.40147846937179565\n",
      "\tEpoch 28, Batch 1300, Loss: 0.7667737007141113\n",
      "\tEpoch 28, Batch 1400, Loss: 0.41958191990852356\n",
      "\tEpoch 28, Batch 1500, Loss: 0.5920764207839966\n",
      "\tEpoch 28, Batch 1600, Loss: 0.506598711013794\n",
      "\tEpoch 28, Batch 1700, Loss: 0.5388364195823669\n",
      "\tEpoch 28, Batch 1800, Loss: 0.5403741598129272\n",
      "\tEpoch 28, Batch 1900, Loss: 0.6567255258560181\n",
      "\tEpoch 28, Batch 2000, Loss: 0.6348122358322144\n",
      "\tEpoch 28, Batch 2100, Loss: 0.4901154339313507\n",
      "Epoch: 28, Train Accuracy: 85.14%, Train Loss: 0.5241, Val Accuracy: 84.86%, Val Loss: 0.5494\n",
      "\tEpoch 29, Batch 100, Loss: 0.36886829137802124\n",
      "\tEpoch 29, Batch 200, Loss: 0.6685240268707275\n",
      "\tEpoch 29, Batch 300, Loss: 0.32339900732040405\n",
      "\tEpoch 29, Batch 400, Loss: 0.405594140291214\n",
      "\tEpoch 29, Batch 500, Loss: 0.598355233669281\n",
      "\tEpoch 29, Batch 600, Loss: 0.6745008230209351\n",
      "\tEpoch 29, Batch 700, Loss: 0.5712180733680725\n",
      "\tEpoch 29, Batch 800, Loss: 0.5935067534446716\n",
      "\tEpoch 29, Batch 900, Loss: 0.5829852819442749\n",
      "\tEpoch 29, Batch 1000, Loss: 0.3920217752456665\n",
      "\tEpoch 29, Batch 1100, Loss: 0.5015188455581665\n",
      "\tEpoch 29, Batch 1200, Loss: 0.6063736081123352\n",
      "\tEpoch 29, Batch 1300, Loss: 0.4974973797798157\n",
      "\tEpoch 29, Batch 1400, Loss: 0.6712639927864075\n",
      "\tEpoch 29, Batch 1500, Loss: 0.4010317027568817\n",
      "\tEpoch 29, Batch 1600, Loss: 0.6843177080154419\n",
      "\tEpoch 29, Batch 1700, Loss: 0.475469172000885\n",
      "\tEpoch 29, Batch 1800, Loss: 0.6829618215560913\n",
      "\tEpoch 29, Batch 1900, Loss: 0.3944953382015228\n",
      "\tEpoch 29, Batch 2000, Loss: 0.8971840143203735\n",
      "\tEpoch 29, Batch 2100, Loss: 0.42915067076683044\n",
      "Epoch: 29, Train Accuracy: 85.45%, Train Loss: 0.5151, Val Accuracy: 83.57%, Val Loss: 0.5842\n",
      "\tEpoch 30, Batch 100, Loss: 0.5052720904350281\n",
      "\tEpoch 30, Batch 200, Loss: 0.6034268140792847\n",
      "\tEpoch 30, Batch 300, Loss: 0.4693697690963745\n",
      "\tEpoch 30, Batch 400, Loss: 0.42233574390411377\n",
      "\tEpoch 30, Batch 500, Loss: 0.6197283864021301\n",
      "\tEpoch 30, Batch 600, Loss: 0.3887269198894501\n",
      "\tEpoch 30, Batch 700, Loss: 0.4360932409763336\n",
      "\tEpoch 30, Batch 800, Loss: 0.5349236726760864\n",
      "\tEpoch 30, Batch 900, Loss: 0.5135105848312378\n",
      "\tEpoch 30, Batch 1000, Loss: 0.6680135726928711\n",
      "\tEpoch 30, Batch 1100, Loss: 0.4562080204486847\n",
      "\tEpoch 30, Batch 1200, Loss: 0.479574978351593\n",
      "\tEpoch 30, Batch 1300, Loss: 0.36524274945259094\n",
      "\tEpoch 30, Batch 1400, Loss: 0.7061842083930969\n",
      "\tEpoch 30, Batch 1500, Loss: 0.5403038859367371\n",
      "\tEpoch 30, Batch 1600, Loss: 0.3258683681488037\n",
      "\tEpoch 30, Batch 1700, Loss: 0.45076853036880493\n",
      "\tEpoch 30, Batch 1800, Loss: 0.6572562456130981\n",
      "\tEpoch 30, Batch 1900, Loss: 0.501987874507904\n",
      "\tEpoch 30, Batch 2000, Loss: 0.5549818277359009\n",
      "\tEpoch 30, Batch 2100, Loss: 0.6392171382904053\n",
      "Epoch: 30, Train Accuracy: 85.55%, Train Loss: 0.5081, Val Accuracy: 81.42%, Val Loss: 0.6724\n",
      "\tEpoch 31, Batch 100, Loss: 0.4585404694080353\n",
      "\tEpoch 31, Batch 200, Loss: 0.6315433382987976\n",
      "\tEpoch 31, Batch 300, Loss: 0.4432336688041687\n",
      "\tEpoch 31, Batch 400, Loss: 0.48619818687438965\n",
      "\tEpoch 31, Batch 500, Loss: 0.32885581254959106\n",
      "\tEpoch 31, Batch 600, Loss: 0.4745403528213501\n",
      "\tEpoch 31, Batch 700, Loss: 0.7408298850059509\n",
      "\tEpoch 31, Batch 800, Loss: 0.46368783712387085\n",
      "\tEpoch 31, Batch 900, Loss: 0.18273043632507324\n",
      "\tEpoch 31, Batch 1000, Loss: 0.36892279982566833\n",
      "\tEpoch 31, Batch 1100, Loss: 0.43543949723243713\n",
      "\tEpoch 31, Batch 1200, Loss: 0.5146777033805847\n",
      "\tEpoch 31, Batch 1300, Loss: 0.32616549730300903\n",
      "\tEpoch 31, Batch 1400, Loss: 0.4356626868247986\n",
      "\tEpoch 31, Batch 1500, Loss: 0.37045714259147644\n",
      "\tEpoch 31, Batch 1600, Loss: 0.298968106508255\n",
      "\tEpoch 31, Batch 1700, Loss: 0.5536574125289917\n",
      "\tEpoch 31, Batch 1800, Loss: 0.5899218916893005\n",
      "\tEpoch 31, Batch 1900, Loss: 0.5138428211212158\n",
      "\tEpoch 31, Batch 2000, Loss: 0.3533063530921936\n",
      "\tEpoch 31, Batch 2100, Loss: 0.5889514684677124\n",
      "Epoch: 31, Train Accuracy: 85.99%, Train Loss: 0.4961, Val Accuracy: 84.22%, Val Loss: 0.5723\n",
      "\tEpoch 32, Batch 100, Loss: 0.5061087608337402\n",
      "\tEpoch 32, Batch 200, Loss: 0.48759517073631287\n",
      "\tEpoch 32, Batch 300, Loss: 0.47954249382019043\n",
      "\tEpoch 32, Batch 400, Loss: 0.40815240144729614\n",
      "\tEpoch 32, Batch 500, Loss: 0.6303563714027405\n",
      "\tEpoch 32, Batch 600, Loss: 0.4942057132720947\n",
      "\tEpoch 32, Batch 700, Loss: 0.2912416458129883\n",
      "\tEpoch 32, Batch 800, Loss: 0.6772089004516602\n",
      "\tEpoch 32, Batch 900, Loss: 0.5226056575775146\n",
      "\tEpoch 32, Batch 1000, Loss: 0.45408520102500916\n",
      "\tEpoch 32, Batch 1100, Loss: 0.3818265497684479\n",
      "\tEpoch 32, Batch 1200, Loss: 0.420920729637146\n",
      "\tEpoch 32, Batch 1300, Loss: 0.4808415472507477\n",
      "\tEpoch 32, Batch 1400, Loss: 0.44341224431991577\n",
      "\tEpoch 32, Batch 1500, Loss: 0.6056295037269592\n",
      "\tEpoch 32, Batch 1600, Loss: 0.4238690733909607\n",
      "\tEpoch 32, Batch 1700, Loss: 0.6228368878364563\n",
      "\tEpoch 32, Batch 1800, Loss: 0.3862493932247162\n",
      "\tEpoch 32, Batch 1900, Loss: 0.44821587204933167\n",
      "\tEpoch 32, Batch 2000, Loss: 0.630199670791626\n",
      "\tEpoch 32, Batch 2100, Loss: 0.36394670605659485\n",
      "Epoch: 32, Train Accuracy: 86.22%, Train Loss: 0.4872, Val Accuracy: 84.36%, Val Loss: 0.5562\n",
      "\tEpoch 33, Batch 100, Loss: 0.5004639029502869\n",
      "\tEpoch 33, Batch 200, Loss: 0.6256622076034546\n",
      "\tEpoch 33, Batch 300, Loss: 0.41505300998687744\n",
      "\tEpoch 33, Batch 400, Loss: 0.3880011737346649\n",
      "\tEpoch 33, Batch 500, Loss: 0.37681472301483154\n",
      "\tEpoch 33, Batch 600, Loss: 0.5266990661621094\n",
      "\tEpoch 33, Batch 700, Loss: 0.46100905537605286\n",
      "\tEpoch 33, Batch 800, Loss: 0.4905467629432678\n",
      "\tEpoch 33, Batch 900, Loss: 0.37514856457710266\n",
      "\tEpoch 33, Batch 1000, Loss: 0.3760157823562622\n",
      "\tEpoch 33, Batch 1100, Loss: 0.45313897728919983\n",
      "\tEpoch 33, Batch 1200, Loss: 0.4198668599128723\n",
      "\tEpoch 33, Batch 1300, Loss: 0.72810959815979\n",
      "\tEpoch 33, Batch 1400, Loss: 0.41481196880340576\n",
      "\tEpoch 33, Batch 1500, Loss: 0.5727564096450806\n",
      "\tEpoch 33, Batch 1600, Loss: 0.6585121750831604\n",
      "\tEpoch 33, Batch 1700, Loss: 0.45200493931770325\n",
      "\tEpoch 33, Batch 1800, Loss: 0.4422394633293152\n",
      "\tEpoch 33, Batch 1900, Loss: 0.6000672578811646\n",
      "\tEpoch 33, Batch 2000, Loss: 0.27481579780578613\n",
      "\tEpoch 33, Batch 2100, Loss: 0.7753037810325623\n",
      "Epoch: 33, Train Accuracy: 86.38%, Train Loss: 0.4809, Val Accuracy: 85.98%, Val Loss: 0.5061\n",
      "\tEpoch 34, Batch 100, Loss: 0.4234392046928406\n",
      "\tEpoch 34, Batch 200, Loss: 0.45973676443099976\n",
      "\tEpoch 34, Batch 300, Loss: 0.3583158254623413\n",
      "\tEpoch 34, Batch 400, Loss: 0.5044543743133545\n",
      "\tEpoch 34, Batch 500, Loss: 0.4085584878921509\n",
      "\tEpoch 34, Batch 600, Loss: 0.41375476121902466\n",
      "\tEpoch 34, Batch 700, Loss: 0.4048580825328827\n",
      "\tEpoch 34, Batch 800, Loss: 0.7394947409629822\n",
      "\tEpoch 34, Batch 900, Loss: 0.5727192759513855\n",
      "\tEpoch 34, Batch 1000, Loss: 0.32399171590805054\n",
      "\tEpoch 34, Batch 1100, Loss: 0.44606849551200867\n",
      "\tEpoch 34, Batch 1200, Loss: 0.5186759233474731\n",
      "\tEpoch 34, Batch 1300, Loss: 0.3625769019126892\n",
      "\tEpoch 34, Batch 1400, Loss: 0.2962331175804138\n",
      "\tEpoch 34, Batch 1500, Loss: 0.8135974407196045\n",
      "\tEpoch 34, Batch 1600, Loss: 0.6893452405929565\n",
      "\tEpoch 34, Batch 1700, Loss: 0.36485615372657776\n",
      "\tEpoch 34, Batch 1800, Loss: 0.5957636833190918\n",
      "\tEpoch 34, Batch 1900, Loss: 0.5096865296363831\n",
      "\tEpoch 34, Batch 2000, Loss: 0.4005856215953827\n",
      "\tEpoch 34, Batch 2100, Loss: 0.29825207591056824\n",
      "Epoch: 34, Train Accuracy: 86.67%, Train Loss: 0.4695, Val Accuracy: 85.52%, Val Loss: 0.5122\n",
      "\tEpoch 35, Batch 100, Loss: 0.5200934410095215\n",
      "\tEpoch 35, Batch 200, Loss: 0.4335823655128479\n",
      "\tEpoch 35, Batch 300, Loss: 0.6111796498298645\n",
      "\tEpoch 35, Batch 400, Loss: 0.41619959473609924\n",
      "\tEpoch 35, Batch 500, Loss: 0.4069504141807556\n",
      "\tEpoch 35, Batch 600, Loss: 0.3369842767715454\n",
      "\tEpoch 35, Batch 700, Loss: 0.44667184352874756\n",
      "\tEpoch 35, Batch 800, Loss: 0.2560172975063324\n",
      "\tEpoch 35, Batch 900, Loss: 0.4940790832042694\n",
      "\tEpoch 35, Batch 1000, Loss: 0.30967286229133606\n",
      "\tEpoch 35, Batch 1100, Loss: 0.7692798376083374\n",
      "\tEpoch 35, Batch 1200, Loss: 0.33103761076927185\n",
      "\tEpoch 35, Batch 1300, Loss: 0.48250171542167664\n",
      "\tEpoch 35, Batch 1400, Loss: 0.427971750497818\n",
      "\tEpoch 35, Batch 1500, Loss: 0.3790774941444397\n",
      "\tEpoch 35, Batch 1600, Loss: 0.6241524815559387\n",
      "\tEpoch 35, Batch 1700, Loss: 0.4023360013961792\n",
      "\tEpoch 35, Batch 1800, Loss: 0.374283105134964\n",
      "\tEpoch 35, Batch 1900, Loss: 0.5061419606208801\n",
      "\tEpoch 35, Batch 2000, Loss: 0.3725599944591522\n",
      "\tEpoch 35, Batch 2100, Loss: 0.6166293621063232\n",
      "Epoch: 35, Train Accuracy: 86.70%, Train Loss: 0.4662, Val Accuracy: 86.22%, Val Loss: 0.4843\n",
      "\tEpoch 36, Batch 100, Loss: 0.36719146370887756\n",
      "\tEpoch 36, Batch 200, Loss: 0.8148047924041748\n",
      "\tEpoch 36, Batch 300, Loss: 0.4956459701061249\n",
      "\tEpoch 36, Batch 400, Loss: 0.4636535942554474\n",
      "\tEpoch 36, Batch 500, Loss: 0.42240872979164124\n",
      "\tEpoch 36, Batch 600, Loss: 0.6070075035095215\n",
      "\tEpoch 36, Batch 700, Loss: 0.590290367603302\n",
      "\tEpoch 36, Batch 800, Loss: 0.6821486353874207\n",
      "\tEpoch 36, Batch 900, Loss: 0.34893032908439636\n",
      "\tEpoch 36, Batch 1000, Loss: 0.4356081187725067\n",
      "\tEpoch 36, Batch 1100, Loss: 0.27916646003723145\n",
      "\tEpoch 36, Batch 1200, Loss: 0.4752451181411743\n",
      "\tEpoch 36, Batch 1300, Loss: 0.3881128132343292\n",
      "\tEpoch 36, Batch 1400, Loss: 0.47932031750679016\n",
      "\tEpoch 36, Batch 1500, Loss: 0.5128247737884521\n",
      "\tEpoch 36, Batch 1600, Loss: 0.2461017370223999\n",
      "\tEpoch 36, Batch 1700, Loss: 0.3368470072746277\n",
      "\tEpoch 36, Batch 1800, Loss: 0.6723820567131042\n",
      "\tEpoch 36, Batch 1900, Loss: 0.4480932950973511\n",
      "\tEpoch 36, Batch 2000, Loss: 0.2526823878288269\n",
      "\tEpoch 36, Batch 2100, Loss: 0.5441485047340393\n",
      "Epoch: 36, Train Accuracy: 87.06%, Train Loss: 0.4572, Val Accuracy: 85.01%, Val Loss: 0.5672\n",
      "\tEpoch 37, Batch 100, Loss: 0.4008733630180359\n",
      "\tEpoch 37, Batch 200, Loss: 0.4450657069683075\n",
      "\tEpoch 37, Batch 300, Loss: 0.6720995306968689\n",
      "\tEpoch 37, Batch 400, Loss: 0.2723393738269806\n",
      "\tEpoch 37, Batch 500, Loss: 0.4875979721546173\n",
      "\tEpoch 37, Batch 600, Loss: 0.5204996466636658\n",
      "\tEpoch 37, Batch 700, Loss: 0.4167254567146301\n",
      "\tEpoch 37, Batch 800, Loss: 0.38730043172836304\n",
      "\tEpoch 37, Batch 900, Loss: 0.4760855436325073\n",
      "\tEpoch 37, Batch 1000, Loss: 0.4464956521987915\n",
      "\tEpoch 37, Batch 1100, Loss: 0.5131177306175232\n",
      "\tEpoch 37, Batch 1200, Loss: 0.40351417660713196\n",
      "\tEpoch 37, Batch 1300, Loss: 0.377930611371994\n",
      "\tEpoch 37, Batch 1400, Loss: 0.6849941611289978\n",
      "\tEpoch 37, Batch 1500, Loss: 0.6092811822891235\n",
      "\tEpoch 37, Batch 1600, Loss: 0.44305115938186646\n",
      "\tEpoch 37, Batch 1700, Loss: 0.7100934982299805\n",
      "\tEpoch 37, Batch 1800, Loss: 0.2945387363433838\n",
      "\tEpoch 37, Batch 1900, Loss: 0.6119040250778198\n",
      "\tEpoch 37, Batch 2000, Loss: 0.2961284816265106\n",
      "\tEpoch 37, Batch 2100, Loss: 0.4922569990158081\n",
      "Epoch: 37, Train Accuracy: 87.05%, Train Loss: 0.4526, Val Accuracy: 85.36%, Val Loss: 0.5222\n",
      "\tEpoch 38, Batch 100, Loss: 0.5222114324569702\n",
      "\tEpoch 38, Batch 200, Loss: 0.40657785534858704\n",
      "\tEpoch 38, Batch 300, Loss: 0.3679180145263672\n",
      "\tEpoch 38, Batch 400, Loss: 0.3494011163711548\n",
      "\tEpoch 38, Batch 500, Loss: 0.4436528980731964\n",
      "\tEpoch 38, Batch 600, Loss: 0.596251904964447\n",
      "\tEpoch 38, Batch 700, Loss: 0.372361421585083\n",
      "\tEpoch 38, Batch 800, Loss: 0.40632110834121704\n",
      "\tEpoch 38, Batch 900, Loss: 0.37783417105674744\n",
      "\tEpoch 38, Batch 1000, Loss: 0.4226932227611542\n",
      "\tEpoch 38, Batch 1100, Loss: 0.4635924696922302\n",
      "\tEpoch 38, Batch 1200, Loss: 0.45025739073753357\n",
      "\tEpoch 38, Batch 1300, Loss: 0.24501973390579224\n",
      "\tEpoch 38, Batch 1400, Loss: 0.5525804162025452\n",
      "\tEpoch 38, Batch 1500, Loss: 0.32920944690704346\n",
      "\tEpoch 38, Batch 1600, Loss: 0.547343373298645\n",
      "\tEpoch 38, Batch 1700, Loss: 0.21256013214588165\n",
      "\tEpoch 38, Batch 1800, Loss: 0.3228413760662079\n",
      "\tEpoch 38, Batch 1900, Loss: 0.6317979693412781\n",
      "\tEpoch 38, Batch 2000, Loss: 0.48465055227279663\n",
      "\tEpoch 38, Batch 2100, Loss: 0.38381874561309814\n",
      "Epoch: 38, Train Accuracy: 87.23%, Train Loss: 0.4491, Val Accuracy: 86.92%, Val Loss: 0.4732\n",
      "\tEpoch 39, Batch 100, Loss: 0.39301127195358276\n",
      "\tEpoch 39, Batch 200, Loss: 0.30317673087120056\n",
      "\tEpoch 39, Batch 300, Loss: 0.5812058448791504\n",
      "\tEpoch 39, Batch 400, Loss: 0.3485114276409149\n",
      "\tEpoch 39, Batch 500, Loss: 0.41531264781951904\n",
      "\tEpoch 39, Batch 600, Loss: 0.5370814800262451\n",
      "\tEpoch 39, Batch 700, Loss: 0.3444257974624634\n",
      "\tEpoch 39, Batch 800, Loss: 0.4472792446613312\n",
      "\tEpoch 39, Batch 900, Loss: 0.27402904629707336\n",
      "\tEpoch 39, Batch 1000, Loss: 0.2199980616569519\n",
      "\tEpoch 39, Batch 1100, Loss: 0.3385637700557709\n",
      "\tEpoch 39, Batch 1200, Loss: 0.28225401043891907\n",
      "\tEpoch 39, Batch 1300, Loss: 0.4894762337207794\n",
      "\tEpoch 39, Batch 1400, Loss: 0.2657908797264099\n",
      "\tEpoch 39, Batch 1500, Loss: 0.42898714542388916\n",
      "\tEpoch 39, Batch 1600, Loss: 0.4327455163002014\n",
      "\tEpoch 39, Batch 1700, Loss: 0.6468083262443542\n",
      "\tEpoch 39, Batch 1800, Loss: 0.575689435005188\n",
      "\tEpoch 39, Batch 1900, Loss: 0.2926042079925537\n",
      "\tEpoch 39, Batch 2000, Loss: 0.4464854300022125\n",
      "\tEpoch 39, Batch 2100, Loss: 0.5721952319145203\n",
      "Epoch: 39, Train Accuracy: 87.40%, Train Loss: 0.4422, Val Accuracy: 86.29%, Val Loss: 0.4907\n",
      "\tEpoch 40, Batch 100, Loss: 0.3658447861671448\n",
      "\tEpoch 40, Batch 200, Loss: 0.4283333122730255\n",
      "\tEpoch 40, Batch 300, Loss: 0.6006856560707092\n",
      "\tEpoch 40, Batch 400, Loss: 0.43399420380592346\n",
      "\tEpoch 40, Batch 500, Loss: 0.38706353306770325\n",
      "\tEpoch 40, Batch 600, Loss: 0.320451945066452\n",
      "\tEpoch 40, Batch 700, Loss: 0.5702905654907227\n",
      "\tEpoch 40, Batch 800, Loss: 0.4078288674354553\n",
      "\tEpoch 40, Batch 900, Loss: 0.21678116917610168\n",
      "\tEpoch 40, Batch 1000, Loss: 0.545892059803009\n",
      "\tEpoch 40, Batch 1100, Loss: 0.4254731833934784\n",
      "\tEpoch 40, Batch 1200, Loss: 0.7041317820549011\n",
      "\tEpoch 40, Batch 1300, Loss: 0.40289953351020813\n",
      "\tEpoch 40, Batch 1400, Loss: 0.39687517285346985\n",
      "\tEpoch 40, Batch 1500, Loss: 0.31576573848724365\n",
      "\tEpoch 40, Batch 1600, Loss: 0.29870080947875977\n",
      "\tEpoch 40, Batch 1700, Loss: 0.2293138951063156\n",
      "\tEpoch 40, Batch 1800, Loss: 0.5754452347755432\n",
      "\tEpoch 40, Batch 1900, Loss: 0.5954241752624512\n",
      "\tEpoch 40, Batch 2000, Loss: 0.4413934051990509\n",
      "\tEpoch 40, Batch 2100, Loss: 0.4746488928794861\n",
      "Epoch: 40, Train Accuracy: 87.66%, Train Loss: 0.4352, Val Accuracy: 86.43%, Val Loss: 0.4841\n",
      "\tEpoch 41, Batch 100, Loss: 0.4037853181362152\n",
      "\tEpoch 41, Batch 200, Loss: 0.38809627294540405\n",
      "\tEpoch 41, Batch 300, Loss: 0.4857197403907776\n",
      "\tEpoch 41, Batch 400, Loss: 0.3357482850551605\n",
      "\tEpoch 41, Batch 500, Loss: 0.4897090792655945\n",
      "\tEpoch 41, Batch 600, Loss: 0.36779239773750305\n",
      "\tEpoch 41, Batch 700, Loss: 0.4596283435821533\n",
      "\tEpoch 41, Batch 800, Loss: 0.4493335783481598\n",
      "\tEpoch 41, Batch 900, Loss: 0.5435159802436829\n",
      "\tEpoch 41, Batch 1000, Loss: 0.31935688853263855\n",
      "\tEpoch 41, Batch 1100, Loss: 0.3247545063495636\n",
      "\tEpoch 41, Batch 1200, Loss: 0.45227065682411194\n",
      "\tEpoch 41, Batch 1300, Loss: 0.3511219024658203\n",
      "\tEpoch 41, Batch 1400, Loss: 0.335933655500412\n",
      "\tEpoch 41, Batch 1500, Loss: 0.4878268539905548\n",
      "\tEpoch 41, Batch 1600, Loss: 0.2851797342300415\n",
      "\tEpoch 41, Batch 1700, Loss: 0.5445752739906311\n",
      "\tEpoch 41, Batch 1800, Loss: 0.3785122036933899\n",
      "\tEpoch 41, Batch 1900, Loss: 0.5372148752212524\n",
      "\tEpoch 41, Batch 2000, Loss: 0.42133891582489014\n",
      "\tEpoch 41, Batch 2100, Loss: 0.6075127720832825\n",
      "Epoch: 41, Train Accuracy: 87.74%, Train Loss: 0.4297, Val Accuracy: 85.41%, Val Loss: 0.5393\n",
      "\tEpoch 42, Batch 100, Loss: 0.5006804466247559\n",
      "\tEpoch 42, Batch 200, Loss: 0.379124253988266\n",
      "\tEpoch 42, Batch 300, Loss: 0.564217209815979\n",
      "\tEpoch 42, Batch 400, Loss: 0.38417816162109375\n",
      "\tEpoch 42, Batch 500, Loss: 0.3847638666629791\n",
      "\tEpoch 42, Batch 600, Loss: 0.38731083273887634\n",
      "\tEpoch 42, Batch 700, Loss: 0.42128318548202515\n",
      "\tEpoch 42, Batch 800, Loss: 0.5322046875953674\n",
      "\tEpoch 42, Batch 900, Loss: 0.34537622332572937\n",
      "\tEpoch 42, Batch 1000, Loss: 0.4964729845523834\n",
      "\tEpoch 42, Batch 1100, Loss: 0.6783894896507263\n",
      "\tEpoch 42, Batch 1200, Loss: 0.4359623193740845\n",
      "\tEpoch 42, Batch 1300, Loss: 0.4291676878929138\n",
      "\tEpoch 42, Batch 1400, Loss: 0.5146001577377319\n",
      "\tEpoch 42, Batch 1500, Loss: 0.3969084918498993\n",
      "\tEpoch 42, Batch 1600, Loss: 0.3024166226387024\n",
      "\tEpoch 42, Batch 1700, Loss: 0.2941679358482361\n",
      "\tEpoch 42, Batch 1800, Loss: 0.675624430179596\n",
      "\tEpoch 42, Batch 1900, Loss: 0.3786616027355194\n",
      "\tEpoch 42, Batch 2000, Loss: 0.3799845278263092\n",
      "\tEpoch 42, Batch 2100, Loss: 0.32420051097869873\n",
      "Epoch: 42, Train Accuracy: 87.92%, Train Loss: 0.4222, Val Accuracy: 86.75%, Val Loss: 0.4738\n",
      "\tEpoch 43, Batch 100, Loss: 0.2894311845302582\n",
      "\tEpoch 43, Batch 200, Loss: 0.3313540816307068\n",
      "\tEpoch 43, Batch 300, Loss: 0.5917453765869141\n",
      "\tEpoch 43, Batch 400, Loss: 0.3137488067150116\n",
      "\tEpoch 43, Batch 500, Loss: 0.3360631465911865\n",
      "\tEpoch 43, Batch 600, Loss: 0.48958349227905273\n",
      "\tEpoch 43, Batch 700, Loss: 0.4659130275249481\n",
      "\tEpoch 43, Batch 800, Loss: 0.2701255679130554\n",
      "\tEpoch 43, Batch 900, Loss: 0.5354263782501221\n",
      "\tEpoch 43, Batch 1000, Loss: 0.3740064203739166\n",
      "\tEpoch 43, Batch 1100, Loss: 0.4489794671535492\n",
      "\tEpoch 43, Batch 1200, Loss: 0.47136738896369934\n",
      "\tEpoch 43, Batch 1300, Loss: 0.4271598160266876\n",
      "\tEpoch 43, Batch 1400, Loss: 0.423410564661026\n",
      "\tEpoch 43, Batch 1500, Loss: 0.45638033747673035\n",
      "\tEpoch 43, Batch 1600, Loss: 0.2746116816997528\n",
      "\tEpoch 43, Batch 1700, Loss: 0.6344456672668457\n",
      "\tEpoch 43, Batch 1800, Loss: 0.40293052792549133\n",
      "\tEpoch 43, Batch 1900, Loss: 0.7238863110542297\n",
      "\tEpoch 43, Batch 2000, Loss: 0.3789936304092407\n",
      "\tEpoch 43, Batch 2100, Loss: 0.4768482446670532\n",
      "Epoch: 43, Train Accuracy: 88.00%, Train Loss: 0.4216, Val Accuracy: 84.59%, Val Loss: 0.5594\n",
      "\tEpoch 44, Batch 100, Loss: 0.37763625383377075\n",
      "\tEpoch 44, Batch 200, Loss: 0.4618041515350342\n",
      "\tEpoch 44, Batch 300, Loss: 0.41474059224128723\n",
      "\tEpoch 44, Batch 400, Loss: 0.39861157536506653\n",
      "\tEpoch 44, Batch 500, Loss: 0.4287228584289551\n",
      "\tEpoch 44, Batch 600, Loss: 0.4144585430622101\n",
      "\tEpoch 44, Batch 700, Loss: 0.4175359606742859\n",
      "\tEpoch 44, Batch 800, Loss: 0.40114861726760864\n",
      "\tEpoch 44, Batch 900, Loss: 0.660322904586792\n",
      "\tEpoch 44, Batch 1000, Loss: 0.5656255483627319\n",
      "\tEpoch 44, Batch 1100, Loss: 0.29511338472366333\n",
      "\tEpoch 44, Batch 1200, Loss: 0.48778200149536133\n",
      "\tEpoch 44, Batch 1300, Loss: 0.35579338669776917\n",
      "\tEpoch 44, Batch 1400, Loss: 0.1900435984134674\n",
      "\tEpoch 44, Batch 1500, Loss: 0.5390865206718445\n",
      "\tEpoch 44, Batch 1600, Loss: 0.3122304677963257\n",
      "\tEpoch 44, Batch 1700, Loss: 0.5222079753875732\n",
      "\tEpoch 44, Batch 1800, Loss: 0.4532800614833832\n",
      "\tEpoch 44, Batch 1900, Loss: 0.729131817817688\n",
      "\tEpoch 44, Batch 2000, Loss: 0.48747119307518005\n",
      "\tEpoch 44, Batch 2100, Loss: 0.29389145970344543\n",
      "Epoch: 44, Train Accuracy: 88.20%, Train Loss: 0.4154, Val Accuracy: 87.67%, Val Loss: 0.4513\n",
      "\tEpoch 45, Batch 100, Loss: 0.490074098110199\n",
      "\tEpoch 45, Batch 200, Loss: 0.4822368621826172\n",
      "\tEpoch 45, Batch 300, Loss: 0.4347137212753296\n",
      "\tEpoch 45, Batch 400, Loss: 0.3751440942287445\n",
      "\tEpoch 45, Batch 500, Loss: 0.3625316917896271\n",
      "\tEpoch 45, Batch 600, Loss: 0.42835119366645813\n",
      "\tEpoch 45, Batch 700, Loss: 0.5414294004440308\n",
      "\tEpoch 45, Batch 800, Loss: 0.271918386220932\n",
      "\tEpoch 45, Batch 900, Loss: 0.35321465134620667\n",
      "\tEpoch 45, Batch 1000, Loss: 0.25687775015830994\n",
      "\tEpoch 45, Batch 1100, Loss: 0.7476773262023926\n",
      "\tEpoch 45, Batch 1200, Loss: 0.21113941073417664\n",
      "\tEpoch 45, Batch 1300, Loss: 0.21440450847148895\n",
      "\tEpoch 45, Batch 1400, Loss: 0.328024297952652\n",
      "\tEpoch 45, Batch 1500, Loss: 0.38051509857177734\n",
      "\tEpoch 45, Batch 1600, Loss: 0.5020391941070557\n",
      "\tEpoch 45, Batch 1700, Loss: 0.33946800231933594\n",
      "\tEpoch 45, Batch 1800, Loss: 0.26504406332969666\n",
      "\tEpoch 45, Batch 1900, Loss: 0.406662255525589\n",
      "\tEpoch 45, Batch 2000, Loss: 0.4732833504676819\n",
      "\tEpoch 45, Batch 2100, Loss: 0.3262012004852295\n",
      "Epoch: 45, Train Accuracy: 88.42%, Train Loss: 0.4068, Val Accuracy: 88.57%, Val Loss: 0.4233\n",
      "\tEpoch 46, Batch 100, Loss: 0.3935014009475708\n",
      "\tEpoch 46, Batch 200, Loss: 0.4218752682209015\n",
      "\tEpoch 46, Batch 300, Loss: 0.4106236696243286\n",
      "\tEpoch 46, Batch 400, Loss: 0.2585030198097229\n",
      "\tEpoch 46, Batch 500, Loss: 0.305693119764328\n",
      "\tEpoch 46, Batch 600, Loss: 0.6484529376029968\n",
      "\tEpoch 46, Batch 700, Loss: 0.29795965552330017\n",
      "\tEpoch 46, Batch 800, Loss: 0.6628344655036926\n",
      "\tEpoch 46, Batch 900, Loss: 0.5026717782020569\n",
      "\tEpoch 46, Batch 1000, Loss: 0.3965073525905609\n",
      "\tEpoch 46, Batch 1100, Loss: 0.35128292441368103\n",
      "\tEpoch 46, Batch 1200, Loss: 0.6845667362213135\n",
      "\tEpoch 46, Batch 1300, Loss: 0.47446325421333313\n",
      "\tEpoch 46, Batch 1400, Loss: 0.4619443416595459\n",
      "\tEpoch 46, Batch 1500, Loss: 0.2532457709312439\n",
      "\tEpoch 46, Batch 1600, Loss: 0.35588449239730835\n",
      "\tEpoch 46, Batch 1700, Loss: 0.21774818003177643\n",
      "\tEpoch 46, Batch 1800, Loss: 0.5065808296203613\n",
      "\tEpoch 46, Batch 1900, Loss: 0.29899290204048157\n",
      "\tEpoch 46, Batch 2000, Loss: 0.2769496738910675\n",
      "\tEpoch 46, Batch 2100, Loss: 0.18347042798995972\n",
      "Epoch: 46, Train Accuracy: 88.44%, Train Loss: 0.4048, Val Accuracy: 87.49%, Val Loss: 0.4490\n",
      "\tEpoch 47, Batch 100, Loss: 0.49301663041114807\n",
      "\tEpoch 47, Batch 200, Loss: 0.5754666328430176\n",
      "\tEpoch 47, Batch 300, Loss: 0.5115718245506287\n",
      "\tEpoch 47, Batch 400, Loss: 0.35153627395629883\n",
      "\tEpoch 47, Batch 500, Loss: 0.5378302931785583\n",
      "\tEpoch 47, Batch 600, Loss: 0.2717802822589874\n",
      "\tEpoch 47, Batch 700, Loss: 0.15116584300994873\n",
      "\tEpoch 47, Batch 800, Loss: 0.4765625\n",
      "\tEpoch 47, Batch 900, Loss: 0.3706948459148407\n",
      "\tEpoch 47, Batch 1000, Loss: 0.3951110243797302\n",
      "\tEpoch 47, Batch 1100, Loss: 0.4043957591056824\n",
      "\tEpoch 47, Batch 1200, Loss: 0.49067267775535583\n",
      "\tEpoch 47, Batch 1300, Loss: 0.2173987478017807\n",
      "\tEpoch 47, Batch 1400, Loss: 0.3822770118713379\n",
      "\tEpoch 47, Batch 1500, Loss: 0.2990349233150482\n",
      "\tEpoch 47, Batch 1600, Loss: 0.3509797155857086\n",
      "\tEpoch 47, Batch 1700, Loss: 0.42197129130363464\n",
      "\tEpoch 47, Batch 1800, Loss: 0.27306699752807617\n",
      "\tEpoch 47, Batch 1900, Loss: 0.23195233941078186\n",
      "\tEpoch 47, Batch 2000, Loss: 0.6946110725402832\n",
      "\tEpoch 47, Batch 2100, Loss: 0.37964290380477905\n",
      "Epoch: 47, Train Accuracy: 88.58%, Train Loss: 0.4007, Val Accuracy: 87.79%, Val Loss: 0.4399\n",
      "\tEpoch 48, Batch 100, Loss: 0.7591042518615723\n",
      "\tEpoch 48, Batch 200, Loss: 0.4545236825942993\n",
      "\tEpoch 48, Batch 300, Loss: 0.18391044437885284\n",
      "\tEpoch 48, Batch 400, Loss: 0.22930431365966797\n",
      "\tEpoch 48, Batch 500, Loss: 0.39876821637153625\n",
      "\tEpoch 48, Batch 600, Loss: 0.20240402221679688\n",
      "\tEpoch 48, Batch 700, Loss: 0.31938591599464417\n",
      "\tEpoch 48, Batch 800, Loss: 0.654801607131958\n",
      "\tEpoch 48, Batch 900, Loss: 0.5049723386764526\n",
      "\tEpoch 48, Batch 1000, Loss: 0.538600504398346\n",
      "\tEpoch 48, Batch 1100, Loss: 0.48811233043670654\n",
      "\tEpoch 48, Batch 1200, Loss: 0.3512202799320221\n",
      "\tEpoch 48, Batch 1300, Loss: 0.33288419246673584\n",
      "\tEpoch 48, Batch 1400, Loss: 0.5382106304168701\n",
      "\tEpoch 48, Batch 1500, Loss: 0.16016848385334015\n",
      "\tEpoch 48, Batch 1600, Loss: 0.32779714465141296\n",
      "\tEpoch 48, Batch 1700, Loss: 0.49906471371650696\n",
      "\tEpoch 48, Batch 1800, Loss: 0.5815515518188477\n",
      "\tEpoch 48, Batch 1900, Loss: 0.3585834503173828\n",
      "\tEpoch 48, Batch 2000, Loss: 0.43114951252937317\n",
      "\tEpoch 48, Batch 2100, Loss: 0.507392942905426\n",
      "Epoch: 48, Train Accuracy: 88.64%, Train Loss: 0.3965, Val Accuracy: 88.40%, Val Loss: 0.4307\n",
      "\tEpoch 49, Batch 100, Loss: 0.3251711428165436\n",
      "\tEpoch 49, Batch 200, Loss: 0.3849868178367615\n",
      "\tEpoch 49, Batch 300, Loss: 0.5538929104804993\n",
      "\tEpoch 49, Batch 400, Loss: 0.47776612639427185\n",
      "\tEpoch 49, Batch 500, Loss: 0.4770641326904297\n",
      "\tEpoch 49, Batch 600, Loss: 0.4555259048938751\n",
      "\tEpoch 49, Batch 700, Loss: 0.48424655199050903\n",
      "\tEpoch 49, Batch 800, Loss: 0.2901146709918976\n",
      "\tEpoch 49, Batch 900, Loss: 0.430925577878952\n",
      "\tEpoch 49, Batch 1000, Loss: 0.49108386039733887\n",
      "\tEpoch 49, Batch 1100, Loss: 0.3174300789833069\n",
      "\tEpoch 49, Batch 1200, Loss: 0.34451329708099365\n",
      "\tEpoch 49, Batch 1300, Loss: 0.3816851079463959\n",
      "\tEpoch 49, Batch 1400, Loss: 0.4437512755393982\n",
      "\tEpoch 49, Batch 1500, Loss: 0.4163838028907776\n",
      "\tEpoch 49, Batch 1600, Loss: 0.4336336851119995\n",
      "\tEpoch 49, Batch 1700, Loss: 0.3601640462875366\n",
      "\tEpoch 49, Batch 1800, Loss: 0.40923377871513367\n",
      "\tEpoch 49, Batch 1900, Loss: 0.5990821719169617\n",
      "\tEpoch 49, Batch 2000, Loss: 0.3154793977737427\n",
      "\tEpoch 49, Batch 2100, Loss: 0.4629876911640167\n",
      "Epoch: 49, Train Accuracy: 88.80%, Train Loss: 0.3909, Val Accuracy: 86.85%, Val Loss: 0.4778\n",
      "\tEpoch 50, Batch 100, Loss: 0.35911357402801514\n",
      "\tEpoch 50, Batch 200, Loss: 0.26894789934158325\n",
      "\tEpoch 50, Batch 300, Loss: 0.4442020356655121\n",
      "\tEpoch 50, Batch 400, Loss: 0.29640302062034607\n",
      "\tEpoch 50, Batch 500, Loss: 0.22652822732925415\n",
      "\tEpoch 50, Batch 600, Loss: 0.4763198792934418\n",
      "\tEpoch 50, Batch 700, Loss: 0.2842128276824951\n",
      "\tEpoch 50, Batch 800, Loss: 0.5651710629463196\n",
      "\tEpoch 50, Batch 900, Loss: 0.28308388590812683\n",
      "\tEpoch 50, Batch 1000, Loss: 0.4039563238620758\n",
      "\tEpoch 50, Batch 1100, Loss: 0.559479832649231\n",
      "\tEpoch 50, Batch 1200, Loss: 0.32659152150154114\n",
      "\tEpoch 50, Batch 1300, Loss: 0.34197354316711426\n",
      "\tEpoch 50, Batch 1400, Loss: 0.4513731300830841\n",
      "\tEpoch 50, Batch 1500, Loss: 0.4716373682022095\n",
      "\tEpoch 50, Batch 1600, Loss: 0.5066637992858887\n",
      "\tEpoch 50, Batch 1700, Loss: 0.33087053894996643\n",
      "\tEpoch 50, Batch 1800, Loss: 0.4940360188484192\n",
      "\tEpoch 50, Batch 1900, Loss: 0.4086623191833496\n",
      "\tEpoch 50, Batch 2000, Loss: 0.2545463442802429\n",
      "\tEpoch 50, Batch 2100, Loss: 0.375376433134079\n",
      "Epoch: 50, Train Accuracy: 88.76%, Train Loss: 0.3900, Val Accuracy: 88.42%, Val Loss: 0.4218\n",
      "\tEpoch 51, Batch 100, Loss: 0.38344570994377136\n",
      "\tEpoch 51, Batch 200, Loss: 0.417988657951355\n",
      "\tEpoch 51, Batch 300, Loss: 0.32237523794174194\n",
      "\tEpoch 51, Batch 400, Loss: 0.3059867024421692\n",
      "\tEpoch 51, Batch 500, Loss: 0.5447543263435364\n",
      "\tEpoch 51, Batch 600, Loss: 0.3224899172782898\n",
      "\tEpoch 51, Batch 700, Loss: 0.26111799478530884\n",
      "\tEpoch 51, Batch 800, Loss: 0.3461688458919525\n",
      "\tEpoch 51, Batch 900, Loss: 0.23887977004051208\n",
      "\tEpoch 51, Batch 1000, Loss: 0.3950754702091217\n",
      "\tEpoch 51, Batch 1100, Loss: 0.34907135367393494\n",
      "\tEpoch 51, Batch 1200, Loss: 0.44680869579315186\n",
      "\tEpoch 51, Batch 1300, Loss: 0.3419191539287567\n",
      "\tEpoch 51, Batch 1400, Loss: 0.5074188113212585\n",
      "\tEpoch 51, Batch 1500, Loss: 0.252655565738678\n",
      "\tEpoch 51, Batch 1600, Loss: 0.6881815791130066\n",
      "\tEpoch 51, Batch 1700, Loss: 0.3396923542022705\n",
      "\tEpoch 51, Batch 1800, Loss: 0.40921422839164734\n",
      "\tEpoch 51, Batch 1900, Loss: 0.1930711418390274\n",
      "\tEpoch 51, Batch 2000, Loss: 0.5001618266105652\n",
      "\tEpoch 51, Batch 2100, Loss: 0.6645469069480896\n",
      "Epoch: 51, Train Accuracy: 89.05%, Train Loss: 0.3832, Val Accuracy: 88.68%, Val Loss: 0.4088\n",
      "\tEpoch 52, Batch 100, Loss: 0.2647716701030731\n",
      "\tEpoch 52, Batch 200, Loss: 0.3624700903892517\n",
      "\tEpoch 52, Batch 300, Loss: 0.3382148742675781\n",
      "\tEpoch 52, Batch 400, Loss: 0.24594444036483765\n",
      "\tEpoch 52, Batch 500, Loss: 0.5915471911430359\n",
      "\tEpoch 52, Batch 600, Loss: 0.25070109963417053\n",
      "\tEpoch 52, Batch 700, Loss: 0.44338470697402954\n",
      "\tEpoch 52, Batch 800, Loss: 0.35886988043785095\n",
      "\tEpoch 52, Batch 900, Loss: 0.3248259127140045\n",
      "\tEpoch 52, Batch 1000, Loss: 0.3841268718242645\n",
      "\tEpoch 52, Batch 1100, Loss: 0.3360469341278076\n",
      "\tEpoch 52, Batch 1200, Loss: 0.39237427711486816\n",
      "\tEpoch 52, Batch 1300, Loss: 0.34652113914489746\n",
      "\tEpoch 52, Batch 1400, Loss: 0.5905999541282654\n",
      "\tEpoch 52, Batch 1500, Loss: 0.30250269174575806\n",
      "\tEpoch 52, Batch 1600, Loss: 0.5137529373168945\n",
      "\tEpoch 52, Batch 1700, Loss: 0.3202262818813324\n",
      "\tEpoch 52, Batch 1800, Loss: 0.5278515219688416\n",
      "\tEpoch 52, Batch 1900, Loss: 0.31685230135917664\n",
      "\tEpoch 52, Batch 2000, Loss: 0.28414788842201233\n",
      "\tEpoch 52, Batch 2100, Loss: 0.46576887369155884\n",
      "Epoch: 52, Train Accuracy: 89.15%, Train Loss: 0.3801, Val Accuracy: 88.41%, Val Loss: 0.4180\n",
      "\tEpoch 53, Batch 100, Loss: 0.5091469287872314\n",
      "\tEpoch 53, Batch 200, Loss: 0.24684977531433105\n",
      "\tEpoch 53, Batch 300, Loss: 0.4087159335613251\n",
      "\tEpoch 53, Batch 400, Loss: 0.3804206848144531\n",
      "\tEpoch 53, Batch 500, Loss: 0.36286288499832153\n",
      "\tEpoch 53, Batch 600, Loss: 0.39433637261390686\n",
      "\tEpoch 53, Batch 700, Loss: 0.5795493125915527\n",
      "\tEpoch 53, Batch 800, Loss: 0.37826472520828247\n",
      "\tEpoch 53, Batch 900, Loss: 0.27343499660491943\n",
      "\tEpoch 53, Batch 1000, Loss: 0.2917189598083496\n",
      "\tEpoch 53, Batch 1100, Loss: 0.23691412806510925\n",
      "\tEpoch 53, Batch 1200, Loss: 0.2803919315338135\n",
      "\tEpoch 53, Batch 1300, Loss: 0.23403355479240417\n",
      "\tEpoch 53, Batch 1400, Loss: 0.32543009519577026\n",
      "\tEpoch 53, Batch 1500, Loss: 0.2920646667480469\n",
      "\tEpoch 53, Batch 1600, Loss: 0.3731084167957306\n",
      "\tEpoch 53, Batch 1700, Loss: 0.27421897649765015\n",
      "\tEpoch 53, Batch 1800, Loss: 0.29064705967903137\n",
      "\tEpoch 53, Batch 1900, Loss: 0.4355485141277313\n",
      "\tEpoch 53, Batch 2000, Loss: 0.4313378930091858\n",
      "\tEpoch 53, Batch 2100, Loss: 0.34159770607948303\n",
      "Epoch: 53, Train Accuracy: 89.18%, Train Loss: 0.3778, Val Accuracy: 87.81%, Val Loss: 0.4490\n",
      "\tEpoch 54, Batch 100, Loss: 0.41025158762931824\n",
      "\tEpoch 54, Batch 200, Loss: 0.37605801224708557\n",
      "\tEpoch 54, Batch 300, Loss: 0.4780673086643219\n",
      "\tEpoch 54, Batch 400, Loss: 0.40665245056152344\n",
      "\tEpoch 54, Batch 500, Loss: 0.2683647871017456\n",
      "\tEpoch 54, Batch 600, Loss: 0.379957377910614\n",
      "\tEpoch 54, Batch 700, Loss: 0.3449994623661041\n",
      "\tEpoch 54, Batch 800, Loss: 0.13707606494426727\n",
      "\tEpoch 54, Batch 900, Loss: 0.3226308524608612\n",
      "\tEpoch 54, Batch 1000, Loss: 0.23934035003185272\n",
      "\tEpoch 54, Batch 1100, Loss: 0.5843917727470398\n",
      "\tEpoch 54, Batch 1200, Loss: 0.549174427986145\n",
      "\tEpoch 54, Batch 1300, Loss: 0.3582380414009094\n",
      "\tEpoch 54, Batch 1400, Loss: 0.25353148579597473\n",
      "\tEpoch 54, Batch 1500, Loss: 0.4735586643218994\n",
      "\tEpoch 54, Batch 1600, Loss: 0.45532095432281494\n",
      "\tEpoch 54, Batch 1700, Loss: 0.2923651337623596\n",
      "\tEpoch 54, Batch 1800, Loss: 0.44723689556121826\n",
      "\tEpoch 54, Batch 1900, Loss: 0.4696352481842041\n",
      "\tEpoch 54, Batch 2000, Loss: 0.8018664121627808\n",
      "\tEpoch 54, Batch 2100, Loss: 0.43444961309432983\n",
      "Epoch: 54, Train Accuracy: 89.38%, Train Loss: 0.3717, Val Accuracy: 88.20%, Val Loss: 0.4385\n",
      "\tEpoch 55, Batch 100, Loss: 0.4474250376224518\n",
      "\tEpoch 55, Batch 200, Loss: 0.24446021020412445\n",
      "\tEpoch 55, Batch 300, Loss: 0.5135910511016846\n",
      "\tEpoch 55, Batch 400, Loss: 0.30891963839530945\n",
      "\tEpoch 55, Batch 500, Loss: 0.26288557052612305\n",
      "\tEpoch 55, Batch 600, Loss: 0.378974050283432\n",
      "\tEpoch 55, Batch 700, Loss: 0.5312433242797852\n",
      "\tEpoch 55, Batch 800, Loss: 0.23241831362247467\n",
      "\tEpoch 55, Batch 900, Loss: 0.34974151849746704\n",
      "\tEpoch 55, Batch 1000, Loss: 0.3569089472293854\n",
      "\tEpoch 55, Batch 1100, Loss: 0.4086063802242279\n",
      "\tEpoch 55, Batch 1200, Loss: 0.2887129783630371\n",
      "\tEpoch 55, Batch 1300, Loss: 0.32754385471343994\n",
      "\tEpoch 55, Batch 1400, Loss: 0.2929457426071167\n",
      "\tEpoch 55, Batch 1500, Loss: 0.5722131133079529\n",
      "\tEpoch 55, Batch 1600, Loss: 0.2683543860912323\n",
      "\tEpoch 55, Batch 1700, Loss: 0.39380672574043274\n",
      "\tEpoch 55, Batch 1800, Loss: 0.7681191563606262\n",
      "\tEpoch 55, Batch 1900, Loss: 0.3184698522090912\n",
      "\tEpoch 55, Batch 2000, Loss: 0.47669345140457153\n",
      "\tEpoch 55, Batch 2100, Loss: 0.37310680747032166\n",
      "Epoch: 55, Train Accuracy: 89.44%, Train Loss: 0.3701, Val Accuracy: 88.24%, Val Loss: 0.4107\n",
      "\tEpoch 56, Batch 100, Loss: 0.4095602333545685\n",
      "\tEpoch 56, Batch 200, Loss: 0.3763768970966339\n",
      "\tEpoch 56, Batch 300, Loss: 0.4774005711078644\n",
      "\tEpoch 56, Batch 400, Loss: 0.3778039216995239\n",
      "\tEpoch 56, Batch 500, Loss: 0.3370225131511688\n",
      "\tEpoch 56, Batch 600, Loss: 0.22313019633293152\n",
      "\tEpoch 56, Batch 700, Loss: 0.6038529872894287\n",
      "\tEpoch 56, Batch 800, Loss: 0.42743682861328125\n",
      "\tEpoch 56, Batch 900, Loss: 0.3639647960662842\n",
      "\tEpoch 56, Batch 1000, Loss: 0.3316730856895447\n",
      "\tEpoch 56, Batch 1100, Loss: 0.4161994755268097\n",
      "\tEpoch 56, Batch 1200, Loss: 0.3993358016014099\n",
      "\tEpoch 56, Batch 1300, Loss: 0.41264083981513977\n",
      "\tEpoch 56, Batch 1400, Loss: 0.3403071463108063\n",
      "\tEpoch 56, Batch 1500, Loss: 0.46572306752204895\n",
      "\tEpoch 56, Batch 1600, Loss: 0.16146108508110046\n",
      "\tEpoch 56, Batch 1700, Loss: 0.3143704831600189\n",
      "\tEpoch 56, Batch 1800, Loss: 0.30490222573280334\n",
      "\tEpoch 56, Batch 1900, Loss: 0.3401133716106415\n",
      "\tEpoch 56, Batch 2000, Loss: 0.4439239799976349\n",
      "\tEpoch 56, Batch 2100, Loss: 0.3801238238811493\n",
      "Epoch: 56, Train Accuracy: 89.56%, Train Loss: 0.3646, Val Accuracy: 88.45%, Val Loss: 0.4213\n",
      "\tEpoch 57, Batch 100, Loss: 0.3524007499217987\n",
      "\tEpoch 57, Batch 200, Loss: 0.16667887568473816\n",
      "\tEpoch 57, Batch 300, Loss: 0.6139379143714905\n",
      "\tEpoch 57, Batch 400, Loss: 0.498561292886734\n",
      "\tEpoch 57, Batch 500, Loss: 0.3908936083316803\n",
      "\tEpoch 57, Batch 600, Loss: 0.36779141426086426\n",
      "\tEpoch 57, Batch 700, Loss: 0.32603541016578674\n",
      "\tEpoch 57, Batch 800, Loss: 0.2098132073879242\n",
      "\tEpoch 57, Batch 900, Loss: 0.32077959179878235\n",
      "\tEpoch 57, Batch 1000, Loss: 0.4738699197769165\n",
      "\tEpoch 57, Batch 1100, Loss: 0.36093994975090027\n",
      "\tEpoch 57, Batch 1200, Loss: 0.319297194480896\n",
      "\tEpoch 57, Batch 1300, Loss: 0.37303414940834045\n",
      "\tEpoch 57, Batch 1400, Loss: 0.24987788498401642\n",
      "\tEpoch 57, Batch 1500, Loss: 0.4585235118865967\n",
      "\tEpoch 57, Batch 1600, Loss: 0.44278064370155334\n",
      "\tEpoch 57, Batch 1700, Loss: 0.3839026391506195\n",
      "\tEpoch 57, Batch 1800, Loss: 0.3541173040866852\n",
      "\tEpoch 57, Batch 1900, Loss: 0.5530709028244019\n",
      "\tEpoch 57, Batch 2000, Loss: 0.5394073128700256\n",
      "\tEpoch 57, Batch 2100, Loss: 0.6059991121292114\n",
      "Epoch: 57, Train Accuracy: 89.56%, Train Loss: 0.3641, Val Accuracy: 88.68%, Val Loss: 0.4053\n",
      "\tEpoch 58, Batch 100, Loss: 0.3357360363006592\n",
      "\tEpoch 58, Batch 200, Loss: 0.389763742685318\n",
      "\tEpoch 58, Batch 300, Loss: 0.3380259871482849\n",
      "\tEpoch 58, Batch 400, Loss: 0.4206730127334595\n",
      "\tEpoch 58, Batch 500, Loss: 0.4458582103252411\n",
      "\tEpoch 58, Batch 600, Loss: 0.36118149757385254\n",
      "\tEpoch 58, Batch 700, Loss: 0.3381892442703247\n",
      "\tEpoch 58, Batch 800, Loss: 0.1522064357995987\n",
      "\tEpoch 58, Batch 900, Loss: 0.747334897518158\n",
      "\tEpoch 58, Batch 1000, Loss: 0.45506882667541504\n",
      "\tEpoch 58, Batch 1100, Loss: 0.3486417829990387\n",
      "\tEpoch 58, Batch 1200, Loss: 0.36437833309173584\n",
      "\tEpoch 58, Batch 1300, Loss: 0.33045345544815063\n",
      "\tEpoch 58, Batch 1400, Loss: 0.3079161047935486\n",
      "\tEpoch 58, Batch 1500, Loss: 0.21478271484375\n",
      "\tEpoch 58, Batch 1600, Loss: 0.32670503854751587\n",
      "\tEpoch 58, Batch 1700, Loss: 0.2775988280773163\n",
      "\tEpoch 58, Batch 1800, Loss: 0.32159146666526794\n",
      "\tEpoch 58, Batch 1900, Loss: 0.2744286358356476\n",
      "\tEpoch 58, Batch 2000, Loss: 0.27624374628067017\n",
      "\tEpoch 58, Batch 2100, Loss: 0.30435824394226074\n",
      "Epoch: 58, Train Accuracy: 89.78%, Train Loss: 0.3581, Val Accuracy: 89.10%, Val Loss: 0.3960\n",
      "\tEpoch 59, Batch 100, Loss: 0.2306206375360489\n",
      "\tEpoch 59, Batch 200, Loss: 0.18964892625808716\n",
      "\tEpoch 59, Batch 300, Loss: 0.7044329047203064\n",
      "\tEpoch 59, Batch 400, Loss: 0.26810187101364136\n",
      "\tEpoch 59, Batch 500, Loss: 0.4452489912509918\n",
      "\tEpoch 59, Batch 600, Loss: 0.18979349732398987\n",
      "\tEpoch 59, Batch 700, Loss: 0.12417441606521606\n",
      "\tEpoch 59, Batch 800, Loss: 0.48228251934051514\n",
      "\tEpoch 59, Batch 900, Loss: 0.2880013883113861\n",
      "\tEpoch 59, Batch 1000, Loss: 0.4254835546016693\n",
      "\tEpoch 59, Batch 1100, Loss: 0.3793866038322449\n",
      "\tEpoch 59, Batch 1200, Loss: 0.4407532215118408\n",
      "\tEpoch 59, Batch 1300, Loss: 0.28006845712661743\n",
      "\tEpoch 59, Batch 1400, Loss: 0.3095889389514923\n",
      "\tEpoch 59, Batch 1500, Loss: 0.39300015568733215\n",
      "\tEpoch 59, Batch 1600, Loss: 0.349332332611084\n",
      "\tEpoch 59, Batch 1700, Loss: 0.5566704869270325\n",
      "\tEpoch 59, Batch 1800, Loss: 0.45570164918899536\n",
      "\tEpoch 59, Batch 1900, Loss: 0.2669403851032257\n",
      "\tEpoch 59, Batch 2000, Loss: 0.3434436619281769\n",
      "\tEpoch 59, Batch 2100, Loss: 0.352609783411026\n",
      "Epoch: 59, Train Accuracy: 89.75%, Train Loss: 0.3558, Val Accuracy: 89.12%, Val Loss: 0.3981\n",
      "\tEpoch 60, Batch 100, Loss: 0.361349880695343\n",
      "\tEpoch 60, Batch 200, Loss: 0.43118801712989807\n",
      "\tEpoch 60, Batch 300, Loss: 0.4864330589771271\n",
      "\tEpoch 60, Batch 400, Loss: 0.2974758744239807\n",
      "\tEpoch 60, Batch 500, Loss: 0.2102733701467514\n",
      "\tEpoch 60, Batch 600, Loss: 0.3088744580745697\n",
      "\tEpoch 60, Batch 700, Loss: 0.3974803388118744\n",
      "\tEpoch 60, Batch 800, Loss: 0.25059810280799866\n",
      "\tEpoch 60, Batch 900, Loss: 0.2671845853328705\n",
      "\tEpoch 60, Batch 1000, Loss: 0.21607856452465057\n",
      "\tEpoch 60, Batch 1100, Loss: 0.35445916652679443\n",
      "\tEpoch 60, Batch 1200, Loss: 0.5334456562995911\n",
      "\tEpoch 60, Batch 1300, Loss: 0.3861122131347656\n",
      "\tEpoch 60, Batch 1400, Loss: 0.4883279502391815\n",
      "\tEpoch 60, Batch 1500, Loss: 0.3269704580307007\n",
      "\tEpoch 60, Batch 1600, Loss: 0.3472878038883209\n",
      "\tEpoch 60, Batch 1700, Loss: 0.33596327900886536\n",
      "\tEpoch 60, Batch 1800, Loss: 0.3418658375740051\n",
      "\tEpoch 60, Batch 1900, Loss: 0.468807190656662\n",
      "\tEpoch 60, Batch 2000, Loss: 0.43112295866012573\n",
      "\tEpoch 60, Batch 2100, Loss: 0.5687651038169861\n",
      "Epoch: 60, Train Accuracy: 89.89%, Train Loss: 0.3530, Val Accuracy: 87.93%, Val Loss: 0.4392\n",
      "\tEpoch 61, Batch 100, Loss: 0.43703708052635193\n",
      "\tEpoch 61, Batch 200, Loss: 0.6149771809577942\n",
      "\tEpoch 61, Batch 300, Loss: 0.6287739276885986\n",
      "\tEpoch 61, Batch 400, Loss: 0.6271119117736816\n",
      "\tEpoch 61, Batch 500, Loss: 0.3526465594768524\n",
      "\tEpoch 61, Batch 600, Loss: 0.19319675862789154\n",
      "\tEpoch 61, Batch 700, Loss: 0.17840027809143066\n",
      "\tEpoch 61, Batch 800, Loss: 0.5110570788383484\n",
      "\tEpoch 61, Batch 900, Loss: 0.31161636114120483\n",
      "\tEpoch 61, Batch 1000, Loss: 0.43253153562545776\n",
      "\tEpoch 61, Batch 1100, Loss: 0.3193788528442383\n",
      "\tEpoch 61, Batch 1200, Loss: 0.45969119668006897\n",
      "\tEpoch 61, Batch 1300, Loss: 0.5541595816612244\n",
      "\tEpoch 61, Batch 1400, Loss: 0.30687379837036133\n",
      "\tEpoch 61, Batch 1500, Loss: 0.3731144070625305\n",
      "\tEpoch 61, Batch 1600, Loss: 0.2853999733924866\n",
      "\tEpoch 61, Batch 1700, Loss: 0.25748634338378906\n",
      "\tEpoch 61, Batch 1800, Loss: 0.2310064435005188\n",
      "\tEpoch 61, Batch 1900, Loss: 0.5628625154495239\n",
      "\tEpoch 61, Batch 2000, Loss: 0.3435181975364685\n",
      "\tEpoch 61, Batch 2100, Loss: 0.3680616319179535\n",
      "Epoch: 61, Train Accuracy: 89.81%, Train Loss: 0.3520, Val Accuracy: 88.83%, Val Loss: 0.4179\n",
      "\tEpoch 62, Batch 100, Loss: 0.24356640875339508\n",
      "\tEpoch 62, Batch 200, Loss: 0.41166990995407104\n",
      "\tEpoch 62, Batch 300, Loss: 0.31361475586891174\n",
      "\tEpoch 62, Batch 400, Loss: 0.2005840539932251\n",
      "\tEpoch 62, Batch 500, Loss: 0.37331995368003845\n",
      "\tEpoch 62, Batch 600, Loss: 0.1926230788230896\n",
      "\tEpoch 62, Batch 700, Loss: 0.47483524680137634\n",
      "\tEpoch 62, Batch 800, Loss: 0.37431979179382324\n",
      "\tEpoch 62, Batch 900, Loss: 0.27154266834259033\n",
      "\tEpoch 62, Batch 1000, Loss: 0.5265788435935974\n",
      "\tEpoch 62, Batch 1100, Loss: 0.26435625553131104\n",
      "\tEpoch 62, Batch 1200, Loss: 0.4660295248031616\n",
      "\tEpoch 62, Batch 1300, Loss: 0.4019884169101715\n",
      "\tEpoch 62, Batch 1400, Loss: 0.29197853803634644\n",
      "\tEpoch 62, Batch 1500, Loss: 0.1296478658914566\n",
      "\tEpoch 62, Batch 1600, Loss: 0.28280872106552124\n",
      "\tEpoch 62, Batch 1700, Loss: 0.5452456474304199\n",
      "\tEpoch 62, Batch 1800, Loss: 0.206195667386055\n",
      "\tEpoch 62, Batch 1900, Loss: 0.2210773080587387\n",
      "\tEpoch 62, Batch 2000, Loss: 0.35818883776664734\n",
      "\tEpoch 62, Batch 2100, Loss: 0.3712237477302551\n",
      "Epoch: 62, Train Accuracy: 89.89%, Train Loss: 0.3497, Val Accuracy: 88.89%, Val Loss: 0.4141\n",
      "\tEpoch 63, Batch 100, Loss: 0.20391549170017242\n",
      "\tEpoch 63, Batch 200, Loss: 0.22193193435668945\n",
      "\tEpoch 63, Batch 300, Loss: 0.22912050783634186\n",
      "\tEpoch 63, Batch 400, Loss: 0.6097707748413086\n",
      "\tEpoch 63, Batch 500, Loss: 0.2746157646179199\n",
      "\tEpoch 63, Batch 600, Loss: 0.32040122151374817\n",
      "\tEpoch 63, Batch 700, Loss: 0.33844754099845886\n",
      "\tEpoch 63, Batch 800, Loss: 0.34692153334617615\n",
      "\tEpoch 63, Batch 900, Loss: 0.2843998372554779\n",
      "\tEpoch 63, Batch 1000, Loss: 0.3738383948802948\n",
      "\tEpoch 63, Batch 1100, Loss: 0.20625776052474976\n",
      "\tEpoch 63, Batch 1200, Loss: 0.324259489774704\n",
      "\tEpoch 63, Batch 1300, Loss: 0.4582977890968323\n",
      "\tEpoch 63, Batch 1400, Loss: 0.25909826159477234\n",
      "\tEpoch 63, Batch 1500, Loss: 0.4769425094127655\n",
      "\tEpoch 63, Batch 1600, Loss: 0.3509366810321808\n",
      "\tEpoch 63, Batch 1700, Loss: 0.28403240442276\n",
      "\tEpoch 63, Batch 1800, Loss: 0.38499870896339417\n",
      "\tEpoch 63, Batch 1900, Loss: 0.21230322122573853\n",
      "\tEpoch 63, Batch 2000, Loss: 0.42866113781929016\n",
      "\tEpoch 63, Batch 2100, Loss: 0.31031695008277893\n",
      "Epoch: 63, Train Accuracy: 90.07%, Train Loss: 0.3436, Val Accuracy: 89.43%, Val Loss: 0.3958\n",
      "\tEpoch 64, Batch 100, Loss: 0.20208358764648438\n",
      "\tEpoch 64, Batch 200, Loss: 0.14273089170455933\n",
      "\tEpoch 64, Batch 300, Loss: 0.2880896329879761\n",
      "\tEpoch 64, Batch 400, Loss: 0.36916622519493103\n",
      "\tEpoch 64, Batch 500, Loss: 0.3232365548610687\n",
      "\tEpoch 64, Batch 600, Loss: 0.4516224265098572\n",
      "\tEpoch 64, Batch 700, Loss: 0.3949955999851227\n",
      "\tEpoch 64, Batch 800, Loss: 0.24892252683639526\n",
      "\tEpoch 64, Batch 900, Loss: 0.36180680990219116\n",
      "\tEpoch 64, Batch 1000, Loss: 0.3321865499019623\n",
      "\tEpoch 64, Batch 1100, Loss: 0.27550897002220154\n",
      "\tEpoch 64, Batch 1200, Loss: 0.40824949741363525\n",
      "\tEpoch 64, Batch 1300, Loss: 0.42983826994895935\n",
      "\tEpoch 64, Batch 1400, Loss: 0.3409917652606964\n",
      "\tEpoch 64, Batch 1500, Loss: 0.2864302694797516\n",
      "\tEpoch 64, Batch 1600, Loss: 0.3568463921546936\n",
      "\tEpoch 64, Batch 1700, Loss: 0.2542343735694885\n",
      "\tEpoch 64, Batch 1800, Loss: 0.4518464505672455\n",
      "\tEpoch 64, Batch 1900, Loss: 0.4075673520565033\n",
      "\tEpoch 64, Batch 2000, Loss: 0.34504109621047974\n",
      "\tEpoch 64, Batch 2100, Loss: 0.4776337146759033\n",
      "Epoch: 64, Train Accuracy: 90.08%, Train Loss: 0.3434, Val Accuracy: 88.71%, Val Loss: 0.4129\n",
      "\tEpoch 65, Batch 100, Loss: 0.19814127683639526\n",
      "\tEpoch 65, Batch 200, Loss: 0.3158465623855591\n",
      "\tEpoch 65, Batch 300, Loss: 0.35505837202072144\n",
      "\tEpoch 65, Batch 400, Loss: 0.30794858932495117\n",
      "\tEpoch 65, Batch 500, Loss: 0.13045787811279297\n",
      "\tEpoch 65, Batch 600, Loss: 0.43854832649230957\n",
      "\tEpoch 65, Batch 700, Loss: 0.29083776473999023\n",
      "\tEpoch 65, Batch 800, Loss: 0.2554073631763458\n",
      "\tEpoch 65, Batch 900, Loss: 0.4924061894416809\n",
      "\tEpoch 65, Batch 1000, Loss: 0.3314438462257385\n",
      "\tEpoch 65, Batch 1100, Loss: 0.495511919260025\n",
      "\tEpoch 65, Batch 1200, Loss: 0.27685442566871643\n",
      "\tEpoch 65, Batch 1300, Loss: 0.1409752517938614\n",
      "\tEpoch 65, Batch 1400, Loss: 0.24713702499866486\n",
      "\tEpoch 65, Batch 1500, Loss: 0.3260263502597809\n",
      "\tEpoch 65, Batch 1600, Loss: 0.3435593247413635\n",
      "\tEpoch 65, Batch 1700, Loss: 0.2931419610977173\n",
      "\tEpoch 65, Batch 1800, Loss: 0.25601398944854736\n",
      "\tEpoch 65, Batch 1900, Loss: 0.30172207951545715\n",
      "\tEpoch 65, Batch 2000, Loss: 0.27127811312675476\n",
      "\tEpoch 65, Batch 2100, Loss: 0.25653862953186035\n",
      "Epoch: 65, Train Accuracy: 90.16%, Train Loss: 0.3416, Val Accuracy: 89.63%, Val Loss: 0.3803\n",
      "\tEpoch 66, Batch 100, Loss: 0.27471885085105896\n",
      "\tEpoch 66, Batch 200, Loss: 0.5225430727005005\n",
      "\tEpoch 66, Batch 300, Loss: 0.35679036378860474\n",
      "\tEpoch 66, Batch 400, Loss: 0.3625052571296692\n",
      "\tEpoch 66, Batch 500, Loss: 0.31361693143844604\n",
      "\tEpoch 66, Batch 600, Loss: 0.28343072533607483\n",
      "\tEpoch 66, Batch 700, Loss: 0.21909070014953613\n",
      "\tEpoch 66, Batch 800, Loss: 0.3331354558467865\n",
      "\tEpoch 66, Batch 900, Loss: 0.2387637495994568\n",
      "\tEpoch 66, Batch 1000, Loss: 0.32374122738838196\n",
      "\tEpoch 66, Batch 1100, Loss: 0.4306415915489197\n",
      "\tEpoch 66, Batch 1200, Loss: 0.27179375290870667\n",
      "\tEpoch 66, Batch 1300, Loss: 0.6089711785316467\n",
      "\tEpoch 66, Batch 1400, Loss: 0.521580696105957\n",
      "\tEpoch 66, Batch 1500, Loss: 0.3362729847431183\n",
      "\tEpoch 66, Batch 1600, Loss: 0.35869500041007996\n",
      "\tEpoch 66, Batch 1700, Loss: 0.3443918228149414\n",
      "\tEpoch 66, Batch 1800, Loss: 0.5850915312767029\n",
      "\tEpoch 66, Batch 1900, Loss: 0.28523480892181396\n",
      "\tEpoch 66, Batch 2000, Loss: 0.35314232110977173\n",
      "\tEpoch 66, Batch 2100, Loss: 0.4005204737186432\n",
      "Epoch: 66, Train Accuracy: 90.37%, Train Loss: 0.3373, Val Accuracy: 90.02%, Val Loss: 0.3691\n",
      "\tEpoch 67, Batch 100, Loss: 0.25244027376174927\n",
      "\tEpoch 67, Batch 200, Loss: 0.282746821641922\n",
      "\tEpoch 67, Batch 300, Loss: 0.23879264295101166\n",
      "\tEpoch 67, Batch 400, Loss: 0.27229368686676025\n",
      "\tEpoch 67, Batch 500, Loss: 0.23834076523780823\n",
      "\tEpoch 67, Batch 600, Loss: 0.4484817683696747\n",
      "\tEpoch 67, Batch 700, Loss: 0.27882692217826843\n",
      "\tEpoch 67, Batch 800, Loss: 0.36602288484573364\n",
      "\tEpoch 67, Batch 900, Loss: 0.34672048687934875\n",
      "\tEpoch 67, Batch 1000, Loss: 0.4575417935848236\n",
      "\tEpoch 67, Batch 1100, Loss: 0.3257433772087097\n",
      "\tEpoch 67, Batch 1200, Loss: 0.2572050988674164\n",
      "\tEpoch 67, Batch 1300, Loss: 0.1964409351348877\n",
      "\tEpoch 67, Batch 1400, Loss: 0.45755648612976074\n",
      "\tEpoch 67, Batch 1500, Loss: 0.2682299315929413\n",
      "\tEpoch 67, Batch 1600, Loss: 0.3179308772087097\n",
      "\tEpoch 67, Batch 1700, Loss: 0.23815937340259552\n",
      "\tEpoch 67, Batch 1800, Loss: 0.4867335557937622\n",
      "\tEpoch 67, Batch 1900, Loss: 0.29686447978019714\n",
      "\tEpoch 67, Batch 2000, Loss: 0.27323997020721436\n",
      "\tEpoch 67, Batch 2100, Loss: 0.28684094548225403\n",
      "Epoch: 67, Train Accuracy: 90.41%, Train Loss: 0.3364, Val Accuracy: 89.63%, Val Loss: 0.3753\n",
      "\tEpoch 68, Batch 100, Loss: 0.34140217304229736\n",
      "\tEpoch 68, Batch 200, Loss: 0.5368355512619019\n",
      "\tEpoch 68, Batch 300, Loss: 0.30737626552581787\n",
      "\tEpoch 68, Batch 400, Loss: 0.2026432305574417\n",
      "\tEpoch 68, Batch 500, Loss: 0.25844329595565796\n",
      "\tEpoch 68, Batch 600, Loss: 0.4708746075630188\n",
      "\tEpoch 68, Batch 700, Loss: 0.404241144657135\n",
      "\tEpoch 68, Batch 800, Loss: 0.28701651096343994\n",
      "\tEpoch 68, Batch 900, Loss: 0.2995529770851135\n",
      "\tEpoch 68, Batch 1000, Loss: 0.26444387435913086\n",
      "\tEpoch 68, Batch 1100, Loss: 0.35225826501846313\n",
      "\tEpoch 68, Batch 1200, Loss: 0.24641795456409454\n",
      "\tEpoch 68, Batch 1300, Loss: 0.3442390263080597\n",
      "\tEpoch 68, Batch 1400, Loss: 0.2236863672733307\n",
      "\tEpoch 68, Batch 1500, Loss: 0.38849982619285583\n",
      "\tEpoch 68, Batch 1600, Loss: 0.1661371886730194\n",
      "\tEpoch 68, Batch 1700, Loss: 0.1606677621603012\n",
      "\tEpoch 68, Batch 1800, Loss: 0.39727887511253357\n",
      "\tEpoch 68, Batch 1900, Loss: 0.38020408153533936\n",
      "\tEpoch 68, Batch 2000, Loss: 0.36821267008781433\n",
      "\tEpoch 68, Batch 2100, Loss: 0.41472387313842773\n",
      "Epoch: 68, Train Accuracy: 90.37%, Train Loss: 0.3340, Val Accuracy: 89.18%, Val Loss: 0.3884\n",
      "\tEpoch 69, Batch 100, Loss: 0.2472095489501953\n",
      "\tEpoch 69, Batch 200, Loss: 0.5541703104972839\n",
      "\tEpoch 69, Batch 300, Loss: 0.2530674934387207\n",
      "\tEpoch 69, Batch 400, Loss: 0.1583269238471985\n",
      "\tEpoch 69, Batch 500, Loss: 0.3165566921234131\n",
      "\tEpoch 69, Batch 600, Loss: 0.2517133057117462\n",
      "\tEpoch 69, Batch 700, Loss: 0.47224387526512146\n",
      "\tEpoch 69, Batch 800, Loss: 0.3669833540916443\n",
      "\tEpoch 69, Batch 900, Loss: 0.26552721858024597\n",
      "\tEpoch 69, Batch 1000, Loss: 0.16765029728412628\n",
      "\tEpoch 69, Batch 1100, Loss: 0.281827449798584\n",
      "\tEpoch 69, Batch 1200, Loss: 0.30485162138938904\n",
      "\tEpoch 69, Batch 1300, Loss: 0.41022107005119324\n",
      "\tEpoch 69, Batch 1400, Loss: 0.3491668701171875\n",
      "\tEpoch 69, Batch 1500, Loss: 0.25882577896118164\n",
      "\tEpoch 69, Batch 1600, Loss: 0.25407058000564575\n",
      "\tEpoch 69, Batch 1700, Loss: 0.2904929220676422\n",
      "\tEpoch 69, Batch 1800, Loss: 0.10948474705219269\n",
      "\tEpoch 69, Batch 1900, Loss: 0.428075909614563\n",
      "\tEpoch 69, Batch 2000, Loss: 0.3082994520664215\n",
      "\tEpoch 69, Batch 2100, Loss: 0.7782953381538391\n",
      "Epoch: 69, Train Accuracy: 90.53%, Train Loss: 0.3303, Val Accuracy: 88.89%, Val Loss: 0.4092\n",
      "\tEpoch 70, Batch 100, Loss: 0.19772730767726898\n",
      "\tEpoch 70, Batch 200, Loss: 0.2531423270702362\n",
      "\tEpoch 70, Batch 300, Loss: 0.2936035096645355\n",
      "\tEpoch 70, Batch 400, Loss: 0.31770190596580505\n",
      "\tEpoch 70, Batch 500, Loss: 0.4622335135936737\n",
      "\tEpoch 70, Batch 600, Loss: 0.25811535120010376\n",
      "\tEpoch 70, Batch 700, Loss: 0.4383716583251953\n",
      "\tEpoch 70, Batch 800, Loss: 0.5135226845741272\n",
      "\tEpoch 70, Batch 900, Loss: 0.524137020111084\n",
      "\tEpoch 70, Batch 1000, Loss: 0.24866443872451782\n",
      "\tEpoch 70, Batch 1100, Loss: 0.24998080730438232\n",
      "\tEpoch 70, Batch 1200, Loss: 0.4513506591320038\n",
      "\tEpoch 70, Batch 1300, Loss: 0.28163135051727295\n",
      "\tEpoch 70, Batch 1400, Loss: 0.34953877329826355\n",
      "\tEpoch 70, Batch 1500, Loss: 0.36605095863342285\n",
      "\tEpoch 70, Batch 1600, Loss: 0.34271320700645447\n",
      "\tEpoch 70, Batch 1700, Loss: 0.5506584644317627\n",
      "\tEpoch 70, Batch 1800, Loss: 0.3135739862918854\n",
      "\tEpoch 70, Batch 1900, Loss: 0.3653964698314667\n",
      "\tEpoch 70, Batch 2000, Loss: 0.39645981788635254\n",
      "\tEpoch 70, Batch 2100, Loss: 0.3681742250919342\n",
      "Epoch: 70, Train Accuracy: 90.62%, Train Loss: 0.3256, Val Accuracy: 89.37%, Val Loss: 0.3814\n",
      "\tEpoch 71, Batch 100, Loss: 0.1431778073310852\n",
      "\tEpoch 71, Batch 200, Loss: 0.2755638659000397\n",
      "\tEpoch 71, Batch 300, Loss: 0.5872606039047241\n",
      "\tEpoch 71, Batch 400, Loss: 0.4324074685573578\n",
      "\tEpoch 71, Batch 500, Loss: 0.3150908648967743\n",
      "\tEpoch 71, Batch 600, Loss: 0.3180186450481415\n",
      "\tEpoch 71, Batch 700, Loss: 0.26668938994407654\n",
      "\tEpoch 71, Batch 800, Loss: 0.34193098545074463\n",
      "\tEpoch 71, Batch 900, Loss: 0.19146579504013062\n",
      "\tEpoch 71, Batch 1000, Loss: 0.2839624583721161\n",
      "\tEpoch 71, Batch 1100, Loss: 0.5005744695663452\n",
      "\tEpoch 71, Batch 1200, Loss: 0.22941860556602478\n",
      "\tEpoch 71, Batch 1300, Loss: 0.2746863067150116\n",
      "\tEpoch 71, Batch 1400, Loss: 0.42956113815307617\n",
      "\tEpoch 71, Batch 1500, Loss: 0.22156578302383423\n",
      "\tEpoch 71, Batch 1600, Loss: 0.3104966878890991\n",
      "\tEpoch 71, Batch 1700, Loss: 0.5739107131958008\n",
      "\tEpoch 71, Batch 1800, Loss: 0.30978673696517944\n",
      "\tEpoch 71, Batch 1900, Loss: 0.22216546535491943\n",
      "\tEpoch 71, Batch 2000, Loss: 0.22924429178237915\n",
      "\tEpoch 71, Batch 2100, Loss: 0.12272953987121582\n",
      "Epoch: 71, Train Accuracy: 90.59%, Train Loss: 0.3253, Val Accuracy: 88.62%, Val Loss: 0.4155\n",
      "\tEpoch 72, Batch 100, Loss: 0.31301337480545044\n",
      "\tEpoch 72, Batch 200, Loss: 0.16954825818538666\n",
      "\tEpoch 72, Batch 300, Loss: 0.3745811879634857\n",
      "\tEpoch 72, Batch 400, Loss: 0.2232670783996582\n",
      "\tEpoch 72, Batch 500, Loss: 0.2713313698768616\n",
      "\tEpoch 72, Batch 600, Loss: 0.34772446751594543\n",
      "\tEpoch 72, Batch 700, Loss: 0.28368592262268066\n",
      "\tEpoch 72, Batch 800, Loss: 0.3954331576824188\n",
      "\tEpoch 72, Batch 900, Loss: 0.2789149284362793\n",
      "\tEpoch 72, Batch 1000, Loss: 0.20436789095401764\n",
      "\tEpoch 72, Batch 1100, Loss: 0.28165557980537415\n",
      "\tEpoch 72, Batch 1200, Loss: 0.31776443123817444\n",
      "\tEpoch 72, Batch 1300, Loss: 0.4021487534046173\n",
      "\tEpoch 72, Batch 1400, Loss: 0.2434413582086563\n",
      "\tEpoch 72, Batch 1500, Loss: 0.4383406639099121\n",
      "\tEpoch 72, Batch 1600, Loss: 0.20003053545951843\n",
      "\tEpoch 72, Batch 1700, Loss: 0.23541413247585297\n",
      "\tEpoch 72, Batch 1800, Loss: 0.46216022968292236\n",
      "\tEpoch 72, Batch 1900, Loss: 0.25375890731811523\n",
      "\tEpoch 72, Batch 2000, Loss: 0.260470449924469\n",
      "\tEpoch 72, Batch 2100, Loss: 0.3772316575050354\n",
      "Epoch: 72, Train Accuracy: 90.73%, Train Loss: 0.3210, Val Accuracy: 90.38%, Val Loss: 0.3531\n",
      "\tEpoch 73, Batch 100, Loss: 0.3558073043823242\n",
      "\tEpoch 73, Batch 200, Loss: 0.2574821710586548\n",
      "\tEpoch 73, Batch 300, Loss: 0.3517950773239136\n",
      "\tEpoch 73, Batch 400, Loss: 0.3314281702041626\n",
      "\tEpoch 73, Batch 500, Loss: 0.21952883899211884\n",
      "\tEpoch 73, Batch 600, Loss: 0.20417679846286774\n",
      "\tEpoch 73, Batch 700, Loss: 0.34742289781570435\n",
      "\tEpoch 73, Batch 800, Loss: 0.4413401186466217\n",
      "\tEpoch 73, Batch 900, Loss: 0.3785306215286255\n",
      "\tEpoch 73, Batch 1000, Loss: 0.2610113024711609\n",
      "\tEpoch 73, Batch 1100, Loss: 0.31141576170921326\n",
      "\tEpoch 73, Batch 1200, Loss: 0.6270526051521301\n",
      "\tEpoch 73, Batch 1300, Loss: 0.34127435088157654\n",
      "\tEpoch 73, Batch 1400, Loss: 0.3615555167198181\n",
      "\tEpoch 73, Batch 1500, Loss: 0.2895548343658447\n",
      "\tEpoch 73, Batch 1600, Loss: 0.23969613015651703\n",
      "\tEpoch 73, Batch 1700, Loss: 0.3612983524799347\n",
      "\tEpoch 73, Batch 1800, Loss: 0.37392809987068176\n",
      "\tEpoch 73, Batch 1900, Loss: 0.2277679592370987\n",
      "\tEpoch 73, Batch 2000, Loss: 0.3346373736858368\n",
      "\tEpoch 73, Batch 2100, Loss: 0.32972225546836853\n",
      "Epoch: 73, Train Accuracy: 90.77%, Train Loss: 0.3208, Val Accuracy: 89.86%, Val Loss: 0.3696\n",
      "\tEpoch 74, Batch 100, Loss: 0.3468261659145355\n",
      "\tEpoch 74, Batch 200, Loss: 0.4220336973667145\n",
      "\tEpoch 74, Batch 300, Loss: 0.1983349621295929\n",
      "\tEpoch 74, Batch 400, Loss: 0.2913886308670044\n",
      "\tEpoch 74, Batch 500, Loss: 0.21370448172092438\n",
      "\tEpoch 74, Batch 600, Loss: 0.3484262526035309\n",
      "\tEpoch 74, Batch 700, Loss: 0.2874964475631714\n",
      "\tEpoch 74, Batch 800, Loss: 0.3220783472061157\n",
      "\tEpoch 74, Batch 900, Loss: 0.42459821701049805\n",
      "\tEpoch 74, Batch 1000, Loss: 0.2901204526424408\n",
      "\tEpoch 74, Batch 1100, Loss: 0.4727271795272827\n",
      "\tEpoch 74, Batch 1200, Loss: 0.38537782430648804\n",
      "\tEpoch 74, Batch 1300, Loss: 0.32829561829566956\n",
      "\tEpoch 74, Batch 1400, Loss: 0.18247991800308228\n",
      "\tEpoch 74, Batch 1500, Loss: 0.3584851920604706\n",
      "\tEpoch 74, Batch 1600, Loss: 0.34313255548477173\n",
      "\tEpoch 74, Batch 1700, Loss: 0.291957288980484\n",
      "\tEpoch 74, Batch 1800, Loss: 0.18981477618217468\n",
      "\tEpoch 74, Batch 1900, Loss: 0.25848472118377686\n",
      "\tEpoch 74, Batch 2000, Loss: 0.2382717728614807\n",
      "\tEpoch 74, Batch 2100, Loss: 0.44758373498916626\n",
      "Epoch: 74, Train Accuracy: 90.86%, Train Loss: 0.3194, Val Accuracy: 88.61%, Val Loss: 0.4198\n",
      "\tEpoch 75, Batch 100, Loss: 0.19633689522743225\n",
      "\tEpoch 75, Batch 200, Loss: 0.333227276802063\n",
      "\tEpoch 75, Batch 300, Loss: 0.3489319682121277\n",
      "\tEpoch 75, Batch 400, Loss: 0.28889918327331543\n",
      "\tEpoch 75, Batch 500, Loss: 0.31217437982559204\n",
      "\tEpoch 75, Batch 600, Loss: 0.3396535813808441\n",
      "\tEpoch 75, Batch 700, Loss: 0.3883439600467682\n",
      "\tEpoch 75, Batch 800, Loss: 0.22026580572128296\n",
      "\tEpoch 75, Batch 900, Loss: 0.2922199070453644\n",
      "\tEpoch 75, Batch 1000, Loss: 0.3495280146598816\n",
      "\tEpoch 75, Batch 1100, Loss: 0.2116924673318863\n",
      "\tEpoch 75, Batch 1200, Loss: 0.3044808506965637\n",
      "\tEpoch 75, Batch 1300, Loss: 0.36499011516571045\n",
      "\tEpoch 75, Batch 1400, Loss: 0.3395344018936157\n",
      "\tEpoch 75, Batch 1500, Loss: 0.20144060254096985\n",
      "\tEpoch 75, Batch 1600, Loss: 0.28428468108177185\n",
      "\tEpoch 75, Batch 1700, Loss: 0.1777729094028473\n",
      "\tEpoch 75, Batch 1800, Loss: 0.3339335322380066\n",
      "\tEpoch 75, Batch 1900, Loss: 0.49505504965782166\n",
      "\tEpoch 75, Batch 2000, Loss: 0.33056095242500305\n",
      "\tEpoch 75, Batch 2100, Loss: 0.3240027129650116\n",
      "Epoch: 75, Train Accuracy: 90.99%, Train Loss: 0.3144, Val Accuracy: 89.39%, Val Loss: 0.3851\n",
      "\tEpoch 76, Batch 100, Loss: 0.3264950215816498\n",
      "\tEpoch 76, Batch 200, Loss: 0.250159353017807\n",
      "\tEpoch 76, Batch 300, Loss: 0.26716259121894836\n",
      "\tEpoch 76, Batch 400, Loss: 0.2043844759464264\n",
      "\tEpoch 76, Batch 500, Loss: 0.3547511696815491\n",
      "\tEpoch 76, Batch 600, Loss: 0.36942145228385925\n",
      "\tEpoch 76, Batch 700, Loss: 0.42606937885284424\n",
      "\tEpoch 76, Batch 800, Loss: 0.15376895666122437\n",
      "\tEpoch 76, Batch 900, Loss: 0.6253941059112549\n",
      "\tEpoch 76, Batch 1000, Loss: 0.13360391557216644\n",
      "\tEpoch 76, Batch 1100, Loss: 0.4107523262500763\n",
      "\tEpoch 76, Batch 1200, Loss: 0.36954525113105774\n",
      "\tEpoch 76, Batch 1300, Loss: 0.26626768708229065\n",
      "\tEpoch 76, Batch 1400, Loss: 0.3445996344089508\n",
      "\tEpoch 76, Batch 1500, Loss: 0.33637723326683044\n",
      "\tEpoch 76, Batch 1600, Loss: 0.5295217037200928\n",
      "\tEpoch 76, Batch 1700, Loss: 0.3128216564655304\n",
      "\tEpoch 76, Batch 1800, Loss: 0.19852103292942047\n",
      "\tEpoch 76, Batch 1900, Loss: 0.38802701234817505\n",
      "\tEpoch 76, Batch 2000, Loss: 0.3021524250507355\n",
      "\tEpoch 76, Batch 2100, Loss: 0.34373795986175537\n",
      "Epoch: 76, Train Accuracy: 90.94%, Train Loss: 0.3143, Val Accuracy: 89.07%, Val Loss: 0.3974\n",
      "\tEpoch 77, Batch 100, Loss: 0.3192930221557617\n",
      "\tEpoch 77, Batch 200, Loss: 0.3561924695968628\n",
      "\tEpoch 77, Batch 300, Loss: 0.2680838108062744\n",
      "\tEpoch 77, Batch 400, Loss: 0.6456567645072937\n",
      "\tEpoch 77, Batch 500, Loss: 0.4351576268672943\n",
      "\tEpoch 77, Batch 600, Loss: 0.1969151645898819\n",
      "\tEpoch 77, Batch 700, Loss: 0.22202923893928528\n",
      "\tEpoch 77, Batch 800, Loss: 0.3619694411754608\n",
      "\tEpoch 77, Batch 900, Loss: 0.4757763743400574\n",
      "\tEpoch 77, Batch 1000, Loss: 0.440459668636322\n",
      "\tEpoch 77, Batch 1100, Loss: 0.3144448399543762\n",
      "\tEpoch 77, Batch 1200, Loss: 0.3102028965950012\n",
      "\tEpoch 77, Batch 1300, Loss: 0.36219164729118347\n",
      "\tEpoch 77, Batch 1400, Loss: 0.3590530455112457\n",
      "\tEpoch 77, Batch 1500, Loss: 0.4901231825351715\n",
      "\tEpoch 77, Batch 1600, Loss: 0.22588513791561127\n",
      "\tEpoch 77, Batch 1700, Loss: 0.39908933639526367\n",
      "\tEpoch 77, Batch 1800, Loss: 0.25884145498275757\n",
      "\tEpoch 77, Batch 1900, Loss: 0.20723043382167816\n",
      "\tEpoch 77, Batch 2000, Loss: 0.3285868465900421\n",
      "\tEpoch 77, Batch 2100, Loss: 0.19818124175071716\n",
      "Epoch: 77, Train Accuracy: 90.93%, Train Loss: 0.3128, Val Accuracy: 88.26%, Val Loss: 0.4187\n",
      "\tEpoch 78, Batch 100, Loss: 0.3666936755180359\n",
      "\tEpoch 78, Batch 200, Loss: 0.3779471516609192\n",
      "\tEpoch 78, Batch 300, Loss: 0.19152112305164337\n",
      "\tEpoch 78, Batch 400, Loss: 0.4070470333099365\n",
      "\tEpoch 78, Batch 500, Loss: 0.47942590713500977\n",
      "\tEpoch 78, Batch 600, Loss: 0.2737569808959961\n",
      "\tEpoch 78, Batch 700, Loss: 0.3019539713859558\n",
      "\tEpoch 78, Batch 800, Loss: 0.1744501292705536\n",
      "\tEpoch 78, Batch 900, Loss: 0.35933026671409607\n",
      "\tEpoch 78, Batch 1000, Loss: 0.21019268035888672\n",
      "\tEpoch 78, Batch 1100, Loss: 0.45015284419059753\n",
      "\tEpoch 78, Batch 1200, Loss: 0.39586031436920166\n",
      "\tEpoch 78, Batch 1300, Loss: 0.30396321415901184\n",
      "\tEpoch 78, Batch 1400, Loss: 0.38631418347358704\n",
      "\tEpoch 78, Batch 1500, Loss: 0.37612608075141907\n",
      "\tEpoch 78, Batch 1600, Loss: 0.3061680793762207\n",
      "\tEpoch 78, Batch 1700, Loss: 0.3994842767715454\n",
      "\tEpoch 78, Batch 1800, Loss: 0.30959925055503845\n",
      "\tEpoch 78, Batch 1900, Loss: 0.30291348695755005\n",
      "\tEpoch 78, Batch 2000, Loss: 0.17632430791854858\n",
      "\tEpoch 78, Batch 2100, Loss: 0.4759179949760437\n",
      "Epoch: 78, Train Accuracy: 91.06%, Train Loss: 0.3113, Val Accuracy: 90.34%, Val Loss: 0.3570\n",
      "\tEpoch 79, Batch 100, Loss: 0.3138639032840729\n",
      "\tEpoch 79, Batch 200, Loss: 0.307783305644989\n",
      "\tEpoch 79, Batch 300, Loss: 0.252141535282135\n",
      "\tEpoch 79, Batch 400, Loss: 0.41610363125801086\n",
      "\tEpoch 79, Batch 500, Loss: 0.6956760287284851\n",
      "\tEpoch 79, Batch 600, Loss: 0.25980475544929504\n",
      "\tEpoch 79, Batch 700, Loss: 0.34391099214553833\n",
      "\tEpoch 79, Batch 800, Loss: 0.16053025424480438\n",
      "\tEpoch 79, Batch 900, Loss: 0.31534555554389954\n",
      "\tEpoch 79, Batch 1000, Loss: 0.3207632005214691\n",
      "\tEpoch 79, Batch 1100, Loss: 0.19576583802700043\n",
      "\tEpoch 79, Batch 1200, Loss: 0.15299971401691437\n",
      "\tEpoch 79, Batch 1300, Loss: 0.37260034680366516\n",
      "\tEpoch 79, Batch 1400, Loss: 0.27066949009895325\n",
      "\tEpoch 79, Batch 1500, Loss: 0.3116835355758667\n",
      "\tEpoch 79, Batch 1600, Loss: 0.26856133341789246\n",
      "\tEpoch 79, Batch 1700, Loss: 0.44138050079345703\n",
      "\tEpoch 79, Batch 1800, Loss: 0.28068971633911133\n",
      "\tEpoch 79, Batch 1900, Loss: 0.337319940328598\n",
      "\tEpoch 79, Batch 2000, Loss: 0.22002707421779633\n",
      "\tEpoch 79, Batch 2100, Loss: 0.41021525859832764\n",
      "Epoch: 79, Train Accuracy: 91.00%, Train Loss: 0.3101, Val Accuracy: 90.05%, Val Loss: 0.3533\n",
      "\tEpoch 80, Batch 100, Loss: 0.26852715015411377\n",
      "\tEpoch 80, Batch 200, Loss: 0.36259207129478455\n",
      "\tEpoch 80, Batch 300, Loss: 0.5134816765785217\n",
      "\tEpoch 80, Batch 400, Loss: 0.21205313503742218\n",
      "\tEpoch 80, Batch 500, Loss: 0.17139215767383575\n",
      "\tEpoch 80, Batch 600, Loss: 0.34310075640678406\n",
      "\tEpoch 80, Batch 700, Loss: 0.30888694524765015\n",
      "\tEpoch 80, Batch 800, Loss: 0.22446030378341675\n",
      "\tEpoch 80, Batch 900, Loss: 0.2624532878398895\n",
      "\tEpoch 80, Batch 1000, Loss: 0.45200684666633606\n",
      "\tEpoch 80, Batch 1100, Loss: 0.17383703589439392\n",
      "\tEpoch 80, Batch 1200, Loss: 0.26236599683761597\n",
      "\tEpoch 80, Batch 1300, Loss: 0.4574021100997925\n",
      "\tEpoch 80, Batch 1400, Loss: 0.3363927900791168\n",
      "\tEpoch 80, Batch 1500, Loss: 0.2986106276512146\n",
      "\tEpoch 80, Batch 1600, Loss: 0.42389345169067383\n",
      "\tEpoch 80, Batch 1700, Loss: 0.18013446033000946\n",
      "\tEpoch 80, Batch 1800, Loss: 0.4972982108592987\n",
      "\tEpoch 80, Batch 1900, Loss: 0.3605799674987793\n",
      "\tEpoch 80, Batch 2000, Loss: 0.26182541251182556\n",
      "\tEpoch 80, Batch 2100, Loss: 0.3327379822731018\n",
      "Epoch: 80, Train Accuracy: 91.24%, Train Loss: 0.3057, Val Accuracy: 90.19%, Val Loss: 0.3628\n",
      "\tEpoch 81, Batch 100, Loss: 0.27438318729400635\n",
      "\tEpoch 81, Batch 200, Loss: 0.3477936089038849\n",
      "\tEpoch 81, Batch 300, Loss: 0.33226847648620605\n",
      "\tEpoch 81, Batch 400, Loss: 0.3409532904624939\n",
      "\tEpoch 81, Batch 500, Loss: 0.14073239266872406\n",
      "\tEpoch 81, Batch 600, Loss: 0.2625638246536255\n",
      "\tEpoch 81, Batch 700, Loss: 0.12128335982561111\n",
      "\tEpoch 81, Batch 800, Loss: 0.14510126411914825\n",
      "\tEpoch 81, Batch 900, Loss: 0.2109089344739914\n",
      "\tEpoch 81, Batch 1000, Loss: 0.22855500876903534\n",
      "\tEpoch 81, Batch 1100, Loss: 0.2335895597934723\n",
      "\tEpoch 81, Batch 1200, Loss: 0.20069219172000885\n",
      "\tEpoch 81, Batch 1300, Loss: 0.39526036381721497\n",
      "\tEpoch 81, Batch 1400, Loss: 0.28389328718185425\n",
      "\tEpoch 81, Batch 1500, Loss: 0.12132502347230911\n",
      "\tEpoch 81, Batch 1600, Loss: 0.22709789872169495\n",
      "\tEpoch 81, Batch 1700, Loss: 0.29480838775634766\n",
      "\tEpoch 81, Batch 1800, Loss: 0.2839289605617523\n",
      "\tEpoch 81, Batch 1900, Loss: 0.1482601761817932\n",
      "\tEpoch 81, Batch 2000, Loss: 0.20494344830513\n",
      "\tEpoch 81, Batch 2100, Loss: 0.22986851632595062\n",
      "Epoch: 81, Train Accuracy: 92.76%, Train Loss: 0.2566, Val Accuracy: 92.16%, Val Loss: 0.2883\n",
      "\tEpoch 82, Batch 100, Loss: 0.14845579862594604\n",
      "\tEpoch 82, Batch 200, Loss: 0.26409095525741577\n",
      "\tEpoch 82, Batch 300, Loss: 0.30487939715385437\n",
      "\tEpoch 82, Batch 400, Loss: 0.12768660485744476\n",
      "\tEpoch 82, Batch 500, Loss: 0.2822834849357605\n",
      "\tEpoch 82, Batch 600, Loss: 0.19152569770812988\n",
      "\tEpoch 82, Batch 700, Loss: 0.20229561626911163\n",
      "\tEpoch 82, Batch 800, Loss: 0.3256447911262512\n",
      "\tEpoch 82, Batch 900, Loss: 0.20771531760692596\n",
      "\tEpoch 82, Batch 1000, Loss: 0.34506332874298096\n",
      "\tEpoch 82, Batch 1100, Loss: 0.13746121525764465\n",
      "\tEpoch 82, Batch 1200, Loss: 0.21292823553085327\n",
      "\tEpoch 82, Batch 1300, Loss: 0.08780501782894135\n",
      "\tEpoch 82, Batch 1400, Loss: 0.26944947242736816\n",
      "\tEpoch 82, Batch 1500, Loss: 0.25350767374038696\n",
      "\tEpoch 82, Batch 1600, Loss: 0.41197654604911804\n",
      "\tEpoch 82, Batch 1700, Loss: 0.43404310941696167\n",
      "\tEpoch 82, Batch 1800, Loss: 0.19663621485233307\n",
      "\tEpoch 82, Batch 1900, Loss: 0.24959948658943176\n",
      "\tEpoch 82, Batch 2000, Loss: 0.1784805953502655\n",
      "\tEpoch 82, Batch 2100, Loss: 0.16761049628257751\n",
      "Epoch: 82, Train Accuracy: 93.01%, Train Loss: 0.2476, Val Accuracy: 92.23%, Val Loss: 0.2913\n",
      "\tEpoch 83, Batch 100, Loss: 0.31105735898017883\n",
      "\tEpoch 83, Batch 200, Loss: 0.13026627898216248\n",
      "\tEpoch 83, Batch 300, Loss: 0.19417013227939606\n",
      "\tEpoch 83, Batch 400, Loss: 0.2816906273365021\n",
      "\tEpoch 83, Batch 500, Loss: 0.12518024444580078\n",
      "\tEpoch 83, Batch 600, Loss: 0.3684771955013275\n",
      "\tEpoch 83, Batch 700, Loss: 0.35154634714126587\n",
      "\tEpoch 83, Batch 800, Loss: 0.31284552812576294\n",
      "\tEpoch 83, Batch 900, Loss: 0.13739244639873505\n",
      "\tEpoch 83, Batch 1000, Loss: 0.3577117621898651\n",
      "\tEpoch 83, Batch 1100, Loss: 0.17702534794807434\n",
      "\tEpoch 83, Batch 1200, Loss: 0.10146664083003998\n",
      "\tEpoch 83, Batch 1300, Loss: 0.11958816647529602\n",
      "\tEpoch 83, Batch 1400, Loss: 0.3972197473049164\n",
      "\tEpoch 83, Batch 1500, Loss: 0.2759453356266022\n",
      "\tEpoch 83, Batch 1600, Loss: 0.27555203437805176\n",
      "\tEpoch 83, Batch 1700, Loss: 0.24604322016239166\n",
      "\tEpoch 83, Batch 1800, Loss: 0.2207506000995636\n",
      "\tEpoch 83, Batch 1900, Loss: 0.2860511541366577\n",
      "\tEpoch 83, Batch 2000, Loss: 0.2295195460319519\n",
      "\tEpoch 83, Batch 2100, Loss: 0.24124568700790405\n",
      "Epoch: 83, Train Accuracy: 93.16%, Train Loss: 0.2445, Val Accuracy: 91.96%, Val Loss: 0.2922\n",
      "\tEpoch 84, Batch 100, Loss: 0.2595450282096863\n",
      "\tEpoch 84, Batch 200, Loss: 0.42558753490448\n",
      "\tEpoch 84, Batch 300, Loss: 0.4846839904785156\n",
      "\tEpoch 84, Batch 400, Loss: 0.1855688989162445\n",
      "\tEpoch 84, Batch 500, Loss: 0.11417234688997269\n",
      "\tEpoch 84, Batch 600, Loss: 0.3159347176551819\n",
      "\tEpoch 84, Batch 700, Loss: 0.27574753761291504\n",
      "\tEpoch 84, Batch 800, Loss: 0.12868280708789825\n",
      "\tEpoch 84, Batch 900, Loss: 0.3006875216960907\n",
      "\tEpoch 84, Batch 1000, Loss: 0.2661203145980835\n",
      "\tEpoch 84, Batch 1100, Loss: 0.1502545028924942\n",
      "\tEpoch 84, Batch 1200, Loss: 0.21386703848838806\n",
      "\tEpoch 84, Batch 1300, Loss: 0.3101041316986084\n",
      "\tEpoch 84, Batch 1400, Loss: 0.3140689730644226\n",
      "\tEpoch 84, Batch 1500, Loss: 0.3708333969116211\n",
      "\tEpoch 84, Batch 1600, Loss: 0.26241424679756165\n",
      "\tEpoch 84, Batch 1700, Loss: 0.19829025864601135\n",
      "\tEpoch 84, Batch 1800, Loss: 0.37303945422172546\n",
      "\tEpoch 84, Batch 1900, Loss: 0.22381941974163055\n",
      "\tEpoch 84, Batch 2000, Loss: 0.14106962084770203\n",
      "\tEpoch 84, Batch 2100, Loss: 0.1728373020887375\n",
      "Epoch: 84, Train Accuracy: 93.09%, Train Loss: 0.2436, Val Accuracy: 92.19%, Val Loss: 0.2910\n",
      "\tEpoch 85, Batch 100, Loss: 0.23856905102729797\n",
      "\tEpoch 85, Batch 200, Loss: 0.11206468939781189\n",
      "\tEpoch 85, Batch 300, Loss: 0.2893330752849579\n",
      "\tEpoch 85, Batch 400, Loss: 0.10396504402160645\n",
      "\tEpoch 85, Batch 500, Loss: 0.15399257838726044\n",
      "\tEpoch 85, Batch 600, Loss: 0.33241286873817444\n",
      "\tEpoch 85, Batch 700, Loss: 0.30810976028442383\n",
      "\tEpoch 85, Batch 800, Loss: 0.3321451246738434\n",
      "\tEpoch 85, Batch 900, Loss: 0.25721654295921326\n",
      "\tEpoch 85, Batch 1000, Loss: 0.2203505039215088\n",
      "\tEpoch 85, Batch 1100, Loss: 0.1533474624156952\n",
      "\tEpoch 85, Batch 1200, Loss: 0.1715090423822403\n",
      "\tEpoch 85, Batch 1300, Loss: 0.2880389392375946\n",
      "\tEpoch 85, Batch 1400, Loss: 0.3381636440753937\n",
      "\tEpoch 85, Batch 1500, Loss: 0.188388854265213\n",
      "\tEpoch 85, Batch 1600, Loss: 0.35683220624923706\n",
      "\tEpoch 85, Batch 1700, Loss: 0.16662417352199554\n",
      "\tEpoch 85, Batch 1800, Loss: 0.15859192609786987\n",
      "\tEpoch 85, Batch 1900, Loss: 0.2672305703163147\n",
      "\tEpoch 85, Batch 2000, Loss: 0.17782743275165558\n",
      "\tEpoch 85, Batch 2100, Loss: 0.23702697455883026\n",
      "Epoch: 85, Train Accuracy: 93.08%, Train Loss: 0.2430, Val Accuracy: 92.07%, Val Loss: 0.2939\n",
      "\tEpoch 86, Batch 100, Loss: 0.22984276711940765\n",
      "\tEpoch 86, Batch 200, Loss: 0.1467438042163849\n",
      "\tEpoch 86, Batch 300, Loss: 0.32109150290489197\n",
      "\tEpoch 86, Batch 400, Loss: 0.35229310393333435\n",
      "\tEpoch 86, Batch 500, Loss: 0.18814490735530853\n",
      "\tEpoch 86, Batch 600, Loss: 0.3345909118652344\n",
      "\tEpoch 86, Batch 700, Loss: 0.20091649889945984\n",
      "\tEpoch 86, Batch 800, Loss: 0.1834709495306015\n",
      "\tEpoch 86, Batch 900, Loss: 0.24926382303237915\n",
      "\tEpoch 86, Batch 1000, Loss: 0.2081007957458496\n",
      "\tEpoch 86, Batch 1100, Loss: 0.3899611532688141\n",
      "\tEpoch 86, Batch 1200, Loss: 0.2809177041053772\n",
      "\tEpoch 86, Batch 1300, Loss: 0.2177647054195404\n",
      "\tEpoch 86, Batch 1400, Loss: 0.23935890197753906\n",
      "\tEpoch 86, Batch 1500, Loss: 0.3195188343524933\n",
      "\tEpoch 86, Batch 1600, Loss: 0.14422671496868134\n",
      "\tEpoch 86, Batch 1700, Loss: 0.16250383853912354\n",
      "\tEpoch 86, Batch 1800, Loss: 0.17194676399230957\n",
      "\tEpoch 86, Batch 1900, Loss: 0.15769712626934052\n",
      "\tEpoch 86, Batch 2000, Loss: 0.25142815709114075\n",
      "\tEpoch 86, Batch 2100, Loss: 0.4226241111755371\n",
      "Epoch: 86, Train Accuracy: 93.18%, Train Loss: 0.2414, Val Accuracy: 92.37%, Val Loss: 0.2879\n",
      "\tEpoch 87, Batch 100, Loss: 0.31378981471061707\n",
      "\tEpoch 87, Batch 200, Loss: 0.2499353289604187\n",
      "\tEpoch 87, Batch 300, Loss: 0.19891256093978882\n",
      "\tEpoch 87, Batch 400, Loss: 0.20956754684448242\n",
      "\tEpoch 87, Batch 500, Loss: 0.18398885428905487\n",
      "\tEpoch 87, Batch 600, Loss: 0.18845951557159424\n",
      "\tEpoch 87, Batch 700, Loss: 0.24415794014930725\n",
      "\tEpoch 87, Batch 800, Loss: 0.2828925848007202\n",
      "\tEpoch 87, Batch 900, Loss: 0.2194078117609024\n",
      "\tEpoch 87, Batch 1000, Loss: 0.10466122627258301\n",
      "\tEpoch 87, Batch 1100, Loss: 0.23843909800052643\n",
      "\tEpoch 87, Batch 1200, Loss: 0.3659200072288513\n",
      "\tEpoch 87, Batch 1300, Loss: 0.38691267371177673\n",
      "\tEpoch 87, Batch 1400, Loss: 0.3482309579849243\n",
      "\tEpoch 87, Batch 1500, Loss: 0.18616937100887299\n",
      "\tEpoch 87, Batch 1600, Loss: 0.3489725887775421\n",
      "\tEpoch 87, Batch 1700, Loss: 0.36308515071868896\n",
      "\tEpoch 87, Batch 1800, Loss: 0.3388448655605316\n",
      "\tEpoch 87, Batch 1900, Loss: 0.2611031234264374\n",
      "\tEpoch 87, Batch 2000, Loss: 0.23007354140281677\n",
      "\tEpoch 87, Batch 2100, Loss: 0.22030837833881378\n",
      "Epoch: 87, Train Accuracy: 93.22%, Train Loss: 0.2407, Val Accuracy: 92.35%, Val Loss: 0.2871\n",
      "\tEpoch 88, Batch 100, Loss: 0.17531614005565643\n",
      "\tEpoch 88, Batch 200, Loss: 0.08907392621040344\n",
      "\tEpoch 88, Batch 300, Loss: 0.08531805127859116\n",
      "\tEpoch 88, Batch 400, Loss: 0.16217464208602905\n",
      "\tEpoch 88, Batch 500, Loss: 0.2636081874370575\n",
      "\tEpoch 88, Batch 600, Loss: 0.24911406636238098\n",
      "\tEpoch 88, Batch 700, Loss: 0.1570633351802826\n",
      "\tEpoch 88, Batch 800, Loss: 0.25619006156921387\n",
      "\tEpoch 88, Batch 900, Loss: 0.2830217480659485\n",
      "\tEpoch 88, Batch 1000, Loss: 0.27699047327041626\n",
      "\tEpoch 88, Batch 1100, Loss: 0.26155462861061096\n",
      "\tEpoch 88, Batch 1200, Loss: 0.2032659500837326\n",
      "\tEpoch 88, Batch 1300, Loss: 0.22585617005825043\n",
      "\tEpoch 88, Batch 1400, Loss: 0.38476723432540894\n",
      "\tEpoch 88, Batch 1500, Loss: 0.1336916983127594\n",
      "\tEpoch 88, Batch 1600, Loss: 0.201270192861557\n",
      "\tEpoch 88, Batch 1700, Loss: 0.16326592862606049\n",
      "\tEpoch 88, Batch 1800, Loss: 0.16309195756912231\n",
      "\tEpoch 88, Batch 1900, Loss: 0.15916500985622406\n",
      "\tEpoch 88, Batch 2000, Loss: 0.2387394905090332\n",
      "\tEpoch 88, Batch 2100, Loss: 0.35184863209724426\n",
      "Epoch: 88, Train Accuracy: 93.20%, Train Loss: 0.2399, Val Accuracy: 92.28%, Val Loss: 0.2901\n",
      "\tEpoch 89, Batch 100, Loss: 0.3054492175579071\n",
      "\tEpoch 89, Batch 200, Loss: 0.20959258079528809\n",
      "\tEpoch 89, Batch 300, Loss: 0.4007799029350281\n",
      "\tEpoch 89, Batch 400, Loss: 0.11269694566726685\n",
      "\tEpoch 89, Batch 500, Loss: 0.45576193928718567\n",
      "\tEpoch 89, Batch 600, Loss: 0.2048516571521759\n",
      "\tEpoch 89, Batch 700, Loss: 0.23853933811187744\n",
      "\tEpoch 89, Batch 800, Loss: 0.12727835774421692\n",
      "\tEpoch 89, Batch 900, Loss: 0.24261027574539185\n",
      "\tEpoch 89, Batch 1000, Loss: 0.22222205996513367\n",
      "\tEpoch 89, Batch 1100, Loss: 0.23265914618968964\n",
      "\tEpoch 89, Batch 1200, Loss: 0.18966864049434662\n",
      "\tEpoch 89, Batch 1300, Loss: 0.0831962376832962\n",
      "\tEpoch 89, Batch 1400, Loss: 0.33670729398727417\n",
      "\tEpoch 89, Batch 1500, Loss: 0.3625808358192444\n",
      "\tEpoch 89, Batch 1600, Loss: 0.17071709036827087\n",
      "\tEpoch 89, Batch 1700, Loss: 0.2614075839519501\n",
      "\tEpoch 89, Batch 1800, Loss: 0.18105477094650269\n",
      "\tEpoch 89, Batch 1900, Loss: 0.24557587504386902\n",
      "\tEpoch 89, Batch 2000, Loss: 0.22749647498130798\n",
      "\tEpoch 89, Batch 2100, Loss: 0.1812790036201477\n",
      "Epoch: 89, Train Accuracy: 93.20%, Train Loss: 0.2408, Val Accuracy: 92.37%, Val Loss: 0.2862\n",
      "\tEpoch 90, Batch 100, Loss: 0.24194951355457306\n",
      "\tEpoch 90, Batch 200, Loss: 0.156100794672966\n",
      "\tEpoch 90, Batch 300, Loss: 0.12742316722869873\n",
      "\tEpoch 90, Batch 400, Loss: 0.4121097922325134\n",
      "\tEpoch 90, Batch 500, Loss: 0.17784367501735687\n",
      "\tEpoch 90, Batch 600, Loss: 0.2335745096206665\n",
      "\tEpoch 90, Batch 700, Loss: 0.43485817313194275\n",
      "\tEpoch 90, Batch 800, Loss: 0.2499256432056427\n",
      "\tEpoch 90, Batch 900, Loss: 0.17744798958301544\n",
      "\tEpoch 90, Batch 1000, Loss: 0.19089928269386292\n",
      "\tEpoch 90, Batch 1100, Loss: 0.3201601803302765\n",
      "\tEpoch 90, Batch 1200, Loss: 0.1258816421031952\n",
      "\tEpoch 90, Batch 1300, Loss: 0.2822166383266449\n",
      "\tEpoch 90, Batch 1400, Loss: 0.09663494676351547\n",
      "\tEpoch 90, Batch 1500, Loss: 0.21969301998615265\n",
      "\tEpoch 90, Batch 1600, Loss: 0.19486363232135773\n",
      "\tEpoch 90, Batch 1700, Loss: 0.2502632439136505\n",
      "\tEpoch 90, Batch 1800, Loss: 0.1810302585363388\n",
      "\tEpoch 90, Batch 1900, Loss: 0.20344415307044983\n",
      "\tEpoch 90, Batch 2000, Loss: 0.22257208824157715\n",
      "\tEpoch 90, Batch 2100, Loss: 0.3452736437320709\n",
      "Epoch: 90, Train Accuracy: 93.23%, Train Loss: 0.2381, Val Accuracy: 92.37%, Val Loss: 0.2867\n",
      "\tEpoch 91, Batch 100, Loss: 0.1292690485715866\n",
      "\tEpoch 91, Batch 200, Loss: 0.3578203022480011\n",
      "\tEpoch 91, Batch 300, Loss: 0.2547837197780609\n",
      "\tEpoch 91, Batch 400, Loss: 0.20250870287418365\n",
      "\tEpoch 91, Batch 500, Loss: 0.28722813725471497\n",
      "\tEpoch 91, Batch 600, Loss: 0.44824984669685364\n",
      "\tEpoch 91, Batch 700, Loss: 0.18560069799423218\n",
      "\tEpoch 91, Batch 800, Loss: 0.20464497804641724\n",
      "\tEpoch 91, Batch 900, Loss: 0.25141072273254395\n",
      "\tEpoch 91, Batch 1000, Loss: 0.43573057651519775\n",
      "\tEpoch 91, Batch 1100, Loss: 0.19939124584197998\n",
      "\tEpoch 91, Batch 1200, Loss: 0.29201969504356384\n",
      "\tEpoch 91, Batch 1300, Loss: 0.24124231934547424\n",
      "\tEpoch 91, Batch 1400, Loss: 0.14988364279270172\n",
      "\tEpoch 91, Batch 1500, Loss: 0.33937668800354004\n",
      "\tEpoch 91, Batch 1600, Loss: 0.14903567731380463\n",
      "\tEpoch 91, Batch 1700, Loss: 0.1919664889574051\n",
      "\tEpoch 91, Batch 1800, Loss: 0.30245792865753174\n",
      "\tEpoch 91, Batch 1900, Loss: 0.13219860196113586\n",
      "\tEpoch 91, Batch 2000, Loss: 0.11682754009962082\n",
      "\tEpoch 91, Batch 2100, Loss: 0.2569301724433899\n",
      "Epoch: 91, Train Accuracy: 93.33%, Train Loss: 0.2376, Val Accuracy: 92.30%, Val Loss: 0.2884\n",
      "\tEpoch 92, Batch 100, Loss: 0.28028404712677\n",
      "\tEpoch 92, Batch 200, Loss: 0.16042539477348328\n",
      "\tEpoch 92, Batch 300, Loss: 0.33997538685798645\n",
      "\tEpoch 92, Batch 400, Loss: 0.21816445887088776\n",
      "\tEpoch 92, Batch 500, Loss: 0.24788737297058105\n",
      "\tEpoch 92, Batch 600, Loss: 0.2645159065723419\n",
      "\tEpoch 92, Batch 700, Loss: 0.19029228389263153\n",
      "\tEpoch 92, Batch 800, Loss: 0.3587660789489746\n",
      "\tEpoch 92, Batch 900, Loss: 0.24907244741916656\n",
      "\tEpoch 92, Batch 1000, Loss: 0.20299121737480164\n",
      "\tEpoch 92, Batch 1100, Loss: 0.40821731090545654\n",
      "\tEpoch 92, Batch 1200, Loss: 0.24354654550552368\n",
      "\tEpoch 92, Batch 1300, Loss: 0.2397192120552063\n",
      "\tEpoch 92, Batch 1400, Loss: 0.29453644156455994\n",
      "\tEpoch 92, Batch 1500, Loss: 0.3863861560821533\n",
      "\tEpoch 92, Batch 1600, Loss: 0.2799534201622009\n",
      "\tEpoch 92, Batch 1700, Loss: 0.19349530339241028\n",
      "\tEpoch 92, Batch 1800, Loss: 0.26094380021095276\n",
      "\tEpoch 92, Batch 1900, Loss: 0.16856902837753296\n",
      "\tEpoch 92, Batch 2000, Loss: 0.3277371823787689\n",
      "\tEpoch 92, Batch 2100, Loss: 0.2299371063709259\n",
      "Epoch: 92, Train Accuracy: 93.30%, Train Loss: 0.2367, Val Accuracy: 92.36%, Val Loss: 0.2883\n",
      "\tEpoch 93, Batch 100, Loss: 0.13306844234466553\n",
      "\tEpoch 93, Batch 200, Loss: 0.1814412921667099\n",
      "\tEpoch 93, Batch 300, Loss: 0.10679028183221817\n",
      "\tEpoch 93, Batch 400, Loss: 0.2575441896915436\n",
      "\tEpoch 93, Batch 500, Loss: 0.20038029551506042\n",
      "\tEpoch 93, Batch 600, Loss: 0.12030179053544998\n",
      "\tEpoch 93, Batch 700, Loss: 0.1080663874745369\n",
      "\tEpoch 93, Batch 800, Loss: 0.21104326844215393\n",
      "\tEpoch 93, Batch 900, Loss: 0.13810160756111145\n",
      "\tEpoch 93, Batch 1000, Loss: 0.21377751231193542\n",
      "\tEpoch 93, Batch 1100, Loss: 0.11415368318557739\n",
      "\tEpoch 93, Batch 1200, Loss: 0.25282126665115356\n",
      "\tEpoch 93, Batch 1300, Loss: 0.2096371203660965\n",
      "\tEpoch 93, Batch 1400, Loss: 0.14235489070415497\n",
      "\tEpoch 93, Batch 1500, Loss: 0.18778221309185028\n",
      "\tEpoch 93, Batch 1600, Loss: 0.49242889881134033\n",
      "\tEpoch 93, Batch 1700, Loss: 0.1779111623764038\n",
      "\tEpoch 93, Batch 1800, Loss: 0.2831445336341858\n",
      "\tEpoch 93, Batch 1900, Loss: 0.2468089759349823\n",
      "\tEpoch 93, Batch 2000, Loss: 0.18693523108959198\n",
      "\tEpoch 93, Batch 2100, Loss: 0.19316406548023224\n",
      "Epoch: 93, Train Accuracy: 93.24%, Train Loss: 0.2371, Val Accuracy: 92.18%, Val Loss: 0.2884\n",
      "\tEpoch 94, Batch 100, Loss: 0.19702430069446564\n",
      "\tEpoch 94, Batch 200, Loss: 0.1527048498392105\n",
      "\tEpoch 94, Batch 300, Loss: 0.23183226585388184\n",
      "\tEpoch 94, Batch 400, Loss: 0.26066240668296814\n",
      "\tEpoch 94, Batch 500, Loss: 0.2624766230583191\n",
      "\tEpoch 94, Batch 600, Loss: 0.14335452020168304\n",
      "\tEpoch 94, Batch 700, Loss: 0.19354061782360077\n",
      "\tEpoch 94, Batch 800, Loss: 0.23375940322875977\n",
      "\tEpoch 94, Batch 900, Loss: 0.21332967281341553\n",
      "\tEpoch 94, Batch 1000, Loss: 0.20349013805389404\n",
      "\tEpoch 94, Batch 1100, Loss: 0.33588606119155884\n",
      "\tEpoch 94, Batch 1200, Loss: 0.37952661514282227\n",
      "\tEpoch 94, Batch 1300, Loss: 0.17532959580421448\n",
      "\tEpoch 94, Batch 1400, Loss: 0.16636541485786438\n",
      "\tEpoch 94, Batch 1500, Loss: 0.23145915567874908\n",
      "\tEpoch 94, Batch 1600, Loss: 0.28049081563949585\n",
      "\tEpoch 94, Batch 1700, Loss: 0.2536240816116333\n",
      "\tEpoch 94, Batch 1800, Loss: 0.20600007474422455\n",
      "\tEpoch 94, Batch 1900, Loss: 0.5009831190109253\n",
      "\tEpoch 94, Batch 2000, Loss: 0.17176060378551483\n",
      "\tEpoch 94, Batch 2100, Loss: 0.3194584846496582\n",
      "Epoch: 94, Train Accuracy: 93.33%, Train Loss: 0.2354, Val Accuracy: 92.37%, Val Loss: 0.2863\n",
      "\tEpoch 95, Batch 100, Loss: 0.24844211339950562\n",
      "\tEpoch 95, Batch 200, Loss: 0.1609499156475067\n",
      "\tEpoch 95, Batch 300, Loss: 0.12524986267089844\n",
      "\tEpoch 95, Batch 400, Loss: 0.3024744391441345\n",
      "\tEpoch 95, Batch 500, Loss: 0.22661547362804413\n",
      "\tEpoch 95, Batch 600, Loss: 0.2523289918899536\n",
      "\tEpoch 95, Batch 700, Loss: 0.2438005656003952\n",
      "\tEpoch 95, Batch 800, Loss: 0.14902280271053314\n",
      "\tEpoch 95, Batch 900, Loss: 0.3164138197898865\n",
      "\tEpoch 95, Batch 1000, Loss: 0.15269088745117188\n",
      "\tEpoch 95, Batch 1100, Loss: 0.18135976791381836\n",
      "\tEpoch 95, Batch 1200, Loss: 0.26170048117637634\n",
      "\tEpoch 95, Batch 1300, Loss: 0.1923489272594452\n",
      "\tEpoch 95, Batch 1400, Loss: 0.25346967577934265\n",
      "\tEpoch 95, Batch 1500, Loss: 0.18695908784866333\n",
      "\tEpoch 95, Batch 1600, Loss: 0.21858294308185577\n",
      "\tEpoch 95, Batch 1700, Loss: 0.30953341722488403\n",
      "\tEpoch 95, Batch 1800, Loss: 0.259479284286499\n",
      "\tEpoch 95, Batch 1900, Loss: 0.2970871329307556\n",
      "\tEpoch 95, Batch 2000, Loss: 0.1622854769229889\n",
      "\tEpoch 95, Batch 2100, Loss: 0.2564636766910553\n",
      "Epoch: 95, Train Accuracy: 93.32%, Train Loss: 0.2355, Val Accuracy: 92.21%, Val Loss: 0.2901\n",
      "\tEpoch 96, Batch 100, Loss: 0.17725364863872528\n",
      "\tEpoch 96, Batch 200, Loss: 0.20376695692539215\n",
      "\tEpoch 96, Batch 300, Loss: 0.1270104944705963\n",
      "\tEpoch 96, Batch 400, Loss: 0.227881520986557\n",
      "\tEpoch 96, Batch 500, Loss: 0.22541603446006775\n",
      "\tEpoch 96, Batch 600, Loss: 0.4525560140609741\n",
      "\tEpoch 96, Batch 700, Loss: 0.19701197743415833\n",
      "\tEpoch 96, Batch 800, Loss: 0.27249935269355774\n",
      "\tEpoch 96, Batch 900, Loss: 0.4294443428516388\n",
      "\tEpoch 96, Batch 1000, Loss: 0.26157280802726746\n",
      "\tEpoch 96, Batch 1100, Loss: 0.3230488896369934\n",
      "\tEpoch 96, Batch 1200, Loss: 0.23155874013900757\n",
      "\tEpoch 96, Batch 1300, Loss: 0.2731061279773712\n",
      "\tEpoch 96, Batch 1400, Loss: 0.4089929759502411\n",
      "\tEpoch 96, Batch 1500, Loss: 0.19673281908035278\n",
      "\tEpoch 96, Batch 1600, Loss: 0.3839499354362488\n",
      "\tEpoch 96, Batch 1700, Loss: 0.23360642790794373\n",
      "\tEpoch 96, Batch 1800, Loss: 0.2530018091201782\n",
      "\tEpoch 96, Batch 1900, Loss: 0.3357081711292267\n",
      "\tEpoch 96, Batch 2000, Loss: 0.19597606360912323\n",
      "\tEpoch 96, Batch 2100, Loss: 0.19752848148345947\n",
      "Epoch: 96, Train Accuracy: 93.37%, Train Loss: 0.2349, Val Accuracy: 92.17%, Val Loss: 0.2873\n",
      "\tEpoch 97, Batch 100, Loss: 0.3123026192188263\n",
      "\tEpoch 97, Batch 200, Loss: 0.3416900038719177\n",
      "\tEpoch 97, Batch 300, Loss: 0.3108769357204437\n",
      "\tEpoch 97, Batch 400, Loss: 0.23193180561065674\n",
      "\tEpoch 97, Batch 500, Loss: 0.25732728838920593\n",
      "\tEpoch 97, Batch 600, Loss: 0.33671846985816956\n",
      "\tEpoch 97, Batch 700, Loss: 0.24513433873653412\n",
      "\tEpoch 97, Batch 800, Loss: 0.17972972989082336\n",
      "\tEpoch 97, Batch 900, Loss: 0.21614156663417816\n",
      "\tEpoch 97, Batch 1000, Loss: 0.2986997663974762\n",
      "\tEpoch 97, Batch 1100, Loss: 0.31570202112197876\n",
      "\tEpoch 97, Batch 1200, Loss: 0.3036893606185913\n",
      "\tEpoch 97, Batch 1300, Loss: 0.15535107254981995\n",
      "\tEpoch 97, Batch 1400, Loss: 0.12790267169475555\n",
      "\tEpoch 97, Batch 1500, Loss: 0.2381180226802826\n",
      "\tEpoch 97, Batch 1600, Loss: 0.15309807658195496\n",
      "\tEpoch 97, Batch 1700, Loss: 0.3725645840167999\n",
      "\tEpoch 97, Batch 1800, Loss: 0.24457262456417084\n",
      "\tEpoch 97, Batch 1900, Loss: 0.21597717702388763\n",
      "\tEpoch 97, Batch 2000, Loss: 0.21341043710708618\n",
      "\tEpoch 97, Batch 2100, Loss: 0.4826967418193817\n",
      "Epoch: 97, Train Accuracy: 93.48%, Train Loss: 0.2325, Val Accuracy: 92.31%, Val Loss: 0.2857\n",
      "\tEpoch 98, Batch 100, Loss: 0.27812278270721436\n",
      "\tEpoch 98, Batch 200, Loss: 0.1657238006591797\n",
      "\tEpoch 98, Batch 300, Loss: 0.3408694863319397\n",
      "\tEpoch 98, Batch 400, Loss: 0.2250259816646576\n",
      "\tEpoch 98, Batch 500, Loss: 0.1719093918800354\n",
      "\tEpoch 98, Batch 600, Loss: 0.1890067607164383\n",
      "\tEpoch 98, Batch 700, Loss: 0.4236921966075897\n",
      "\tEpoch 98, Batch 800, Loss: 0.1416545957326889\n",
      "\tEpoch 98, Batch 900, Loss: 0.13588742911815643\n",
      "\tEpoch 98, Batch 1000, Loss: 0.20250844955444336\n",
      "\tEpoch 98, Batch 1100, Loss: 0.2347070872783661\n",
      "\tEpoch 98, Batch 1200, Loss: 0.21063773334026337\n",
      "\tEpoch 98, Batch 1300, Loss: 0.14911167323589325\n",
      "\tEpoch 98, Batch 1400, Loss: 0.19172783195972443\n",
      "\tEpoch 98, Batch 1500, Loss: 0.5023102760314941\n",
      "\tEpoch 98, Batch 1600, Loss: 0.3579378128051758\n",
      "\tEpoch 98, Batch 1700, Loss: 0.43384045362472534\n",
      "\tEpoch 98, Batch 1800, Loss: 0.3454478979110718\n",
      "\tEpoch 98, Batch 1900, Loss: 0.1188671737909317\n",
      "\tEpoch 98, Batch 2000, Loss: 0.1970233917236328\n",
      "\tEpoch 98, Batch 2100, Loss: 0.3031964898109436\n",
      "Epoch: 98, Train Accuracy: 93.42%, Train Loss: 0.2333, Val Accuracy: 92.42%, Val Loss: 0.2835\n",
      "\tEpoch 99, Batch 100, Loss: 0.27274128794670105\n",
      "\tEpoch 99, Batch 200, Loss: 0.15922458469867706\n",
      "\tEpoch 99, Batch 300, Loss: 0.1249048113822937\n",
      "\tEpoch 99, Batch 400, Loss: 0.19330783188343048\n",
      "\tEpoch 99, Batch 500, Loss: 0.2760079503059387\n",
      "\tEpoch 99, Batch 600, Loss: 0.3054702877998352\n",
      "\tEpoch 99, Batch 700, Loss: 0.24445891380310059\n",
      "\tEpoch 99, Batch 800, Loss: 0.18554438650608063\n",
      "\tEpoch 99, Batch 900, Loss: 0.1730956733226776\n",
      "\tEpoch 99, Batch 1000, Loss: 0.1928710788488388\n",
      "\tEpoch 99, Batch 1100, Loss: 0.27950888872146606\n",
      "\tEpoch 99, Batch 1200, Loss: 0.3480311930179596\n",
      "\tEpoch 99, Batch 1300, Loss: 0.32573142647743225\n",
      "\tEpoch 99, Batch 1400, Loss: 0.26552510261535645\n",
      "\tEpoch 99, Batch 1500, Loss: 0.2777940034866333\n",
      "\tEpoch 99, Batch 1600, Loss: 0.12383424490690231\n",
      "\tEpoch 99, Batch 1700, Loss: 0.2667003571987152\n",
      "\tEpoch 99, Batch 1800, Loss: 0.2048313170671463\n",
      "\tEpoch 99, Batch 1900, Loss: 0.13727377355098724\n",
      "\tEpoch 99, Batch 2000, Loss: 0.1945369839668274\n",
      "\tEpoch 99, Batch 2100, Loss: 0.15583910048007965\n",
      "Epoch: 99, Train Accuracy: 93.38%, Train Loss: 0.2329, Val Accuracy: 92.03%, Val Loss: 0.2924\n",
      "\tEpoch 100, Batch 100, Loss: 0.2572242021560669\n",
      "\tEpoch 100, Batch 200, Loss: 0.2601032555103302\n",
      "\tEpoch 100, Batch 300, Loss: 0.19107875227928162\n",
      "\tEpoch 100, Batch 400, Loss: 0.2490255981683731\n",
      "\tEpoch 100, Batch 500, Loss: 0.235174298286438\n",
      "\tEpoch 100, Batch 600, Loss: 0.23199601471424103\n",
      "\tEpoch 100, Batch 700, Loss: 0.09635263681411743\n",
      "\tEpoch 100, Batch 800, Loss: 0.2727152705192566\n",
      "\tEpoch 100, Batch 900, Loss: 0.19814956188201904\n",
      "\tEpoch 100, Batch 1000, Loss: 0.2199571430683136\n",
      "\tEpoch 100, Batch 1100, Loss: 0.2911035418510437\n",
      "\tEpoch 100, Batch 1200, Loss: 0.2804577350616455\n",
      "\tEpoch 100, Batch 1300, Loss: 0.19167043268680573\n",
      "\tEpoch 100, Batch 1400, Loss: 0.17491352558135986\n",
      "\tEpoch 100, Batch 1500, Loss: 0.3782549202442169\n",
      "\tEpoch 100, Batch 1600, Loss: 0.22961454093456268\n",
      "\tEpoch 100, Batch 1700, Loss: 0.2804284691810608\n",
      "\tEpoch 100, Batch 1800, Loss: 0.18558433651924133\n",
      "\tEpoch 100, Batch 1900, Loss: 0.30841583013534546\n",
      "\tEpoch 100, Batch 2000, Loss: 0.22400979697704315\n",
      "\tEpoch 100, Batch 2100, Loss: 0.15444029867649078\n",
      "Epoch: 100, Train Accuracy: 93.42%, Train Loss: 0.2318, Val Accuracy: 92.26%, Val Loss: 0.2897\n"
     ]
    }
   ],
   "source": [
    "def early_callback():\n",
    "    \"\"\"\n",
    "    Function in charge of printing out a message when the training process is stopped by the early stopper.\n",
    "    \"\"\"\n",
    "    print(\"\\nEarly stopping criterion met. Stopping training process.\\n\")\n",
    "\n",
    "def callback(epoch, val_accuracy, val_loss, train_accuracy, train_loss):\n",
    "    \"\"\"\n",
    "    Function in charge of printing out the statistics of the model on the current epoch.\n",
    "\n",
    "    Parameters\n",
    "    -------------------------------------------------------------\n",
    "    epoch (int): current epoch.\n",
    "    val_accuracy (float): accuracy of the model on the validation dataset.\n",
    "    val_loss (float): loss of the model on the validation dataset.\n",
    "    train_accuracy (float): accuracy of the model on the training dataset.\n",
    "    train_loss (float): loss of the model on the training dataset.\n",
    "    \"\"\"\n",
    "    print(f\"Epoch: {epoch}, Train Accuracy: {train_accuracy:.2f}%, Train Loss: {train_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "def execute():\n",
    "    \"\"\"\n",
    "    Function in charge of executing the training process of the convolutional neural network.\n",
    "    \"\"\"\n",
    "\n",
    "    # Defining the model, the optimizer, the loss function, the learning rate scheduler and the early stopper\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = ResNet20()\n",
    "    optimizer = optim.SGD(model.parameters(), lr = LEARNING_RATE, momentum = MOMENTUM, nesterov = True, weight_decay = WEIGHT_DECAY)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode = 'min', patience = SCHEDULER_PATIENCE, factor = SCHEDULER_FACTOR, verbose = True)\n",
    "    early_stopper = Early_Stopper(callback = early_callback)\n",
    "\n",
    "    # Defining lists to keep track of the training process\n",
    "    train_accuracies_history = []\n",
    "    train_losses_history = []\n",
    "    eval_accuracies_history = []\n",
    "    eval_losses_history = []\n",
    "\n",
    "    # Checking if a checkpoint is available\n",
    "    if os.path.exists(CHECKPOINT_PATH):\n",
    "        checkpoint = load_checkpoint(model, optimizer, scheduler, CHECKPOINT_PATH)\n",
    "        model.load_state_dict(checkpoint['model'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        scheduler.load_state_dict(checkpoint['scheduler'])\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        tracking_lists = checkpoint['tracking']\n",
    "\n",
    "        # Mover los tensores del estado del optimizador al dispositivo cuda\n",
    "        for state in optimizer.state.values():\n",
    "            for key, value in state.items():\n",
    "                if isinstance(value, torch.Tensor):\n",
    "                    state[key] = value.to(device)\n",
    "    else:\n",
    "        start_epoch = 0\n",
    "        tracking_lists = {\n",
    "            'train_accuracies': train_accuracies_history,\n",
    "            'train_losses': train_losses_history,\n",
    "            'eval_accuracies': eval_accuracies_history,\n",
    "            'eval_losses': eval_losses_history\n",
    "        }\n",
    "\n",
    "    # Defining the data loaders\n",
    "    loaders = {\n",
    "        'train': train_loader,\n",
    "        'val': val_loader\n",
    "    }\n",
    "\n",
    "    # Training the model\n",
    "    postrained_tracking_lists, val_predictions, val_targets = train(model, loaders, optimizer, criterion, scheduler, tracking_lists, callback, early_stopper, start_epoch, epochs = EPOCHS, device = device)\n",
    "\n",
    "execute()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "usr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
